<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  
    <meta name="keywords" content="NO response">
  
  
    <meta name="description" content="what you will be,the world will be">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    吴恩达机器学习第一次编程作业-线性回归 |
    
    种花家</title>
  
    <link rel="shortcut icon" href="/images/rabbit.png">
  
  <link rel="stylesheet" href="../../../../css/style.css">
  <link rel="stylesheet" href="../../../../css/technology.css">
  
    <link rel="stylesheet" href="../../../../fancybox/jquery.fancybox.min.css">
  
  <script src="../../../../js/pace.min.js"></script>
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <section class="outer">
  <article id="post-吴恩达机器学习第一次编程作业-线性回归" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      吴恩达机器学习第一次编程作业-线性回归
    </h1>
  

      </header>
    

    
      <div class="article-meta">
        <a href class="article-date">
  <time datetime="2019-07-11T01:46:00.000Z" itemprop="datePublished">2019-07-11</time>
</a>
        
  <div class="article-category">
    <a class="article-category-link" href="../../../../categories/机器学习/">机器学习</a>
  </div>

      </div>
    

    <div class="article-entry" itemprop="articleBody">
      

      

      
        <h3 id="课时45-编程作业：线性回归"><a href="#课时45-编程作业：线性回归" class="headerlink" title="课时45 编程作业：线性回归"></a><center><a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/text?lessonId=1053430047&amp;courseId=1004570029" target="_blank" rel="noopener">课时45 编程作业：线性回归</a></center></h3><p>原文档使用Octave/MATLAB作为官方教程，这里使用python实现线性回归</p>
<hr>
<p>资料：<a href="https://sfz-lyq.cn/2019/06/05/%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E5%BD%A2%E5%9B%9E%E5%BD%92/">单变量线形回归-笔记</a> 、<a href="https://sfz-lyq.cn/2019/06/24/%E5%A4%9A%E5%8F%98%E9%87%8F%E7%BA%BF%E5%BD%A2%E5%9B%9E%E5%BD%92/#more">多变量线性回归-笔记</a></p>
<hr>
<p>工具：pycharm2018.3.3</p>
<hr>
<h4 id="第一节-引言"><a href="#第一节-引言" class="headerlink" title="第一节 引言"></a>第一节 引言</h4><h4 id="第二节-单变量线性回归"><a href="#第二节-单变量线性回归" class="headerlink" title="第二节 单变量线性回归"></a>第二节 单变量线性回归</h4><p><a href="https://paste.ubuntu.com/p/DbxXWKHGMg/" target="_blank" rel="noopener">完整代码1</a><br><a href="https://paste.ubuntu.com/p/B7fHVxSbVh/" target="_blank" rel="noopener">完整代码2</a><br><a href="https://blog.csdn.net/weixin_42415485/article/details/81285615" target="_blank" rel="noopener">参考1</a></p>
<h5 id="2-0-导入包"><a href="#2-0-导入包" class="headerlink" title="2.0 导入包"></a>2.0 导入包</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">matplotlib.use(&apos;TkAgg&apos;)</span><br></pre></td></tr></table></figure>
<h5 id="2-1-Plotting-the-Data-绘制数据图"><a href="#2-1-Plotting-the-Data-绘制数据图" class="headerlink" title="2.1 Plotting the Data 绘制数据图"></a>2.1 Plotting the Data 绘制数据图</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def get_data():   # 读取数据，返回两个向量</span><br><span class="line">	population_profit = np.genfromtxt(&quot;exercise/ex1/ex1data1.txt&quot;, delimiter=&apos;,&apos;, dtype=float)</span><br><span class="line">	# print(population_profit.shape)</span><br><span class="line">	population = np.array(population_profit[:, 0])   # 以向量形式获取第一列</span><br><span class="line">	profit = np.array(population_profit[:, 1])  # 同上</span><br><span class="line">	return population,profit    # 以向量形式返回横轴数据和纵轴数据</span><br><span class="line"></span><br><span class="line">def draw_picture(x,y):  # 传入横轴和对应纵轴数据</span><br><span class="line">	population,profit=x,y</span><br><span class="line">	plt.xlim(4,24)    # plt.xlim(4,24,2) 表示以2为刻度单位显示横轴坐标范围为[4,24]</span><br><span class="line">	plt.ylim(-5,25)</span><br><span class="line">	plt.plot(population,profit,&apos;xr&apos;)  # ‘x’为样式，‘r’为颜色，可自主调整</span><br><span class="line">	plt.xlabel(&apos;Population of City in 10,000s&apos;)  # 添加横轴标签</span><br><span class="line">	plt.ylabel(&apos;Profit in $10,000s&apos;)</span><br><span class="line">	plt.title(&apos;one variable for the profits&apos;,fontsize=20)   # 设置图片标题</span><br><span class="line">	plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMDkvNWQyNDRlOWJlZjE0Nzg5MDcyLnBuZw" alt="14.png"></p>
<h5 id="2-2-Gradient-Descent-梯度下降"><a href="#2-2-Gradient-Descent-梯度下降" class="headerlink" title="2.2 Gradient Descent 梯度下降"></a>2.2 Gradient Descent 梯度下降</h5><ul>
<li>概念：<a href="https://sfz-lyq.cn/2019/06/05/%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E5%BD%A2%E5%9B%9E%E5%BD%92/">单变量线形回归</a></li>
<li>需要注意的是&theta;需要<strong>同时</strong>更新 - batch gradient descent algorith 批量梯度下降算法</li>
<li>核心在于计算代价函数CostFunction和theta</li>
<li>用python实现时最好把进行运算的对象都转化为相同类型</li>
<li>关于梯度下降同时求&theta;操作理解补充，请看图：<a href="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMTMvNWQyOTRkNDQzZmEwMjQ5MzQ1LnBuZw" target="_blank" rel="noopener">梯度下降.png</a>，这样就解释了为什么代码中每个训练样本x都要与hypothesis相乘再相加，</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def computer_cost(x,y,theta,m):   # 计算平方误差代价函数,x，y为横轴纵轴数据，m为训练样本数</span><br><span class="line">	# print(x.shape,theta.shape,y.shape)</span><br><span class="line">	hypothesis = np.dot(x, theta)   # 假设函数</span><br><span class="line">	# print(hypothesis.shape,y.shape)</span><br><span class="line">	# print((hypothesis-y).shape)</span><br><span class="line">	return  np.sum(((hypothesis-y)**2))/(2.0*m)   # 代价函数 J</span><br><span class="line"></span><br><span class="line">def Gradient_Descent(x,y,m,theta,alpha,iterations):</span><br><span class="line">	cost=np.zeros(iterations+1)  # 将初始值也算进去然后迭代iterations次</span><br><span class="line">	cost[0]=computer_cost(x,y,theta,m)  # 代价函数初始值</span><br><span class="line">	each_theta=np.zeros((2,iterations+1))</span><br><span class="line">	each_theta[0][0],each_theta[1][0]=theta[0][0],theta[1][0]</span><br><span class="line">	for iteration in range(1,iterations+1):</span><br><span class="line">		theta=theta-(alpha/m)*(x.T.dot((x.dot(theta)-y)))</span><br><span class="line">		cost[iteration]=computer_cost(x,y,theta,m)  # 同步记录每一个值，然后作3D图或等高线</span><br><span class="line">		each_theta[0][iteration], each_theta[1][iteration] = theta[0][0], theta[1][0]</span><br><span class="line">		# print(&apos;第%d次迭代，&apos; %iteration,&apos;代价函数值为：&apos;,cost[iteration],&apos;此时theta为：&apos;,theta.ravel())</span><br><span class="line">	return cost,theta,each_theta   # 返回所有的代价函数和参数</span><br></pre></td></tr></table></figure>
<p>设置好初始&theta;、&alpha;、迭代次数iterations后调用即可，接口如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def One_Variable_Linear_Regression(theta,alpha,iterations):</span><br><span class="line">	x, y = get_data()  # 获取数据</span><br><span class="line">	m = x.size   # 训练样本数</span><br><span class="line">	temp = np.ones((m, 1))  # 生成一列1，作为X0</span><br><span class="line">	x = x.reshape(m, 1)  # 将向量变化成矩阵</span><br><span class="line">	y = y.reshape(m, 1)  # 为了避免出错，务必保持参与运算的所有对象类型一致，务必务必</span><br><span class="line">	x = np.hstack((temp, x))  # 制造X0为1</span><br><span class="line">	# print(&apos;初始代价函数值为：&apos;,computer_cost(x,y,theta,m))</span><br><span class="line">	cost,final_theta,each_theta=Gradient_Descent(x,y,m,theta,alpha,iterations)</span><br><span class="line">	predict_profit(final_theta)   # 根据最终的theta预测利润，代码见下</span><br><span class="line">	Adapting(x,y,final_theta)   # 进行拟合，代码见下</span><br></pre></td></tr></table></figure></p>
<p>得到最终的theta后就可以进行预测了和拟合了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def predict_profit(theta):</span><br><span class="line">	# 绘制出cost函数图找到中心对应的theta即我们要找的参数</span><br><span class="line">	# 用得出的参数来计算hypothesis假设函数即预测，再绘制出来观察是否和源数据拟合</span><br><span class="line">	print(theta)</span><br><span class="line">	predict1=(np.array([1,3.5])*theta).sum()</span><br><span class="line">	predict2=(np.array([1,7])*theta).sum()</span><br><span class="line">	print(&apos;当人口数为3.5W时，利润为：&apos;,predict1*10000)</span><br><span class="line">	print(&apos;当人口数为7w时，利润为：&apos;,predict2*10000)</span><br><span class="line"></span><br><span class="line">def Adapting(x,y,theta): # 拟合</span><br><span class="line">	hypothesis=x.dot(theta)    # 预测值</span><br><span class="line">	plt.xticks(np.arange(4,24,2))  # 设置横轴坐标范围和刻度</span><br><span class="line">	plt.yticks(np.arange(-5,25,5))</span><br><span class="line">	plt.xlabel(&apos;Population of City in 10,000s&apos;,fontsize=7)   # 设置横轴标签</span><br><span class="line">	plt.ylabel(&apos;Profit in $10,000s&apos;,fontsize=7)</span><br><span class="line">	plt.title(&apos;Training data with linear regression fit&apos;,fontsize=15)</span><br><span class="line">	plt.plot((x.T)[1],y,&apos;xr&apos;,label=&apos;Training data&apos;)</span><br><span class="line">	plt.plot((x.T)[1],hypothesis,label=&apos;Linear regression&apos;)</span><br><span class="line">	plt.legend(loc=&apos;lower right&apos;,fontsize=8)  # 线条含义说明</span><br><span class="line">	plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMTAvNWQyNWRiMjQ1MjljODU0Mzk0LnBuZw" alt="15.png"></p>
<h5 id="2-4-根据J-theta-绘制等高线和三维图"><a href="#2-4-根据J-theta-绘制等高线和三维图" class="headerlink" title="2.4 根据J(&theta;)绘制等高线和三维图"></a>2.4 根据J(&theta;)绘制等高线和三维图</h5><p>不会<br>占坑</p>
<h5 id="关于单变量线性回归的理解"><a href="#关于单变量线性回归的理解" class="headerlink" title="关于单变量线性回归的理解"></a>关于单变量线性回归的理解</h5><ol>
<li>含义：一个模型而已，目的是用来预测的。单变量指只有一个特征输入</li>
<li>用图片来讲，是让得出的预测模型跑出的数据和源数据更好的拟合如上图所示</li>
<li>那么为了这个目的提出了代价函数J（也叫损失函数）并给出了计算公式，表示模型和源数据的偏差程度，当然越小越好</li>
<li>这个计算公式是根据梯度下降算法得出的，根据偏导数更新。关于梯度下降有一篇比较容易理解的文章：<a href="https://blog.csdn.net/sword_csdn/article/details/77886858" target="_blank" rel="noopener">参考</a> ，里面说明了为什么梯度下降能使&theta;和代价函数最小化</li>
<li>梯度下降这里使用的是批量梯度下降，还有其他的方法，比如随机梯度下降以及….忘了，它有一个缺陷好像是只能达到局部最优，这个比较好理解，可能存在那么一个低谷，它的偏导数正好是0，那么就走不动了。</li>
<li>关于学习率：以前笔记写的是梯度下降每一步走的步长，这里说每一步是指一次迭代相当于走了一步，正常它是往下走的，为什么往下走上面那篇参考也给出了答案：在右边导数为正，那么减去正数，而在右边时导数为负，看似加上一个值，但初始&theta;设置的是0啊，所以是一个负数加上一个正数，还是往低谷走</li>
<li>为什么要绘制图形，等高线和三维图可以直观地反映出梯度下降的过程，所以作图能力Emmmmm</li>
<li>关于工具的使用，这个真的要熟悉语言特性，不然碰到很多很多问题</li>
</ol>
<h4 id="第三节-Linear-regression-with-multiple-variables多变量线性回归"><a href="#第三节-Linear-regression-with-multiple-variables多变量线性回归" class="headerlink" title="第三节 Linear regression with multiple variables多变量线性回归"></a>第三节 Linear regression with multiple variables多变量线性回归</h4><p><a href="https://paste.ubuntu.com/p/ZG6SpmTMj2/" target="_blank" rel="noopener">完整代码1</a></p>
<h5 id="3-1-特征缩放-标准化（Normalization）"><a href="#3-1-特征缩放-标准化（Normalization）" class="headerlink" title="3.1 特征缩放-标准化（Normalization）"></a>3.1 特征缩放-标准化（Normalization）</h5><ul>
<li>关于特征缩放：<a href="https://blog.csdn.net/cluster1893/article/details/80759738" target="_blank" rel="noopener">特征缩放</a></li>
<li>sklearn 库中有快速实现标准化的方法，但….窝不会这个库啊，所以还是用笨方法实现</li>
<li>特征缩放方法很多，这里是减去平均值再除以<a href="https://baike.baidu.com/item/%E6%A0%87%E5%87%86%E5%B7%AE/1415772?fr=aladdin" target="_blank" rel="noopener">标准差</a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def Feature_Normalization(x,m):  # 特征缩放，减去平均值再除以标准差</span><br><span class="line">	mean_value=x.sum(axis=0)/m  # 按列求和再取平均</span><br><span class="line">	std=math.sqrt(((x-mean_value)**2).sum()/m)  # 求标准差</span><br><span class="line">	x=(x-mean_value)/std   # 特征缩放</span><br><span class="line">	return x</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="3-2-梯度下降"><a href="#3-2-梯度下降" class="headerlink" title="3.2 梯度下降"></a>3.2 梯度下降</h5><ul>
<li>和单变量一样，只不过维数增加了，运算完全一样</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def Computer_Cost(x,y,theta,m):</span><br><span class="line">	hypothesis=x.dot(theta) # 假设函数</span><br><span class="line">	return np.sum((hypothesis-y)**2)/2/m   # 代价函数</span><br><span class="line"></span><br><span class="line">def Gradient_Descent(x,y,theta,m,iterations,alpha):</span><br><span class="line">	cost=np.zeros(iterations+1)</span><br><span class="line">	cost[0]=Computer_Cost(x,y,theta,m)</span><br><span class="line">	for iteration in range(1,iterations+1):</span><br><span class="line">		theta=theta-np.dot(x.T,(alpha/m)*(x.dot(theta)-y))</span><br><span class="line">		cost[iteration]=Computer_Cost(x,y,theta,m)</span><br><span class="line">		# print(&apos;第%d次迭代，&apos;%iteration,&apos;代价函数为：&apos;,cost[iteration],&apos;此时theta为：&apos;,theta.ravel())</span><br><span class="line">	return cost,theta</span><br></pre></td></tr></table></figure>
<p>在函数中调用接口即可<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def Draw_Iterations_Cost(iterations,y):  # 根据学习率绘制代价函数随迭代次数的变化</span><br><span class="line">	x=np.linspace(0,iterations,iterations)</span><br><span class="line">	plt.xlim(0,50)</span><br><span class="line">	plt.ylim(0,7e10)</span><br><span class="line">	plt.plot(x,y,&apos;y&apos;)</span><br><span class="line">	plt.xlabel(&apos;Number of iterations&apos;,fontsize=7)</span><br><span class="line">	plt.ylabel(&apos;Cost J&apos;,fontsize=7)</span><br><span class="line">	plt.title(&apos;Convergence of gradient descent&apos;,fontsize=13)</span><br><span class="line">	plt.show()</span><br><span class="line">	</span><br><span class="line">def Multiple_Variables_Linear_Regression(x,y,theta,m,iterations,alpha):</span><br><span class="line">	cost,final_theta=Gradient_Descent(x,y,theta,m,iterations,alpha)</span><br><span class="line">	Draw_Iterations_Cost(iterations+1,cost)  # 绘制代价函数收敛曲线</span><br></pre></td></tr></table></figure></p>
<p>获取数据与初始化</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def Get_Data():</span><br><span class="line">	temp_data=np.genfromtxt(&apos;exercise/ex1/ex1data2.txt&apos;,delimiter=&apos;,&apos;,dtype=float)</span><br><span class="line">	X=temp_data[:,0:2].reshape(temp_data.shape[0],temp_data.shape[1]-1)  # 多特征</span><br><span class="line">	Y=temp_data[:,2].reshape(temp_data.shape[0],1)   # 输出只有一列</span><br><span class="line">	# print(X.shape,Y.shape)</span><br><span class="line">	return X,Y,X.shape[0]      # 返回X特征值，Y为输出，X.size为训练样本数</span><br><span class="line"></span><br><span class="line">x,y,m=Get_Data()</span><br><span class="line">x=Feature_Normalization(x,m)  # 特征缩放</span><br><span class="line">x=np.hstack((np.ones((m,1)),x))  # 添加 X0 = 1</span><br><span class="line">theta=np.zeros((x.shape[1],1))   # 设置初始theta</span><br><span class="line">iterations=100</span><br><span class="line">alpha=0.1  # 学习率 可尝试0.3，0.1，0.03，0.01，绘制出来的收敛曲线是不同的</span><br><span class="line">Multiple_Variables_Linear_Regression(x,y,theta,m,iterations,alpha)</span><br></pre></td></tr></table></figure>
<p>选取&alpha;=0.1绘制出来的收敛曲线如下：<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMTMvNWQyOWE5NTY3OTE5NjU2MTIxLnBuZw" alt="16.png"></p>
<p> TODO 绘制数据三维图与等高线</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://sfz-lyq.cn/2019/07/11/吴恩达机器学习第一次编程作业-线性回归/" data-id="ck5c8g3xb00eo4zpvwgndti0p" class="article-share-link">分享</a>
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../tags/Python/">Python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../tags/机器学习/">机器学习</a></li></ul>

    </footer>

  </div>

  
    
  <nav class="article-nav">
    
      <a href="../文献阅读一/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            文献阅读一
          
        </div>
      </a>
    
    
      <a href="../NumPy库学习/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">NumPy库学习</div>
      </a>
    
  </nav>


  

  
    
  

</article>



</section>
    </div>
    <script src="../../../../js/jquery-2.0.3.min.js"></script>
<script src="../../../../js/lazyload.min.js"></script>
<script src="../../../../js/busuanzi-2.3.pure.min.js"></script>


  <script src="../../../../fancybox/jquery.fancybox.min.js"></script>



  <script src="../../../../js/search.js"></script>


<script src="../../../../js/technology.js"></script>

  </div>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body>
</html>