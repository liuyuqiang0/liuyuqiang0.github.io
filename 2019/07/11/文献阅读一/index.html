<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  
    <meta name="keywords" content="NO response">
  
  
    <meta name="description" content="what you will be,the world will be">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    文献阅读一 |
    
    种花家</title>
  
    <link rel="shortcut icon" href="/images/rabbit.png">
  
  <link rel="stylesheet" href="../../../../css/style.css">
  <link rel="stylesheet" href="../../../../css/technology.css">
  
    <link rel="stylesheet" href="../../../../fancybox/jquery.fancybox.min.css">
  
  <script src="../../../../js/pace.min.js"></script>
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <section class="outer">
  <article id="post-文献阅读一" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      文献阅读一
    </h1>
  

      </header>
    

    
      <div class="article-meta">
        <a href class="article-date">
  <time datetime="2019-07-11T01:49:00.000Z" itemprop="datePublished">2019-07-11</time>
</a>
        
  <div class="article-category">
    <a class="article-category-link" href="../../../../categories/文献/">文献</a>
  </div>

      </div>
    

    <div class="article-entry" itemprop="articleBody">
      

      

      
        <h3 id="Visual-Object-Tracking-for-Unmanned-Aerial-Vehicles"><a href="#Visual-Object-Tracking-for-Unmanned-Aerial-Vehicles" class="headerlink" title="Visual Object Tracking for Unmanned Aerial Vehicles:"></a><center>Visual Object Tracking for Unmanned Aerial Vehicles:</center></h3><h3 id="A-Benchmark-and-New-Motion-Models"><a href="#A-Benchmark-and-New-Motion-Models" class="headerlink" title="A Benchmark and New Motion Models"></a><center>A Benchmark and New Motion Models</center></h3><center><a href="http://lisiyi.me/paper/AAAI17_UAV.pdf" target="_blank" rel="noopener">原文地址</a></center><br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMTEvNWQyNjkwYmUxYTgyMTk3MTQwLnBuZw" alt="14338-66776-1-PB (1)-0.png"><br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMTEvNWQyNjkwYmZjN2RjOTI5MDc2LnBuZw" alt="14338-66776-1-PB (1)-1.png"><br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMTEvNWQyNjkwYmUxYjM2MzYzMjg3LnBuZw" alt="14338-66776-1-PB (1)-2.png"><br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMTEvNWQyNjkwYmU0YmNhYzE2NjA5LnBuZw" alt="14338-66776-1-PB (1)-3.png"><br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMTEvNWQyNjkwYmU2NGRkNzI4NjgzLnBuZw" alt="14338-66776-1-PB (1)-4.png"><br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMTEvNWQyNjkwYzA0ZGNjZTgzNjkxLnBuZw" alt="14338-66776-1-PB (1)-5.png"><br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMTEvNWQyNjkwYmU0YzRlZDI0OTk4LnBuZw" alt="14338-66776-1-PB (1)-6.png"><br><br>### <center>无人机视觉目标追踪：</center><br>###  <center>一个基准和新的运动模型</center><br>##### <center>作者：Siyi Li, Dit-Yan Yeung</center><br>##### <center>单位：香港科技大学计算机科学与工程学系</center><br>##### <center>邮箱：<a href="mailto:sliay@cse.ust.hk" target="_blank" rel="noopener">sliay@cse.ust.hk</a>,<a href="mailto:dyyeung@cse.ust.hk" target="_blank" rel="noopener">dyyeung@cse.ust.hk</a></center>

<h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>尽管最近视觉追踪<strong>领域</strong>有许多新的进展，但到目前为止大多数研究都聚焦在观测模型上。对于追踪系统的另一个重要的组成部分运动模型尤其是一些极端情况却很少有人涉足。本文，我们用组装在无人机或飞行器上的摄像头来考虑这样一种极端情景。我们建立了一个高度多样化的基准数据集，包含了由无人机摄像头捕捉到的70段视频影像。为了解决摄像头剧烈运动这样一个具有挑战性的问题，我们通过基于背景特征点的几何变换设计了一个简单的基线来将摄像头的运动模型化。最近最先进的追踪器为的广泛对比与这些追踪器在我们的无人机追踪数据集上的运动模型变化同时证实了数据集的必要性和所提出方法的有效性。我们做这个工作的目的是为在无人机追踪领域进一步的研究筑好基础。</p>
<h4 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h4><ul>
<li>视觉追踪是与许多现实应用<strong>有关</strong>的一个基础问题，这些应用包括视频监控，自动驾驶，人机交互以及更多。给出视频帧中目标物体的初始状态（例如位置和尺寸），追踪的目标是自动评估移动物体在后续帧中的状态。虽然视觉追踪已经被研究了几十年了，但由于大量的因素例如<strong>部分闭塞</strong>，物体快而突然的移动，照明变化，还有视角和姿势的大<strong>差异</strong>等等而遗留了一个具有挑战性的问题。   </li>
<li>最近几年我们见证了新型机器人，<strong>无人飞行器</strong>或无人机的进展。尽管在过去无人机大多被运用在军事应用上，但最近商业无人机革命也见证了越来越多的致力于研发小型、可购买的人性化的无人机的研究室。商业无人机的迅速发展可能会民间应用上有重大的影响，包括运输、通信。同时，在这个平台上一些可预见的应用将需要视觉追踪作为一个可行的核心科技。<strong>举几个例子</strong>，视觉追踪会对追踪动物、找人和监控实时交通状况等有用。</li>
<li>本文，我们在无人机平台上研究视觉追踪。除了一般的研究视觉追踪的共同问题，我们还要面对一个新的挑战即当使用无人机捕捉视频时频繁遇到摄像头突然移动。特别地，一个小的干扰例如摄像头的轻微旋转经常导致目标位置在图像场景中大的移位。同时，当无人机起飞时，它的运动通常会比许多传统的追踪应用有更高的<strong>自由度</strong>。因此，需要一个更复杂的运动模型。结果就是传统的运动模型对于带有固定的低速摄像机的追踪应用就不再适用了。本文的一个关注点是进行一个基准评估并且提出基线算法来明确估算<strong>自主运动</strong>。</li>
<li>本文的目标有三点：1. 用统计学的详细分析构造一个统一的无人机追踪基准数据集；2. 设计通用的基线算法来估算摄像机运动并将它们整合到不同的追踪系统中；3. 为了在视觉追踪领域开放一个新的研究方向这个目的，进行大量的实验对比并为视觉追踪模型提供基本见解。</li>
</ul>
<h4 id="Camera-Model"><a href="#Camera-Model" class="headerlink" title="Camera Model"></a>Camera Model</h4><ul>
<li>一款摄像机型描述了3D世界与2D平面图的一个映射。大量关于多视图几何学的摄像机型号已经被仔细地研究过了（Hartley and Zisserman 2003)。最广泛使用的一款是通用针孔摄像机型。Hoiem, Efros, and Hebert 2008 提出了一种简化的摄像机模型，假设所关注到的所有物体都停留在地面上。这个简化的摄像机模型已经被用来追踪地面物体例如汽车和行人（(Choi,Pantofaru, and Savarese 2013）。然而，所有的摄像机模型需要摄像机的初始化信息来推断物体的3维定位和校准摄像头。不幸的是，这些信息在许多物体追踪应用中并不容易获得。</li>
<li>在这里我们通过在2D图像平面中直接参数化摄像机来采用一种不同方法。我们注意到由于无人机上的摄像头通常离目标很远，我们可能会简单地忽略掉任何目标和背景线索之间深度的不同，并因此假设所捕捉到的框架可以被看成不同的平面目标。然后，从双视图几何的角度来看，这些平面通过射影变换联系在一起，射影变换也被称为二维同形。在数学上，让g<sub>t</sub>和g<sub>t-1</sub>分别代表t和t-1帧中静态特征点的齐次坐标。接下来我们就可以通过变化矩阵H来将相机模型参数化了。请注意因为我们介绍相机模型的主要目的是大概地为追踪指导一下搜索区域而不是决定精确的位置，上面的同形近似值在实际中效果不错。因此，我们只需要估算变换矩阵H。由于相机的初始信息不再需要，这个方法在其适用性上更为普遍。</li>
</ul>
<h4 id="Baseline-Method"><a href="#Baseline-Method" class="headerlink" title="Baseline Method"></a>Baseline Method</h4><ul>
<li>在传统的追踪方法上，只有目标移动被模型化。让z<sub>t</sub>和z<sub>t-1</sub>分别代表t和t-1帧中的目标坐标。运动模型就被简单地表示成：   <center> z<sub>t</sub>=z<sub>t-1</sub>+&Delta;z<sub>t</sub> </center><br>基于粒子过滤的方法采用高斯分布&Delta;z<sub>t</sub> 模型，而基于滑动窗的方法基于局部均匀分布的&Delta;z<sub>t</sub>模型。这个简单的移动模型在一般情况下效果不错。然而，在极端情况下例如无人机追踪，仅仅&Delta;z<sub>t</sub>模型是不够的。具体来说，假定一个小的&Delta;z<sub>t</sub>将会丢失下一个帧中的目标，而假定一个大的&Delta;z<sub>t</sub>将会增加漂移的风险。  </li>
<li>基于上面的相机假设，我们可以将新的运动模型表示成相机投影和目标运动的结合：<br><center> z<sub>t</sub>=H<sub>t</sub>z<sub>t-1</sub>+&Delta;z<sub>t</sub> </center><br>H<sub>t</sub>代表相机移动，&Delta;z<sub>t</sub>是由于目标移动而引起的位置替换。一旦我们对相机移动H<sub>t</sub>有了一个合理的评估，那么目标移动替换在局部地区也可以更精确地估算。</li>
<li>注意这个基线算法可以更轻易地被纳入到已有地追踪方法中。具体来说，我们首先通过特征点匹配来估算同形图H<sub>t</sub>（Fischler and Bolles 1981）。然后，之前的目标位置估算通过H<sub>t</sub>投影到当前地图像平面。对于基于滑动窗地追踪器，以转换后的目标坐标为中心地局部区域将会被搜索。对于基于粒子过滤器的追踪器，所有被维护的候选样本将会被转换到当前的图像平面中。除了这些修改，每一个追踪器还是以同样的方式工作。</li>
</ul>
<h4 id="Conclusion-and-Future-Work"><a href="#Conclusion-and-Future-Work" class="headerlink" title="Conclusion and Future Work"></a>Conclusion and Future Work</h4><ul>
<li>本文探讨了在无人机平台上进行视觉追踪的潜力。我们提出了一个统一的无人机追踪基准，它包含了由无人机摄像头捕捉到了一些影像。为了解决摄像头突然移动这个有挑战性的问题，我们通过基于背景特征线索的投影变换设计了简单基线来将相机移动模型化。我们将最先进的追踪器以及它们在无人机追踪基准上的移动模型变化进行了大量的对比。结果表明，通过明确地模型化相机移动，追踪器可以在所提出地移动模型下实现巨大的性能提升。</li>
<li>虽然我们提出的基线方法是有效的，但也确实存在一些失败的例子。例如，相机估算是基于传统的低级特征点检测，而低级特征点检测在某些情况下是嘈杂的甚至是错误的。如何设计卷积神经网络在视频数据上来学习更多的相机模型是一个有趣的问题。目前的基线方法中，相机估算是以脱机方式工作的。将相机估算和目标追踪结合在一个连贯的学习框架中希望可以有用。在未来工作中我们将致力于研究这些方面。</li>
</ul>
<p>重点部分完结，撒花～～～</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://sfz-lyq.cn/2019/07/11/文献阅读一/" data-id="ck5t5c5hp00dktds6k08u3suw" class="article-share-link">分享</a>
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../tags/文献/">文献</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../tags/翻译/">翻译</a></li></ul>

    </footer>

  </div>

  
    
  <nav class="article-nav">
    
      <a href="../../14/南航第一周总结/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            2019.7.14 每周总结
          
        </div>
      </a>
    
    
      <a href="../吴恩达机器学习第一次编程作业-线性回归/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">吴恩达机器学习第一次编程作业-线性回归</div>
      </a>
    
  </nav>


  

  
    
  

</article>



</section>
    </div>
    <script src="../../../../js/jquery-2.0.3.min.js"></script>
<script src="../../../../js/lazyload.min.js"></script>
<script src="../../../../js/busuanzi-2.3.pure.min.js"></script>


  <script src="../../../../fancybox/jquery.fancybox.min.js"></script>



  <script src="../../../../js/search.js"></script>


<script src="../../../../js/technology.js"></script>

  </div>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body>
</html>