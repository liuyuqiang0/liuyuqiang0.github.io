<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  
    <meta name="keywords" content="NO response">
  
  
    <meta name="description" content="what you will be,the world will be">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    Logistic 回归 |
    
    种花家</title>
  
    <link rel="shortcut icon" href="/images/rabbit.png">
  
  <link rel="stylesheet" href="../../../../css/style.css">
  <link rel="stylesheet" href="../../../../css/technology.css">
  
    <link rel="stylesheet" href="../../../../fancybox/jquery.fancybox.min.css">
  
  <script src="../../../../js/pace.min.js"></script>
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <section class="outer">
  <article id="post-Logistic-回归" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Logistic 回归
    </h1>
  

      </header>
    

    
      <div class="article-meta">
        <a href class="article-date">
  <time datetime="2019-07-15T01:00:00.000Z" itemprop="datePublished">2019-07-15</time>
</a>
        
  <div class="article-category">
    <a class="article-category-link" href="../../../../categories/机器学习/">机器学习</a>
  </div>

      </div>
    

    <div class="article-entry" itemprop="articleBody">
      

      

      
        <h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1051024622&amp;courseId=1004570029" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="分类classification"><a href="#分类classification" class="headerlink" title="分类classification"></a>分类classification</h4><p>例子   </p>
<ul>
<li>垃圾邮件分类</li>
<li>肿瘤良性还是恶性</li>
</ul>
<p>在这类预测问题中y是一个离散的值，只有{0,1}两种取值。<br>如果用线形回归模型计算分类，得出的h<sub>&theta;</sub>(x)可能远大于1或远小于0，而我们希望得到的是0或者1。所以，提出逻辑回归模型，让0&lt;= h<sub>&theta;</sub>(x) &lt;=1。逻辑回归实际是一个分类算法。  </p>
<h4 id="假设陈述"><a href="#假设陈述" class="headerlink" title="假设陈述"></a>假设陈述</h4><p>目的：当有一个分类问题时，用哪种方程表示假设函数。<br>逻辑回归模型要使得0&lt;= h<sub>&theta;</sub>(x) &lt;=1。<br>那么，令：h<sub>&theta;</sub>(x)=g(&theta;<sup>T</sup>x)<br>而，g(z)=1/(1+e<sup>-z</sup>)<br>则，h<sub>&theta;</sub>(x)=1/(1+e<sup>-&theta;<sup>T</sup>x</sup>)    </p>
<p>假设函数hypothesis意义：估算对于输入的x，y=1的概率是多少    </p>
<h4 id="决策界线"><a href="#决策界线" class="headerlink" title="决策界线"></a>决策界线</h4><ul>
<li>所谓决策界线就是讲将数据集划分为几个部分，当&theta;参数确定时很容易计算出边界线。   </li>
<li>边界线可能是直线也可能是曲线甚至是圆</li>
<li>关于边界线的问题，我们知道边界线是将数据样本点分为几部分，比如一条直线分两部分，直线上方代表1，下方代表0，那么在G(&theta;X)中，只要&theta;X&gt;=0，这个sigma函数就接近1。故只需边界线上点为0即可绘制出边界线了。如下图<br><img src="https://i.loli.net/2019/07/16/5d2ddb835203245861.png" alt="2019-07-16 22-12-36 的屏幕截图.png"></li>
</ul>
<h4 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h4><ol>
<li>先来拟合logistic回归模型的参数&theta;，那么就需要定义用来拟合参数的优化目标或者叫代价函数。  </li>
<li>如果选用线形回归模型中的代价函数会出现什么问题，在线形回归模型中假设函数h<sub>&theta;</sub>(x)是线形函数，而逻辑归回归模型中假设函数是非线形的，带入线形回归模型中所做出来的J(&theta;)是非凸函数，有很多局部最优解，而非线回归模型中碗状的凸函数。  </li>
<li>有了上面理论，我们将线形回归模型中(h<sub>&theta;</sub>(x)-y)<sup>2</sup>记为Cost  </li>
<li><p>应用于逻辑回归模型的代价函数Cost(h<sub>&theta;</sub>(x),y)=  </p>
<ul>
<li>-log( h<sub>&theta;</sub>(x) ) ， if y=1</li>
<li>-log( 1-h<sub>&theta;</sub>(x) ) ，if y=0  </li>
</ul>
</li>
<li><p>如何理解代价函数：代价函数描述了预测结果与样本实际的偏离程度。当y=1，我们的hypothesis也为1时，Cost为0，这是对的（无偏差）这就说明预测对了；而如果预测结果趋于0，那么Cost趋向正无穷，说明与实际偏差很大。对于y=0的情况也是同理。</p>
</li>
</ol>
<h4 id="简化代价函数与梯度下降"><a href="#简化代价函数与梯度下降" class="headerlink" title="简化代价函数与梯度下降"></a>简化代价函数与梯度下降</h4><p>逻辑回归代价函数J(&theta;)：注意y只有两个值故容易合并<br>关于梯度下降推导：<a href="https://www.jianshu.com/p/610859e27f66" target="_blank" rel="noopener">公式推导</a><br><img src="https://i.loli.net/2019/07/15/5d2bed8559bf636025.png" alt="2019-07-15 11-05-31 的屏幕截图.png"><br><img src="https://i.loli.net/2019/07/15/5d2bef64f0ea844193.png" alt="2019-07-15 11-13-25 的屏幕截图.png">  </p>
<ol>
<li>为什么这样选择代价函数，事实上我们知道代价函数有很多，这个是根据统计学的极大似然估计法得出来的，这个函数有一个特点是它是<strong>凸</strong>的。  </li>
<li>将J(&theta;)简化之后我们就可以尝试利用<strong>梯度下降</strong>来最小化它并得到最小化时的参数&theta;，再用参数来做预测。  </li>
<li>梯度下降如下：<br><img src="https://i.loli.net/2019/07/15/5d2c1dee422d076645.png" alt="2019-07-15 14-31-50 的屏幕截图.png"></li>
<li>线形回归中的梯度下降同样可以应用到逻辑回归模型中，使得梯度下降收敛更快 </li>
</ol>
<h4 id="高级优化"><a href="#高级优化" class="headerlink" title="高级优化"></a>高级优化</h4><p>计算代价函数并不只有梯度下降算法。还有：</p>
<ul>
<li>L-BFGS </li>
<li>BFGS</li>
<li>Conjugate gradient <a href="https://baike.baidu.com/item/%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6%E6%B3%95/7139204?fromtitle=Conjugate%20Gradient&amp;fromid=11232557&amp;fr=aladdin" target="_blank" rel="noopener">共轭梯度法</a><br>这些算法都不需要手动选择学习率&alpha;，也都比梯度下降算法快。但要比梯度下降更复杂。  </li>
</ul>
<p>这些高级算法一般封装在某个库中，可以直接调用。</p>
<h4 id="多元分类：一对多"><a href="#多元分类：一对多" class="headerlink" title="多元分类：一对多"></a>多元分类：一对多</h4><p>即： 待分类的对象 对 多个类别<br>比如：邮件，类别有家人、工作、朋友等</p>
<p>大致是对不同的类别设置不同的分类器：选择其中一种作为正，其余都为0，那么就可以得出不同的分类器，我们将数据输入到不同的分类器中选择hypothesis最大的最可信的那个。<br><img src="https://i.loli.net/2019/07/15/5d2c269d5004814353.png" alt="2019-07-15 15-09-03 的屏幕截图.png"><br><img src="https://i.loli.net/2019/07/15/5d2c26ea8ca8062647.png" alt="2019-07-15 15-10-02 的屏幕截图.png"></p>
<hr>
<hr>
<center>补充</center>

<hr>
<ol>
<li><a href="https://blog.csdn.net/sinat_34328764/article/details/80246139" target="_blank" rel="noopener">Python Matplotlib 改变坐标轴的默认位置</a></li>
<li><a href="http://file.sh.peixun.net/file/201812/05/2018120522460ukuzqukq7.pdf" target="_blank" rel="noopener">课件</a></li>
</ol>
<h3 id="sklearn中的逻辑回归"><a href="#sklearn中的逻辑回归" class="headerlink" title="sklearn中的逻辑回归"></a><font size="5px" color="red">sklearn中的逻辑回归</font></h3><h4 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a><font size="3px" color="red">Sigmoid函数</font></h4><p><img src="https://img-blog.csdnimg.cn/20191223145742591.jpg?aHR0cHM6Ly9hdmF0YXIuY3Nkbi5uZXQvNy83L0IvMV9yYWxmX2h4MTYzY29tLmpwZw =250x150" alt="Sigmoid函数"></p>
<ul>
<li>绘制函数<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">x=np.linspace(-10,10,10000)</span><br><span class="line">ax = plt.gca()  # a表示axis</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;) # 隐藏右边的线</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;) # 隐藏上面的线</span><br><span class="line">ax.spines[&apos;bottom&apos;].set_position((&apos;data&apos;, 0)) # 设置纵轴起点</span><br><span class="line">ax.spines[&apos;left&apos;].set_position((&apos;data&apos;, 0)) # 设置横轴中点</span><br><span class="line">plt.plot(x,1/(1+np.power(np.e,-x)))</span><br><span class="line">plt.grid(True)</span><br><span class="line">plt.title(&apos;Figmoid Function&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20191223145855618.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="sigmoid 函数"><br><code>Sigmoid</code>函数是一个S型的函数，当自变量z趋近正无穷时，因变量g(z)趋近于1，而当z趋近负无穷时，g(z)趋近于0，<strong>它能够将任何实数映射到(0,1)区间</strong>，使其可用于将任意值函数转换为更适合二分类的函数。因为这个性质，<strong>Sigmoid函数也被当作是归一化的一种方法</strong>，与我们之前学过的MinMaxSclaer同理，是属于数据预处理中的“<strong>缩放</strong>”功能，可以将数据压缩到[0,1]之内。区别在于，MinMaxScaler归一化之后，是可以取到0和1的（最大值归一化后就是1，最小值归一化后就是0），但<strong>Sigmoid函数只是无限趋近于0和1</strong>。</p>
<h4 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a><font size="3px" color="red">逻辑回归</font></h4><ol>
<li>逻辑回归对<strong>线性关系</strong>的拟合效果好到丧心病狂，相对的，逻辑回归在<strong>非线性数据</strong>的效果很多时候比瞎猜还不如，所以如果你已经知道数据之间的联系是非线性的，千万不要迷信逻辑回归</li>
<li><strong>逻辑回归计算快</strong></li>
<li><strong>逻辑回归返回的分类结果不是固定的0，1，而是以小数形式呈现的类概率数字</strong></li>
</ol>
<h4 id="sklearn中的逻辑回归-1"><a href="#sklearn中的逻辑回归-1" class="headerlink" title="sklearn中的逻辑回归"></a><font size="3px" color="red">sklearn中的逻辑回归</font></h4><table>
<thead>
<tr>
<th>逻辑回归相关的类</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>linear_model.LogisticRegression</td>
<td>逻辑回归回归分类器（又叫logit回归，最大熵分类器）</td>
</tr>
<tr>
<td>linear_model.LogisticRegressionCV</td>
<td>带交叉验证的逻辑回归分类器</td>
</tr>
<tr>
<td>linear_model.logistic_regression_path</td>
<td>计算Logistic回归模型以获得正则化参数的列表</td>
</tr>
<tr>
<td>linear_model.SGDClassifier</td>
<td>利用梯度下降求解的线性分类器（SVM，逻辑回归等等）</td>
</tr>
<tr>
<td>linear_model.SGDRegressor</td>
<td>利用梯度下降最小化正则化后的损失函数的线性回归模型</td>
</tr>
<tr>
<td>metrics.log_loss</td>
<td>对数损失，又称逻辑损失或交叉熵损失</td>
</tr>
<tr>
<td><strong>其他会涉及的类</strong></td>
<td><strong>说明</strong></td>
</tr>
<tr>
<td>metrics.confusion_matrix</td>
<td>混淆矩阵，模型评估指标之一</td>
</tr>
<tr>
<td>metrics.roc_auc_score</td>
<td>ROC曲线，模型评估指标之一</td>
</tr>
<tr>
<td><strong>metrics.accuracy_score</strong></td>
<td>精确性，模型评估指标之一</td>
</tr>
</tbody>
</table>
<h4 id="linear-model-LogisticRegression"><a href="#linear-model-LogisticRegression" class="headerlink" title="linear_model.LogisticRegression"></a><font size="3px" color="red">linear_model.LogisticRegression</font></h4><ul>
<li><code>sklearn.linear_model.LogisticRegression (penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1,   class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None)</code></li>
</ul>
<h4 id="重要参数-penalty-amp-C"><a href="#重要参数-penalty-amp-C" class="headerlink" title="重要参数 penalty &amp; C"></a><font size="3px" color="red">重要参数 penalty &amp; C</font></h4><ul>
<li><code>penalty</code>和<code>C</code>都是正则化参数，<strong>正则化</strong>是用来防止模型过拟合的过程。常用的有L1正则化和L2正则化两种选项，分别通过在损失函数后加上参数向量&theta;的L1范式和L2范式的倍数来实现。  </li>
<li>这个增加的范式，被称为“<strong>正则项</strong>”，也被称为”惩罚项”。其中L1范数表现为参数向量中的每个参数的绝对值之和，L2范数表现为参数向量中的每个参数的平方和的开方值。</li>
<li>损失函数<br><img src="https://img-blog.csdnimg.cn/20191223152842631.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="逻辑回归损失函数"><br>C是用来控制正则化程度的超参数，n是方程中特征的总数，也是方程中参数的总数，j代表每个参数。在这里，J要大于等于1，是因为我们的参数向量&theta;中，第一个参数是&theta;<sub>0</sub>是我们的截距，它通常是不参与正则化的<br><img src="https://img-blog.csdnimg.cn/20191223153110591.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="参数说明"></li>
</ul>
<h4 id="学习曲线选择C"><a href="#学习曲线选择C" class="headerlink" title="学习曲线选择C"></a><font size="3px" color="red">学习曲线选择C</font></h4><ul>
<li>逻辑回归的重要属性<code>coef_</code>，查看每个特征所对应的参数<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.linear_model import LogisticRegression as RL</span><br><span class="line">from sklearn.datasets import load_breast_cancer</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.metrics import accuracy_score</span><br><span class="line"></span><br><span class="line">data=load_breast_cancer()</span><br><span class="line">X=data.data</span><br><span class="line">Y=data.target</span><br><span class="line">print(X.shape,Y.shape) # (569, 30) (569,)</span><br><span class="line"></span><br><span class="line">RL_L1=RL(penalty=&apos;l1&apos;,C=0.5,max_iter=1000) # 实例化并初始化参数</span><br><span class="line">RL_L2=RL(penalty=&apos;l2&apos;,C=0.5,max_iter=1000) # 实例化并初始化参数</span><br><span class="line"></span><br><span class="line">RL_L1=RL_L1.fit(X,Y) # 训练模型</span><br><span class="line">RL_L2=RL_L2.fit(X,Y)</span><br><span class="line"></span><br><span class="line">print(RL_L1.coef_)</span><br><span class="line">print(RL_L2.coef_.shape) # (1, 30)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>可以看见，当选择L1正则化的时候，许多特征的参数都被设置为了0，这些特征在真正建模的时候，就不会出现在我们的模型当中了，而L2正则化则是对所有的特征都给出了参数。</p>
<ul>
<li>分别实例化L1和L2两种正则化方法并进行绘制学习曲线选择最佳C值<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.linear_model import LogisticRegression as RL</span><br><span class="line">from sklearn.datasets import load_breast_cancer</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.metrics import accuracy_score</span><br><span class="line"></span><br><span class="line">data=load_breast_cancer()</span><br><span class="line">X_train,X_test,y_train,y_test=train_test_split(data.data,data.target,test_size=0.3,random_state=10) # 30%数据作为测试集，其余作为训练集</span><br><span class="line"></span><br><span class="line">l1=[]</span><br><span class="line">l2=[]</span><br><span class="line">l1_test=[]</span><br><span class="line">l2_test=[]</span><br><span class="line">for i in np.linspace(0.05,1,19):  # C的可选值</span><br><span class="line">    RL_l1=RL(penalty=&apos;l1&apos;,C=i,max_iter=1000)</span><br><span class="line">    RL_l2=RL(penalty=&apos;l2&apos;,C=i,max_iter=1000)</span><br><span class="line"></span><br><span class="line">    RL_l1=RL_l1.fit(X_train,y_train)</span><br><span class="line">    l1.append(accuracy_score(RL_l1.predict(X_train),y_train))</span><br><span class="line">    l1_test.append(accuracy_score(RL_l1.predict(X_test),y_test))</span><br><span class="line"></span><br><span class="line">    RL_l2 = RL_l2.fit(X_train, y_train)</span><br><span class="line">    l2.append(accuracy_score(RL_l2.predict(X_train), y_train))</span><br><span class="line">    l2_test.append(accuracy_score(RL_l2.predict(X_test), y_test))</span><br><span class="line"></span><br><span class="line">graph=[l1,l2,l1_test,l2_test]</span><br><span class="line">color = [&quot;green&quot;,&quot;black&quot;,&quot;lightgreen&quot;,&quot;gray&quot;]</span><br><span class="line">label = [&quot;l1&quot;,&quot;l2&quot;,&quot;l1test&quot;,&quot;l2test&quot;]</span><br><span class="line">for i in range(len(graph)):</span><br><span class="line">    plt.plot(np.linspace(0.05,1,19),graph[i],color[i],label=label[i])</span><br><span class="line"></span><br><span class="line">plt.legend(loc=&apos;best&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20191223160326919.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="学习曲线"><br>可见，至少在我们的乳腺癌数据集下，两种正则化的结果区别不大。但随着C的逐渐变大，正则化的强度越来越小，模型在训练集和测试集上的表现都呈上升趋势，直到C=0.8左右，训练集上的表现依然在走高，但模型在未知数据集上的表现开始下跌，这时候就是出现了过拟合。我们可以认为，C设定为0.9会比较好。在实际使用时，基本就默认使用l2正则化，如果感觉到模型的效果不好，那就换L1试试看。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://sfz-lyq.cn/2019/07/15/Logistic-回归/" data-id="ck5c8g3x100e74zpvv7y2evei" class="article-share-link">分享</a>
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../tags/机器学习/">机器学习</a></li></ul>

    </footer>

  </div>

  
    
  <nav class="article-nav">
    
      <a href="../正则化/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            正则化
          
        </div>
      </a>
    
    
      <a href="../../14/南航第一周总结/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">2019.7.14 每周总结</div>
      </a>
    
  </nav>


  

  
    
  

</article>



</section>
    </div>
    <script src="../../../../js/jquery-2.0.3.min.js"></script>
<script src="../../../../js/lazyload.min.js"></script>
<script src="../../../../js/busuanzi-2.3.pure.min.js"></script>


  <script src="../../../../fancybox/jquery.fancybox.min.js"></script>



  <script src="../../../../js/search.js"></script>


<script src="../../../../js/technology.js"></script>

  </div>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body>
</html>