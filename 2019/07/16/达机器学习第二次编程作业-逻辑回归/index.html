<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  
    <meta name="keywords" content="NO response">
  
  
    <meta name="description" content="what you will be,the world will be">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    吴恩达机器学习第二次编程作业-逻辑回归 |
    
    种花家</title>
  
    <link rel="shortcut icon" href="/images/rabbit.png">
  
  <link rel="stylesheet" href="../../../../css/style.css">
  <link rel="stylesheet" href="../../../../css/technology.css">
  
    <link rel="stylesheet" href="../../../../fancybox/jquery.fancybox.min.css">
  
  <script src="../../../../js/pace.min.js"></script>
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <section class="outer">
  <article id="post-达机器学习第二次编程作业-逻辑回归" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      吴恩达机器学习第二次编程作业-逻辑回归
    </h1>
  

      </header>
    

    
      <div class="article-meta">
        <a href class="article-date">
  <time datetime="2019-07-16T09:10:00.000Z" itemprop="datePublished">2019-07-16</time>
</a>
        
  <div class="article-category">
    <a class="article-category-link" href="../../../../categories/机器学习/">机器学习</a>
  </div>

      </div>
    

    <div class="article-entry" itemprop="articleBody">
      

      

      
        <h3 id="课时60编程作业：逻辑回归"><a href="#课时60编程作业：逻辑回归" class="headerlink" title="课时60编程作业：逻辑回归"></a><center><a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/text?lessonId=1053422066&amp;courseId=1004570029" target="_blank" rel="noopener">课时60编程作业：逻辑回归</a></center></h3><p>工具：pycharm2019.1，Python3.5  </p>
<p>参考资料：<a href="https://blog.csdn.net/xiaoxiangzi222/article/details/55097570" target="_blank" rel="noopener"> Logistic回归总结</a>、<a href="https://blog.csdn.net/huahuaxiaoshao/article/details/85174312" target="_blank" rel="noopener">梯度下降求解逻辑回归
</a>、<a href="https://blog.csdn.net/CoderPai/article/details/83384499" target="_blank" rel="noopener">数据提取</a>、<a href="https://www.jianshu.com/p/b479ec2da7c2" target="_blank" rel="noopener">Python 不同颜色可视化数据集</a>、<a href="https://jingyan.baidu.com/article/0eb457e508b6d303f0a90572.html" target="_blank" rel="noopener">Dataframe筛选数据</a>、<a href="https://blog.csdn.net/qq_30163461/article/details/80080529" target="_blank" rel="noopener">DataFrame类型转换成array类型</a>  、<a href="https://www.cnblogs.com/dan-baishucaizi/p/9398801.html" target="_blank" rel="noopener">pandas数据读写</a></p>
<p><a href="https://paste.ubuntu.com/p/w992jdTZNw/" target="_blank" rel="noopener">逻辑回归完整代码</a><br><a href="https://paste.ubuntu.com/p/8F6KP3xk3M/" target="_blank" rel="noopener">正则化完整代码</a></p>
<hr>
<h4 id="1-逻辑回归"><a href="#1-逻辑回归" class="headerlink" title="1. 逻辑回归"></a>1. 逻辑回归</h4><p>任务：建立一个逻辑回归分类模型来预测一个学生是否能被大学录取</p>
<h5 id="1-1-数据可视化"><a href="#1-1-数据可视化" class="headerlink" title="1.1 数据可视化"></a>1.1 数据可视化</h5><p>先利用pandas库提取数据，然后分类，绘制。<br>pandas需要手动添加一行表示列名，不然以第一行数据作为列名，数据类型自动识别。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def pandas_getData():</span><br><span class="line">    data = pd.read_csv(&apos;./ex2/ex2data1.txt&apos;)</span><br><span class="line">    y = data[&apos;y&apos;]   # 按列名提取</span><br><span class="line">    data1 = data[y == 0]  # 数据分类</span><br><span class="line">    data2 = data[y == 1]  # 已录取</span><br><span class="line">    return data,data1,data2,data.shape[0]</span><br><span class="line"></span><br><span class="line">def plot_data(data1,data2):</span><br><span class="line">    plt.plot(data1[&apos;x1&apos;],data1[&apos;x2&apos;],&apos;yo&apos;,label=&apos;Not admitted&apos;)</span><br><span class="line">    plt.plot(data2[&apos;x1&apos;],data2[&apos;x2&apos;],&apos;k+&apos;,label=&apos;Admitted&apos;)</span><br><span class="line">    plt.xlim(30,101)</span><br><span class="line">    plt.ylim(30,100)</span><br><span class="line">    plt.xlabel(&apos;Exam 1 score&apos;,fontsize=10)</span><br><span class="line">    plt.ylabel(&apos;Exam 2 score&apos;, fontsize=10)</span><br><span class="line">    plt.legend(loc=1,fontsize=8)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://i.loli.net/2019/07/16/5d2d6e90a260a72174.png" alt="Figure_1.png"></p>
<h5 id="1-2-Cost-function-and-gradient"><a href="#1-2-Cost-function-and-gradient" class="headerlink" title="1.2 Cost function and gradient"></a>1.2 Cost function and gradient</h5><p><img src="https://i.loli.net/2019/07/16/5d2d7269b6c2f65646.png" alt="2019-07-16 14-44-13 的屏幕截图.png">  </p>
<p>关于为什么学习率&alpha;这里不用除以m均值化在参考资料中有回答说&alpha;是个常数，可以省掉m，本人不是很懂。<br>代价函数与梯度下降只要有线形回归的基础就很好计算：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def Cost_Function(x,y,theta,m):</span><br><span class="line">    hypothesis=1/(1+np.exp(-np.dot(x,theta)))   # 假设函数</span><br><span class="line">    Cost_J =-1/m*(y*np.log(hypothesis)+(1-y)*np.log(1-hypothesis)).sum()</span><br><span class="line">    return Cost_J</span><br><span class="line"></span><br><span class="line">def Gradient_descent(x,y,theta,m,alpha,iterations):</span><br><span class="line">    for iteration in range(iterations):</span><br><span class="line">        hypothesis=1/(1+np.exp(-np.dot(x,theta)))</span><br><span class="line">        theta=theta-alpha*np.dot(x.T,hypothesis-y)</span><br><span class="line">    return theta   # 我们只需要最终的参数theta来进行预测</span><br></pre></td></tr></table></figure></p>
<p>用梯度下降的方法来计算参数&theta;还需要初始化参数init_theta、&alpha;以及迭代次数MaxIter，也别忘了X需要增加一列1表示x0。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data,data1,data2,m=pandas_getData()  # 获取数据</span><br><span class="line"># plot_data(data1,data2)  # 绘制数据</span><br><span class="line">x=np.array(data[[&apos;x1&apos;,&apos;x2&apos;]])  # 将DataFrame类型转换成array类型</span><br><span class="line">y=np.array(data[&apos;y&apos;]).reshape(m,1)</span><br><span class="line">x=np.hstack((np.ones((m,1)),x))   # 增加一列 X0=1</span><br><span class="line">init_theta=np.zeros((x.shape[1],1)) # 初始化theta</span><br><span class="line">MaxIter=2000000  # 迭代次数</span><br><span class="line">alpha=0.00001 # 学习率</span><br><span class="line">print(&apos;初始代价函数为：&apos;,Cost_Function(x,y,init_theta,m)) # 这里如果代价函数没写错计算出来的值应该是0.6931471805599453</span><br></pre></td></tr></table></figure></p>
<ul>
<li>在源文档中给出的求参数的方法是用Octave调库直接得出，网上找到的很多博客也是用scipy库函数。毫无疑问库中封装了最简介高效的方法。但如果我就想使用梯度下降呢？  </li>
<li>本人已经试过了，需要不停的调参（&alpha;和MaxIter），这对于机器性能是个考验。用库函数得出的最优参数是：[-25.16131867,   0.20623159,   0.20147149]，用这个参数向量求出的代价函数值为：0.2034977015894744。</li>
<li>本人在不停的调参过程中发现当我把学习率&alpha;设置为0.00001时，再一直增大迭代次数发现实验结果越来越接近文档给出的最优值，考虑到机器性能我只增加到了200000次，梯度下降得出的参数&theta;向量为：[-19.08468795   0.15767916   0.15228738]，用此参数求出的代价函数值为：0.21041324594328958</li>
<li>有了参数&theta;就可以用来进行预测和绘制决策边界线了</li>
</ul>
<h5 id="预测和决策边界线"><a href="#预测和决策边界线" class="headerlink" title="预测和决策边界线"></a>预测和决策边界线</h5><ol>
<li>还记得之前说过h<sub>&theta;</sub>(x)=1/(1+e<sup>-&theta;<sup>T</sup>X</sup>))，这个函数用来预测分类y=1概率。</li>
<li>我们可以设定阈值为0.5，只要h&gt;=0.5，那么就判定这是属于1，否则属于0。</li>
<li>上面是具体计算方法，我们可以有两种形式来预测：一种是用给定的样本X看看其h函数概率；还有一种是直接用训练样本跑，看有多少个正确分类了，这个百分比越高说明预测越准确。</li>
<li>以上是预测方法，那么关于决策边界线，这个在吴恩达机器学习视频中已经给出。没有实践很容易忽略这点，我们要观察h函数，它是一个sigma函数，只要&theta;<sup>T</sup>X的值大于等于0，那么h函数值就大于等于0.5，想想看是不是。而X向量是样本点，这条分界线就是若干个样本点，这些样本点所计算出来的&theta;<sup>T</sup>X=&theta;<sub>0</sub>+&theta;<sub>1</sub>X<sub>1</sub>+… 值应该等于0，两侧的点要么大于0计算出来的h值就大于0.5，反之同理。</li>
<li>用文档给出的样本[1 45 85]预测的概率为0.7221802，文档给出的概率为0.776。看来梯度下降算法确实不如其他算法，唯一的优点可能就是简单了。</li>
<li><p>先用训练样本来看看实验效果如何：  89.00%</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def text_sample(x,y,theta,m):</span><br><span class="line">    hypothesis=1/(1+np.exp(-np.dot(x,theta)))</span><br><span class="line">    cnt_correct=0</span><br><span class="line">    for i in range(m):</span><br><span class="line">        if hypothesis[i][0]&gt;=0.5 and y[i][0]==1:cnt_correct=cnt_correct+1 # 正确预测1</span><br><span class="line">        if hypothesis[i][0]&lt;0.5 and y[i][0]==0:cnt_correct=cnt_correct+1 # 正确预测0</span><br><span class="line">    correct_percent=cnt_correct/m  # 计算准确率</span><br><span class="line">    print(&apos;%.2f%%&apos; %(correct_percent*100))</span><br></pre></td></tr></table></figure>
</li>
<li><p>绘制决策边界，我们已经知道决策边界线的&theta;<sup>T</sup>X=&theta;<sub>0</sub>+&theta;<sub>1</sub>X<sub>1</sub>+…&theta;<sub>n</sub>X<sub>n</sub>=0，数据图的横轴是X1，纵轴是X2，那么随便造几百个X1，根据此公式计算X2再绘制直线即可。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def plotDecisionBoundary(data1,data2,x,y,theta):</span><br><span class="line">    plt.plot(data1[&apos;x1&apos;], data1[&apos;x2&apos;], &apos;yo&apos;, label=&apos;Not admitted&apos;)</span><br><span class="line">    plt.plot(data2[&apos;x1&apos;], data2[&apos;x2&apos;], &apos;k+&apos;, label=&apos;Admitted&apos;)</span><br><span class="line">    plt.xlim(30, 101)</span><br><span class="line">    plt.ylim(30, 100)</span><br><span class="line">    plt.xlabel(&apos;Exam 1 score&apos;, fontsize=10)</span><br><span class="line">    plt.ylabel(&apos;Exam 2 score&apos;, fontsize=10)</span><br><span class="line">    plt.legend(loc=1, fontsize=8)</span><br><span class="line"></span><br><span class="line">    x1 = np.arange(130, step=0.1)</span><br><span class="line">    x2 = -(final_theta[0] + x1 * final_theta[1]) / final_theta[2]</span><br><span class="line">    plt.plot(x1, x2)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><img src="https://i.loli.net/2019/07/17/5d2e7705009fa32467.png" alt="Figure_2.png"></p>
<p>至此，简单逻辑回归分类模型已经完成，下面是第二部分-正则化。</p>
<h4 id="2-正则化"><a href="#2-正则化" class="headerlink" title="2. 正则化"></a>2. 正则化</h4><p>芯片质检问题，两轮检测，一个结果，建立逻辑回归模型。</p>
<h5 id="2-1-绘制数据"><a href="#2-1-绘制数据" class="headerlink" title="2.1 绘制数据"></a>2.1 绘制数据</h5><p>和1.1一样，使用Pandas库提取数据，然后按y=0/1分类，传参绘制<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib</span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">def Get_data():</span><br><span class="line">    data=pd.read_csv(&apos;./ex2/ex2data2.txt&apos;,names=[&apos;x1&apos;,&apos;x2&apos;,&apos;y&apos;]) # setting header or default setting header=None</span><br><span class="line">    data1=data[data[&apos;y&apos;]==1] # accepted</span><br><span class="line">    data2=data[data[&apos;y&apos;]==0] # rejected</span><br><span class="line">    print(data1.shape,data2.shape)</span><br><span class="line">    return data,data1,data2</span><br><span class="line"></span><br><span class="line">def Plot_Data(data1,data2):</span><br><span class="line">    plt.plot(data1[&apos;x1&apos;],data1[&apos;x2&apos;],&apos;k+&apos;,label=&apos;y=1&apos;)</span><br><span class="line">    plt.plot(data2[&apos;x1&apos;],data2[&apos;x2&apos;],&apos;yo&apos;,label=&apos;y=0&apos;)</span><br><span class="line">    plt.xlim(-1,1.5)</span><br><span class="line">    plt.ylim(-0.8,1.2)</span><br><span class="line">    plt.xlabel(&apos;Microchip Test 1&apos;,fontsize=10)</span><br><span class="line">    plt.ylabel(&apos;Microchip Test 2&apos;,fontsize=10)</span><br><span class="line">    plt.legend(loc=&apos;upper right&apos;,fontsize=8)</span><br><span class="line">    plt.show()</span><br><span class="line">data,data1,data2=Get_data()</span><br><span class="line">Plot_Data(data1,data2)</span><br></pre></td></tr></table></figure></p>
<p><img src="https://i.loli.net/2019/07/19/5d3168bc6cb0581582.png" alt="Figure_3.png"></p>
<h5 id="2-2-Feature-mapping-特征扩展"><a href="#2-2-Feature-mapping-特征扩展" class="headerlink" title="2.2  Feature mapping 特征扩展"></a>2.2  Feature mapping 特征扩展</h5><p>特征越多，维数越高，所作出的曲线拟合得也更好（第8章课程总结第8页）。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def mapFeature(x1, x2,m):</span><br><span class="line">    out=np.ones((m,1))</span><br><span class="line">    degree = 6</span><br><span class="line">    for i in range(1,degree+1):</span><br><span class="line">        for j in range(0,i+1):</span><br><span class="line">            out=np.hstack((out,((x1**(i-j))*(x2**j)).reshape(m,1)))</span><br><span class="line">    return out</span><br></pre></td></tr></table></figure></p>
<h5 id="2-3-Cost-function-and-gradient"><a href="#2-3-Cost-function-and-gradient" class="headerlink" title="2.3 Cost function and gradient"></a>2.3 Cost function and gradient</h5><p><img src="https://i.loli.net/2019/07/19/5d317815857a477982.png" alt="2019-07-19 15-57-31 的屏幕截图.png"><br><img src="https://i.loli.net/2019/07/19/5d31781572b6617121.png" alt="2019-07-19 15-57-35 的屏幕截图.png"><br>根据上面的公式就可以计算了。<br>先初始化：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data,data1,data2,m=Get_data()</span><br><span class="line"># Plot_Data(data1,data2)</span><br><span class="line">X=np.array(data[[&apos;x1&apos;,&apos;x2&apos;]])</span><br><span class="line">X=mapFeature(X.T[0],X.T[1],m)</span><br><span class="line">Y=np.array(data[&apos;y&apos;]).reshape(m,1)</span><br><span class="line">init_theta=np.zeros((X.shape[1],1))</span><br><span class="line">lamda=1</span><br><span class="line">alpha=0.0001</span><br><span class="line">MaxIter=2000000</span><br></pre></td></tr></table></figure></p>
<p>代价函数：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def Cost_Function(X,Y,init_theta,m,lamda):</span><br><span class="line">    hypothesis=1/(1+np.exp(-X.dot(init_theta)))</span><br><span class="line">    Cost_J=-1/m*(Y*np.log(hypothesis)+(1-Y)*np.log(1-hypothesis)).sum()</span><br><span class="line">    Cost_J=Cost_J+lamda/(2*m)*(init_theta**2).sum()</span><br><span class="line">    return Cost_J</span><br></pre></td></tr></table></figure></p>
<p>梯度下降：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def Gradient_descent(X,Y,theta,m,alpha,iterations,lamda):</span><br><span class="line">    for i in range(iterations):</span><br><span class="line">        tmp=theta[0][0] # theta0单独处理</span><br><span class="line">        theta=theta-alpha/m*(np.dot(X.T,(1/(1+np.exp(-X.dot(theta)))-Y))+lamda*theta)</span><br><span class="line">        theta[0][0]=tmp+alpha*lamda/m</span><br><span class="line">    return theta</span><br></pre></td></tr></table></figure></p>
<h5 id="2-4-绘制决策边界线"><a href="#2-4-绘制决策边界线" class="headerlink" title="2.4 绘制决策边界线"></a>2.4 绘制决策边界线</h5><p>这里是用等高线的方法作决策边界线，随机生成x1和x2，然后再用特征扩展方法扩展维数与&theta;向量相乘。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def plot_Decision_boundary(data1,data2,theta):</span><br><span class="line">    plt.plot(data1[&apos;x1&apos;], data1[&apos;x2&apos;], &apos;k+&apos;, label=&apos;y=1&apos;)</span><br><span class="line">    plt.plot(data2[&apos;x1&apos;], data2[&apos;x2&apos;], &apos;yo&apos;, label=&apos;y=0&apos;)</span><br><span class="line">    plt.xlim(-1, 1.5)</span><br><span class="line">    plt.ylim(-0.8, 1.2)</span><br><span class="line">    plt.xlabel(&apos;Microchip Test 1&apos;, fontsize=10)</span><br><span class="line">    plt.ylabel(&apos;Microchip Test 2&apos;, fontsize=10)</span><br><span class="line">    plt.title(&apos;lambda = 1&apos;,fontsize=15)</span><br><span class="line">    point_num=50</span><br><span class="line">    x1=np.linspace(-1,1.5,point_num)</span><br><span class="line">    x2=np.linspace(-1,1.5,point_num)</span><br><span class="line">    z=np.zeros((point_num,point_num))</span><br><span class="line">    for i in range(point_num):</span><br><span class="line">        for j in range(point_num):</span><br><span class="line">            z[i][j]=mapFeature(x1[i],x2[j],1).dot(theta)</span><br><span class="line">    plt.contour(x1,x2,z)</span><br><span class="line">    plt.legend(loc=&apos;upper right&apos;, fontsize=8)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://i.loli.net/2019/07/19/5d31ad261d6e455524.png" alt="Figure_4.png"></p>
<p>由于这里的参数&theta;以及学习率分别用梯度下降跑出来的和自己设置的，与用库函数跑出来的最优值不同，所以作出的效果与文档也有差别。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://sfz-lyq.cn/2019/07/16/达机器学习第二次编程作业-逻辑回归/" data-id="ck6bxbgkw00g582s6v67wwddd" class="article-share-link">分享</a>
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../tags/Python/">Python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../tags/机器学习/">机器学习</a></li></ul>

    </footer>

  </div>

  
    
  <nav class="article-nav">
    
      <a href="../../18/吉首大学2019年程序设计竞赛-部分题解/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            吉首大学2019年程序设计竞赛 部分题解
          
        </div>
      </a>
    
    
      <a href="../../15/正则化/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">正则化</div>
      </a>
    
  </nav>


  

  
    
  

</article>



</section>
    </div>
    <script src="../../../../js/jquery-2.0.3.min.js"></script>
<script src="../../../../js/lazyload.min.js"></script>
<script src="../../../../js/busuanzi-2.3.pure.min.js"></script>


  <script src="../../../../fancybox/jquery.fancybox.min.js"></script>



  <script src="../../../../js/search.js"></script>


<script src="../../../../js/technology.js"></script>

  </div>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body>
</html>