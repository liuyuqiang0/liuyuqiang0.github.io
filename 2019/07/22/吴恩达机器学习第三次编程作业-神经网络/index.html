<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  
    <meta name="keywords" content="NO response">
  
  
    <meta name="description" content="what you will be,the world will be">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    吴恩达机器学习第三次编程作业-多元分类与神经网络 |
    
    种花家</title>
  
    <link rel="shortcut icon" href="/images/rabbit.png">
  
  <link rel="stylesheet" href="../../../../css/style.css">
  <link rel="stylesheet" href="../../../../css/technology.css">
  
    <link rel="stylesheet" href="../../../../fancybox/jquery.fancybox.min.css">
  
  <script src="../../../../js/pace.min.js"></script>
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <section class="outer">
  <article id="post-吴恩达机器学习第三次编程作业-神经网络" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      吴恩达机器学习第三次编程作业-多元分类与神经网络
    </h1>
  

      </header>
    

    
      <div class="article-meta">
        <a href class="article-date">
  <time datetime="2019-07-22T08:08:00.000Z" itemprop="datePublished">2019-07-22</time>
</a>
        
  <div class="article-category">
    <a class="article-category-link" href="../../../../categories/机器学习/">机器学习</a>
  </div>

      </div>
    

    <div class="article-entry" itemprop="articleBody">
      

      

      
        <h3 id="课时67编程作业：多元分类与神经网络"><a href="#课时67编程作业：多元分类与神经网络" class="headerlink" title="课时67编程作业：多元分类与神经网络"></a><center><a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/text?lessonId=1053425062&amp;courseId=1004570029" target="_blank" rel="noopener">课时67编程作业：多元分类与神经网络</a></center></h3><p>工具：Pycharm2019.01，Python3.5  </p>
<p>参考资料：<a href="https://sfz-lyq.cn/2019/07/23/SciPy%E5%BA%93%E5%AD%A6%E4%B9%A0/#more">Python库之SciPy</a>，<a href="https://blog.csdn.net/weixin_44750583/article/details/88603512" target="_blank" rel="noopener">逻辑回归解决多元分类</a>，<a href="https://blog.csdn.net/weixin_39223665/article/details/79675927" target="_blank" rel="noopener"><br>Python3 求最大/小值及索引值 Numpy</a>，<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_ncg.html#scipy.optimize.fmin_ncg" target="_blank" rel="noopener">scipy.optimize.fmin_ncg</a>，<a href="https://docs.scipy.org/doc/scipy/reference/optimize.html" target="_blank" rel="noopener">scipy.optimize</a>，<a href="https://blog.csdn.net/u011851421/article/details/83783826" target="_blank" rel="noopener">numpy矩阵乘法运算</a></p>
<p><a href="https://paste.ubuntu.com/p/N2m5CXTNCc/" target="_blank" rel="noopener">多元分类完整代码: fmin_ncg</a><br><a href="https://paste.ubuntu.com/p/fTC64VRr8P/" target="_blank" rel="noopener">多元分类完整代码: fmin_cg</a><br><a href="https://paste.ubuntu.com/p/dwMMj7RYNZ/" target="_blank" rel="noopener">神经网络前向传播算法完整代码</a></p>
<p>注：本章所用参考资料均已给出</p>
<hr>
<p>本次练习实现一对多逻辑回归与神经网络来识别手写数字。</p>
<h4 id="Multi-class-Classification"><a href="#Multi-class-Classification" class="headerlink" title="Multi-class Classification"></a>Multi-class Classification</h4><h5 id="数据集格式"><a href="#数据集格式" class="headerlink" title="数据集格式"></a>数据集格式</h5><p>本次练习数据集格式为.mat类型，故采用Scipy.io.loadmat方式读入数据，以字典dictionary形式存在，其中有两个键’X’和’y’分别代表训练样本输入和输出，对应值是以ndarray形式存在。<br>其中，输入是一个(5000,400)的数组，400表示数字是20*20像素的，被拉伸为400维一行。<br>获取数据：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def Get_Data():</span><br><span class="line">    data=sio.loadmat(&apos;./ex3/ex3data1.mat&apos;) </span><br><span class="line">    # print(type(data)) # dict</span><br><span class="line">    # for key in data: # 如果不清楚文件内容，可以打印出来看看</span><br><span class="line">    #     print(key,&apos;:&apos;,data[key])</span><br><span class="line">    return data[&apos;X&apos;],data[&apos;y&apos;],data[&apos;y&apos;].shape[0] # 输入，输出，以及样本数</span><br></pre></td></tr></table></figure></p>
<h5 id="Visualizing-the-data"><a href="#Visualizing-the-data" class="headerlink" title="Visualizing the data"></a>Visualizing the data</h5><p>由于数据集过大，故随机选取100个样本输入打印出来。<br>每次从<code>[0,5000)</code>中随机取一个数：<code>rand_choose=np.random.random_integers(0,m)</code>    </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def Plot_Data(X,y,m): # random choose 100 samples to plot it</span><br><span class="line">    fig=plt.figure(figsize=(6,6)) # 新建画布并指定size</span><br><span class="line">    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05) # 指定显示比例</span><br><span class="line">    for i in range(1,101): # 区域编号从1开始</span><br><span class="line">        ax = fig.add_subplot(10, 10, i, xticks=[], yticks=[])</span><br><span class="line">        rand_choose=np.random.random_integers(0,m) </span><br><span class="line">     ax.imshow(X[rand_choose].reshape(20,20).T,cmap=plt.cm.binary,interpolation=&apos;nearest&apos;)</span><br><span class="line">        # label the image with the target value</span><br><span class="line">        ax.text(0,20, str(y[rand_choose])) # 显示真实数字</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/07/25/5d399e37023c323968.png" alt="Figure_11.png"></p>
<h5 id="Vectorizing-Logistic-Regression"><a href="#Vectorizing-Logistic-Regression" class="headerlink" title="Vectorizing Logistic Regression"></a>Vectorizing Logistic Regression</h5><p>主要是定义代价函数与梯度下降，然后调用scipy.optimize库方法跑出参数&theta;，最后利用参数进行预测。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def Cost_Func(theta,X,Y,m): # 原来的代价函数</span><br><span class="line">    hypothesis=1/(1+np.exp(-X@theta)) # 假设函数</span><br><span class="line">    Cost_J=-1/m*((Y@np.log(hypothesis)+(1-Y)@np.log(1-hypothesis)))</span><br><span class="line">    return Cost_J</span><br><span class="line"></span><br><span class="line">def Cost_reg(theta,X,Y,m,lamda): # 正则化代价函数，加上正则项</span><br><span class="line">    Cost_J=Cost_Func(theta,X,Y,m)</span><br><span class="line">    theta_tmp=theta[1:] # 不惩罚theta[0]</span><br><span class="line">    Cost_J=Cost_J+lamda/(2*m)*np.sum(theta_tmp*theta_tmp)</span><br><span class="line">    return Cost_J</span><br><span class="line"></span><br><span class="line">def Gredient_Dec(theta,X,Y,m): # 原来的梯度函数</span><br><span class="line">    hypothesis=1/(1+np.exp(-X.dot(theta))) </span><br><span class="line">    partial_derivatives=1/m*np.dot(X.T,hypothesis-Y) # 偏导数</span><br><span class="line">    return partial_derivatives</span><br><span class="line"></span><br><span class="line">def Gredient_reg(theta,X,Y,m,lamda): # 正则化梯度函数</span><br><span class="line">    partial_derivations=Gredient_Dec(theta,X,Y,m) </span><br><span class="line">    theta_tmp=lamda/m*theta</span><br><span class="line">    theta_tmp[0]=0 # 不惩罚theta[0]</span><br><span class="line">    return partial_derivations+theta_tmp</span><br></pre></td></tr></table></figure></p>
<h5 id="One-vs-all-Classification"><a href="#One-vs-all-Classification" class="headerlink" title="One-vs-all Classification"></a>One-vs-all Classification</h5><ul>
<li>将复杂的分类问题简化为若干个二分类任务，最经典的拆分策略有三种：一对一（OvO）、一对其余（OvR）和多对多（MvM）。  </li>
<li>这里的分类问题需要10个分类器，故最终目标需要’训练’出10个&theta;参数向量。而在训练时，使用逻辑回归方法，将待分类的类别当作1，其余为0。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def One_vs_All_classes(X,Y,theta,labda,m,K):</span><br><span class="line">    y=Y.ravel()</span><br><span class="line">    classes_y = np.zeros(m).astype(int)</span><br><span class="line">    for i in range(K):</span><br><span class="line">        classes_y[y!=i]=0 # 不属于此类设为0，否则为1</span><br><span class="line">        classes_y[y==i]=1 # 变成了简单的逻辑回归问题</span><br><span class="line">        # print(i,sum(classes_y==1),sum(y==i))</span><br><span class="line">        result=opt.fmin_ncg(f=Cost_reg,fprime=Gredient_reg,x0=theta[:,i:i+1],args=(X,classes_y,m,lamda),maxiter=400) # 使用牛顿迭代的方法</span><br><span class="line">        theta[:,i:i+1]=np.mat(result).T # 返回的result是一个向量(401,)</span><br><span class="line">    return theta</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="One-vs-all-Prediction"><a href="#One-vs-all-Prediction" class="headerlink" title="One-vs-all Prediction"></a>One-vs-all Prediction</h5><p>预测，使用参数&theta;向量评测每个样本，取H<sub>&theta;</sub>(X)最大的那个作为预测值。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def One_vs_All_prediction(X,Y,fin_theta,m):</span><br><span class="line">    # print(X.shape,fin_theta.shape)</span><br><span class="line">    predict_matrix=1/(1+np.exp(-X.dot(fin_theta)))</span><br><span class="line">    cnt=0</span><br><span class="line">    for i in range(m):</span><br><span class="line">        if np.argmax(predict_matrix[i])==y[i][0]:</span><br><span class="line">            cnt=cnt+1</span><br><span class="line">    print(&apos;%.2f%%&apos; %(cnt/m*100))</span><br></pre></td></tr></table></figure></p>
<p>在本机上跑了大概10分钟？<br>预测结果: 96.46%  </p>
<h5 id="不同的优化函数对比情况"><a href="#不同的优化函数对比情况" class="headerlink" title="不同的优化函数对比情况"></a>不同的优化函数对比情况</h5><p>上述所使用的fmin_ncg()牛顿法原理是利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。但在运行过程中一直打印警告信息提示hassian矩阵不是正数，而且运行时间太长。<br>在scipy.optimize库中使用其他优化方法试试？<br>参考资料见顶部开头。<br>由于一下三种方法的参数列表都相同<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fmin_cg(f, x0[, fprime, args, gtol, norm, …]) Minimize a function using a nonlinear conjugate gradient algorithm.</span><br><span class="line">fmin_bfgs(f, x0[, fprime, args, gtol, norm, …]) Minimize a function using the BFGS algorithm.</span><br><span class="line">fmin_ncg(f, x0, fprime[, fhess_p, fhess, …]) Unconstrained minimization of a function using the Newton-CG method.</span><br></pre></td></tr></table></figure></p>
<ol>
<li><p>故先使用fmin_cg方法，原理上面已经给出：非线性动态梯度算法  .<br>结果运行时长1min左右就出来了。<br>预测结果：96.46%<br>代码见开头</p>
</li>
<li><p>再把方法改为fmin_bfgs()会显示超过最大迭代次数警告，不过最终预测结果准确率竟然达到: 96.48%</p>
</li>
<li><p>用opt.minimize(method=method=’L-BFGS-B’)预测结果为：95.44%。<br>完整代码为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">result=opt.minimize(fun=Cost_reg,x0=theta[:,i:i+1],args=(X,classes_y,m),method=&apos;L-BFGS-B&apos;)</span><br><span class="line">        theta[:,i:i+1]=np.mat(result.x).T</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>梯度下降不知道怎么嵌入。。。<br>路过大神望告知！  </p>
<h4 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h4><p>目标：实现神经网络识别手写数字。<br>核心：实现前向传播算法，前向传播算法即神经网络从输入层到隐藏层到输出层的过程。  </p>
<h5 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h5><p>数字矩阵还是使用之前多元分类器的数据，这里再给出了第一层到第二层的&theta;<sup>1</sup>和第二层到第三层的&theta;<sup>2</sup>。其中，&theta;<sup>1</sup>是25 x 401的，即第二层有a<sup>(2)</sup>有25个单元；&theta;<sup>2</sup>是10 x 26，26是在第二层多加一个bias unit。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def Get_Data():</span><br><span class="line">    data=sio.loadmat(&apos;./ex3/ex3data1.mat&apos;)</span><br><span class="line">    theta=sio.loadmat(&apos;./ex3/ex3weights.mat&apos;)</span><br><span class="line">    return data[&apos;X&apos;],data[&apos;y&apos;],theta[&apos;Theta1&apos;],theta[&apos;Theta2&apos;],data[&apos;y&apos;].size</span><br><span class="line"></span><br><span class="line">X,y,theta1,theta2,m=Get_Data()</span><br><span class="line">X=np.hstack((np.ones((m,1)),X))</span><br></pre></td></tr></table></figure></p>
<h5 id="Model-representation"><a href="#Model-representation" class="headerlink" title="Model representation"></a>Model representation</h5><p><img src="https://i.loli.net/2019/07/28/5d3d842ee0e4b88574.png" alt="2019-07-28 19-16-38 的屏幕截图.png"><br>大致步骤：<br>      不同的样本添加X0=1表示bias unit，分别通过第一层计算得到第二层<br>      第二层得到的数据再添加一个bias unit，通过Theta2计算得到第三层，取第三层概率值H<sub>&theta;</sub>(X)最大的就是最终的分类预测了。 </p>
<h5 id="Feedforward-Propagation-and-Prediction"><a href="#Feedforward-Propagation-and-Prediction" class="headerlink" title="Feedforward Propagation and Prediction"></a>Feedforward Propagation and Prediction</h5><p>前向传播与预测。<br>前向传播就是信息从第一层到最后一层最终的出结果的过程。<br>预测则是类似多元分类，用的出的参数&theta;与样本或者其他数据进行运算，看计算结果属于哪一类。<br>这里可以直接用矩阵运算快速得出结果。<br><code>np.argmax()</code>可得到ndarray类型最大值的索引，同理argmin则是取最小值的索引，可指定参数axis，默认为0表示列，1则表示行即每行最大值的索引<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def prediction(X,y,theta1,theta2,m):</span><br><span class="line">    # print(theta1.shape,X.shape)</span><br><span class="line">    a_2=1/(1+np.exp(-np.dot(X,theta1.T))) # 直接计算第二层结果</span><br><span class="line">    a_2=np.hstack((np.ones((m,1)),a_2)) # 添加一个bias unit</span><br><span class="line">    a_3=1/(1+np.exp(-a_2.dot(theta2.T)))  # 用样的方法计算第三层</span><br><span class="line">    print(&apos;%.2f%%&apos; %(sum(np.argmax(a_3,axis=1)+1==y.ravel())/m*100)) # 加一是y的范围是[1,10]</span><br></pre></td></tr></table></figure></p>
<p>预测结果：97.52%<br>完整代码见开头</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://sfz-lyq.cn/2019/07/22/吴恩达机器学习第三次编程作业-神经网络/" data-id="ck6bxbgk600f782s651u1htxd" class="article-share-link">分享</a>
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../tags/机器学习/">机器学习</a></li></ul>

    </footer>

  </div>

  
    
  <nav class="article-nav">
    
      <a href="../Pandas库学习/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            Pandas库学习
          
        </div>
      </a>
    
    
      <a href="../神经网络/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">神经网络</div>
      </a>
    
  </nav>


  

  
    
  

</article>



</section>
    </div>
    <script src="../../../../js/jquery-2.0.3.min.js"></script>
<script src="../../../../js/lazyload.min.js"></script>
<script src="../../../../js/busuanzi-2.3.pure.min.js"></script>


  <script src="../../../../fancybox/jquery.fancybox.min.js"></script>



  <script src="../../../../js/search.js"></script>


<script src="../../../../js/technology.js"></script>

  </div>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body>
</html>