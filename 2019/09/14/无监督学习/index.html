<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  
    <meta name="keywords" content="NO response">
  
  
    <meta name="description" content="what you will be,the world will be">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    无监督学习： K-Means |
    
    种花家</title>
  
    <link rel="shortcut icon" href="/images/rabbit.png">
  
  <link rel="stylesheet" href="../../../../css/style.css">
  <link rel="stylesheet" href="../../../../css/technology.css">
  
    <link rel="stylesheet" href="../../../../fancybox/jquery.fancybox.min.css">
  
  <script src="../../../../js/pace.min.js"></script>
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <section class="outer">
  <article id="post-无监督学习" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      无监督学习： K-Means
    </h1>
  

      </header>
    

    
      <div class="article-meta">
        <a href class="article-date">
  <time datetime="2019-09-14T08:19:00.000Z" itemprop="datePublished">2019-09-14</time>
</a>
        
  <div class="article-category">
    <a class="article-category-link" href="../../../../categories/机器学习/">机器学习</a>
  </div>

      </div>
    

    <div class="article-entry" itemprop="articleBody">
      

      

      
        <h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1052194132&amp;courseId=1004570029" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><ul>
<li>在监督学习中，我们所使用到的样本都是带有<strong>标签</strong>的，我们需要一个假设函数和决策线来进行分类，也就是说监督学习是有期望输出的，并且可以和实际值比较比较。   </li>
<li>在无监督学习中，样本不在具有标签，只是一堆散落的值而已。我们要做的是设计或者使用算法，并将这些无标签的数据输入到算法中，期望算法能够找出一些隐含在数据中的结构。如聚类算法，见下图<br><img src="https://i.loli.net/2019/09/14/pwXaWET4PUuhA5N.png" alt="2019-09-14 16-30-10 的屏幕截图.png"> </li>
</ul>
<h4 id="K-Means算法，最广泛运用的聚类算法"><a href="#K-Means算法，最广泛运用的聚类算法" class="headerlink" title="K-Means算法，最广泛运用的聚类算法"></a>K-Means算法，最广泛运用的聚类算法</h4><h5 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h5><p><img src="https://i.loli.net/2019/09/14/ljUJbHhMfi1rOIT.png" alt="2019-09-14 16-48-28 的屏幕截图.png"><br>如何将上图样本分类两个簇(cluster)，实际上簇的数量即K-Means中的K，后面会介绍如何选择K。  </p>
<p>这里，我们先随机两个点X作为簇中心 ， 遍历所有点计算离这两个点的距离并归类<br><img src="https://i.loli.net/2019/09/14/A5aDZXtxUn6vGuy.png" alt="2019-09-14 16-53-31 的屏幕截图.png"></p>
<p>然后计算红色点簇的中心和蓝色点簇的中心，将之前的簇中心移动到新的中心位置。重复上述操作，直到簇中心不再改变。</p>
<h5 id="K-Means算法输入"><a href="#K-Means算法输入" class="headerlink" title="K-Means算法输入"></a>K-Means算法输入</h5><p>两个参数：K和X，K表示簇的数量，X为n维向量，不再需要x<sub>0</sub>=1<br><img src="https://i.loli.net/2019/09/14/nFMzIRck9oANmdj.png" alt="2019-09-14 17-01-33 的屏幕截图.png"> </p>
<h5 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h5><p><img src="https://i.loli.net/2019/09/14/FUoHPBqsw9gRSbI.png" alt="2019-09-14 17-09-15 的屏幕截图.png"><br>两个循环分别表示例子中所述的两个步骤。<br>c<sup>(i)</sup>表示第i个样本所属的簇索引编号。u<sub>k</sub>表示簇中心。<br>可能会出线的一种情况，某个簇中心所在的集合为空，这样可以直接移除这个簇中心。</p>
<h5 id="K-Means算法解决分离不佳的簇问题"><a href="#K-Means算法解决分离不佳的簇问题" class="headerlink" title="K-Means算法解决分离不佳的簇问题"></a>K-Means算法解决分离不佳的簇问题</h5><p>衣服尺寸设计问题<br><img src="https://i.loli.net/2019/09/14/yA8QqkVBMtToOZK.png" alt="2019-09-14 17-17-11 的屏幕截图.png"></p>
<h4 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h4><p>最小化所有的样本点到其所属的簇中心距离之和<br><img src="https://i.loli.net/2019/09/15/1KGYF3tdIa4Bzkc.png" alt="2019-09-15 10-44-01 的屏幕截图.png"></p>
<h4 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h4><p>K-Means算法第一步就是如何选取K个簇中心，避免局部最优。  </p>
<ul>
<li>首先，不难理解聚类中心K的数量应该小于训练样本数量m，这里假设K给定</li>
<li>随机选取K个训练样本作为簇中心&mu;<sub>1</sub> … &mu;<sub>k</sub></li>
<li>运行K-Means算法计算代价函数J</li>
<li>重复上述操作100次或者10次，迭代次数根据训练样本数而定；找出代价函数最小的那次。<br><img src="https://i.loli.net/2019/09/15/vnPxB1rZMwyhieF.png" alt="2019-09-15 11-00-27 的屏幕截图.png"></li>
</ul>
<h4 id="选取聚类数量"><a href="#选取聚类数量" class="headerlink" title="选取聚类数量"></a>选取聚类数量</h4><p>肘部法则： 改变K，计算最优代价函数，画图（代价函数关于K的变化）,找到那个肘部即拐点，之后变化平缓了。<br><img src="https://i.loli.net/2019/09/15/CsONdD5vplfyFQc.png" alt="2019-09-15 11-14-48 的屏幕截图.png"></p>
<p>但，大多数时候我们会得到右图的结果，这并不好决定。需要自己决定分类的目的是什么。</p>
<hr>
<center><font size="5px" color="red">补充</font></center>

<hr>
<h3 id="sklearn中的聚类算法K-Means"><a href="#sklearn中的聚类算法K-Means" class="headerlink" title="sklearn中的聚类算法K-Means"></a><font size="5px" color="red">sklearn中的聚类算法K-Means</font></h3><h4 id="聚类与分类"><a href="#聚类与分类" class="headerlink" title="聚类与分类"></a><font size="3px" color="red">聚类与分类</font></h4><table>
<thead>
<tr>
<th></th>
<th>聚类</th>
<th>分类</th>
</tr>
</thead>
<tbody>
<tr>
<td>核心</td>
<td>将数据分成多个组，探索每个组的数据是否有联系</td>
<td>从已经分组的数据中去学习，把新数据放到已经分好的组中去</td>
</tr>
<tr>
<td>学习类型</td>
<td>无监督，无需<strong>标签</strong>进行训练</td>
<td>有监督，需要<strong>标签</strong>进行训练</td>
</tr>
<tr>
<td>典型算法</td>
<td>K-Means，DBSCAN，层次聚类，光谱聚类</td>
<td>决策树，贝叶斯，逻辑回归</td>
</tr>
<tr>
<td>算法输出</td>
<td>聚类结果是不确定的，不一定总是能够反映数据的真实分类，同样的聚类，根据不同的业务需求可能是一个好结果，也可能是一个坏结果</td>
<td>分类结果是确定的，分类的优劣是客观的，不是根据业务或算法需求决定</td>
</tr>
</tbody>
</table>
<h4 id="sklearn中的聚类算法"><a href="#sklearn中的聚类算法" class="headerlink" title="sklearn中的聚类算法"></a><font size="3px" color="red">sklearn中的聚类算法</font></h4><p>聚类算法在sklearn中有两种表现形式，一种是类（和我们目前为止学过的分类算法以及数据预处理方法们都一样），需要<strong>实例化</strong>，训练并使用接口和属性来调用结果。另一种是函数（function），只需要输入特征矩阵和超参数，即可返回聚类的结果和各种指标。</p>
<table>
<thead>
<tr>
<th>类</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>sklearn.cluster.Birch</td>
<td>实现Birtch聚类算法</td>
</tr>
<tr>
<td>sklearn.cluster.DBSCAN</td>
<td>从矢量数组或距离矩阵执行DBSCAN聚类</td>
</tr>
<tr>
<td>cluster.KMeans</td>
<td>K均值聚类</td>
</tr>
<tr>
<td>cluster.MiniBatchKMeans</td>
<td>小批量K均值聚类</td>
</tr>
<tr>
<td>cluster.SpectralClustering</td>
<td>光谱聚类，将聚类应用于规范化拉普拉斯的投影</td>
</tr>
<tr>
<td>cluste.AgglomerativeClustering</td>
<td>凝聚聚类</td>
</tr>
<tr>
<td><strong>函数</strong></td>
<td><strong>含义</strong></td>
</tr>
<tr>
<td>cluster.k_means</td>
<td>k均值聚类</td>
</tr>
<tr>
<td>cluster.ward_tree</td>
<td>光谱聚类</td>
</tr>
<tr>
<td>cluster.affinity_propagation</td>
<td>执行亲和传播数据聚类</td>
</tr>
<tr>
<td>cluster.mean_shift</td>
<td>使用平坦核函数的平均移位聚类</td>
</tr>
</tbody>
</table>
<h4 id="KMeans"><a href="#KMeans" class="headerlink" title="KMeans"></a><font size="3px" color="red">KMeans</font></h4><p><img src="https://img-blog.csdnimg.cn/20191224170942253.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="KMeans"></p>
<font size="4px" color="green">Kmeans没有损失函数</font>  


<h4 id="实例"><a href="#实例" class="headerlink" title="实例"></a><font size="3px" color="red">实例</font></h4><ul>
<li><code>sklearn.cluster.KMeans</code><br>重要参数<code>n_clusters</code>：n_clusters是KMeans中的k，表示着我们告诉模型我们要分几类。这是KMeans当中唯一一个必填的参数，默认为8类。</li>
</ul>
<p>数据：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import make_blobs</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">X, y = make_blobs(n_samples=500,n_features=2,centers=4,random_state=1)</span><br><span class="line">color = [&quot;red&quot;,&quot;pink&quot;,&quot;orange&quot;,&quot;gray&quot;]</span><br><span class="line">for i in range(4):</span><br><span class="line">    plt.scatter(X[y==i, 0], X[y==i, 1]</span><br><span class="line">           ,marker=&apos;o&apos;</span><br><span class="line">           ,s=8</span><br><span class="line">           ,c=color[i]</span><br><span class="line">           )</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://img-blog.csdnimg.cn/20191224173418537.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="数据"><br>先简单实例化：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cluster import KMeans</span><br><span class="line">cluster = KMeans(n_clusters=3, random_state=0).fit(X)  # 先设置3类，训练好模型</span><br><span class="line"></span><br><span class="line"># 重要属性</span><br><span class="line">y_pred = cluster.labels_ # 每个样本所对应的类,(500,)</span><br><span class="line"></span><br><span class="line">pre = cluster.fit_predict(X) # 训练并预测所有数据所属类</span><br><span class="line">print(pre==y_pred) # 全是True</span><br><span class="line"></span><br><span class="line">centroid=cluster.cluster_centers_</span><br><span class="line">print(centroid) # 簇心</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">[[-7.09306648 -8.10994454]</span><br><span class="line"> [-1.54234022  4.43517599]</span><br><span class="line"> [-8.0862351  -3.5179868 ]]</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line">inertia=cluster.inertia_ # 总距离平方和</span><br><span class="line">print(inertia) # 1903.4503741659223</span><br></pre></td></tr></table></figure></p>
<p>查看结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">color = [&quot;red&quot;,&quot;pink&quot;,&quot;orange&quot;,&quot;gray&quot;]</span><br><span class="line">for i in range(3): # 聚类结果</span><br><span class="line">    plt.scatter(X[y_pred==i,0],X[y_pred==i,1]</span><br><span class="line">                ,marker=&apos;o&apos;</span><br><span class="line">                ,s=8</span><br><span class="line">                ,c=color[i])</span><br><span class="line"></span><br><span class="line">plt.scatter(centroid[:,0],centroid[:,1],marker=&apos;X&apos;,s=15,c=&apos;black&apos;) # 绘制质心</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://img-blog.csdnimg.cn/20191224175030244.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="聚类结果"><br>根据整体平方和inertia和聚类结果可以发现橙色区域聚类效果可以更优，inertia可以更小。</p>
<p>尝试其他的n_clusters，可以发现簇数量越大，总距离平方和<code>cluster.inertia_</code>越小，但<code>cluster.inertia_</code>并不能用来评估算法好坏</p>
<h4 id="聚类算法的模型评估指标"><a href="#聚类算法的模型评估指标" class="headerlink" title="聚类算法的模型评估指标"></a><font size="3px" color="red">聚类算法的模型评估指标</font></h4><p>在<strong>分类</strong>中，有直接结果（标签）的输出，并且分类的结果有正误之分，所以我们使用预测的<strong>准确度</strong>，<strong>混淆矩阵</strong>，<strong>ROC曲线</strong>等等指标来进行评估，但无论如何评估，都是在”模型找到正确答案“的能力。而回归中，由于要拟合数据，我们有SSE均方误差，有损失函数来衡量模型的拟合程度。但这些衡量指标都不能够使用于聚类。</p>
<p><font size="5px" color="greey">面试高危问题：如何衡量聚类算法的效果？</font><br>聚类模型的结果不是某种标签输出，并且<strong>聚类的结果是不确定的</strong>，其优劣由业务需求或者算法需求来决定，并且没有永远的正确答案。那我们如何衡量聚类的效果呢？<br>KMeans的目标是确保<strong>簇内差异小，簇外差异大</strong>，可以通过衡量簇内差异来衡量聚类的效果。而<code>Inertia</code>是用距离来衡量簇内差异的指标，但是Inertia越小模型越好吗？   </p>
<ul>
<li>首先，它不是有界的。我们只知道，Inertia是越小越好，是0最好，但我们不知道，一个较小的Inertia究竟有没有达到模型的极限，能否继续提高。</li>
<li>第二，它的计算太容易受到特征数目的影响，数据维度很大的时候，Inertia的计算量会陷入维度诅咒之中，计算量会爆炸，不适合用来一次次评估模型。</li>
<li>第三，Inertia对数据的分布有假设，它假设数据满足凸分布（即数据在二维平面图像上看起来是一个凸函数的样子），并且它假设数据是各向同性的（isotropic），即是说数据的属性在不同方向上代表着相同的含义。但是现实中的数据往往不是这样。所以使用Inertia作为评估指标，会让聚类算法在一些细长簇，环形簇，或者不规则形状的流形时表现不佳：<br><img src="https://img-blog.csdnimg.cn/2019122420540115.jpg" alt="环形簇"><br>可以看出，实际上期望效果应该是内环作为一簇，外环作为一簇，而如果使用<code>cluster.inertia_</code>作为衡量指标，效果就会很差</li>
</ul>
<h5 id="轮廓系数"><a href="#轮廓系数" class="headerlink" title="轮廓系数"></a><font size="3px" color="red">轮廓系数</font></h5><p>最常用的聚类算法的评价指标。它是对每个样本来定义的，它能够同时衡量：</p>
<ol>
<li>样本与其自身所在的簇中的其他样本的相似度<strong>a</strong>，等于样本与同一簇中所有其他点之间的平均距离</li>
<li>样本与其他簇中的样本的相似度<strong>b</strong>，等于样本与下一个最近的簇中得所有点之间的平均距离</li>
</ol>
<p>根据聚类的要求”簇内差异小，簇外差异大“，我们希望b永远大于a，并且大得越多越好。<br>单个样本的轮廓系数计算为：<br><img src="https://img-blog.csdnimg.cn/20191224210005965.png" alt="轮廓系数"><br>即：<br><img src="https://img-blog.csdnimg.cn/20191224210052149.jpg" alt="轮廓系数"><br>很容易理解轮廓系数范围是(-1,1)，其中值越接近1表示样本与自己所在的簇中的样本很相似，并且与其他簇中的样本不相似，当样本点与簇外的样本更相似的时候，轮廓系数就为负。当轮廓系数为0时，则代表两个簇中的样本相似度一致，两个簇本应该是一个簇。</p>
<p><font color="red">在sklearn中，我们使用模块metrics中的类<code>silhouette_score</code>来计算轮廓系数，它返回的是一个数据集中，所有样本的<strong>轮廓系数的均值</strong>。</font></p>
<ul>
<li>学习曲线<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cluster import KMeans</span><br><span class="line">from sklearn.metrics import silhouette_score</span><br><span class="line">from sklearn.metrics import silhouette_samples</span><br><span class="line"></span><br><span class="line">score=[]</span><br><span class="line">for i in range(3,7):</span><br><span class="line">    y_pred=KMeans(n_clusters=i,random_state=10).fit_predict(X)</span><br><span class="line">    score.append(silhouette_score(X,y_pred))</span><br><span class="line">    </span><br><span class="line">plt.plot(range(3,7),score)</span><br><span class="line">plt.xticks([3,4,5,6])</span><br><span class="line">plt.show()</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">print(silhouette_samples(X,y_pred)) </span><br><span class="line">这个是返回所有样本的聚类情况，为True或者False，如果是False则说明这个样本的轮廓系数为负，聚类失败</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20191224211620466.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="学习曲线"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://sfz-lyq.cn/2019/09/14/无监督学习/" data-id="ck6bxbgkt00fy82s6bzilefjj" class="article-share-link">分享</a>
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../tags/ML/">ML</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../tags/机器学习/">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../tags/聚类/">聚类</a></li></ul>

    </footer>

  </div>

  
    
  <nav class="article-nav">
    
      <a href="../../15/降维/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            降维
          
        </div>
      </a>
    
    
      <a href="../../13/Python爬虫小练习/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">Python爬虫小练习</div>
      </a>
    
  </nav>


  

  
    
  

</article>



</section>
    </div>
    <script src="../../../../js/jquery-2.0.3.min.js"></script>
<script src="../../../../js/lazyload.min.js"></script>
<script src="../../../../js/busuanzi-2.3.pure.min.js"></script>


  <script src="../../../../fancybox/jquery.fancybox.min.js"></script>



  <script src="../../../../js/search.js"></script>


<script src="../../../../js/technology.js"></script>

  </div>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body>
</html>