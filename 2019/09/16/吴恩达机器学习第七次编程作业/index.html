<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  
    <meta name="keywords" content="NO response">
  
  
    <meta name="description" content="what you will be,the world will be">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    吴恩达机器学习第七次编程作业-K-means Clustering and Principal Component Analysis |
    
    种花家</title>
  
    <link rel="shortcut icon" href="/images/rabbit.png">
  
  <link rel="stylesheet" href="../../../../css/style.css">
  <link rel="stylesheet" href="../../../../css/technology.css">
  
    <link rel="stylesheet" href="../../../../fancybox/jquery.fancybox.min.css">
  
  <script src="../../../../js/pace.min.js"></script>
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <section class="outer">
  <article id="post-吴恩达机器学习第七次编程作业" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      吴恩达机器学习第七次编程作业-K-means Clustering and Principal Component Analysis
    </h1>
  

      </header>
    

    
      <div class="article-meta">
        <a href class="article-date">
  <time datetime="2019-09-16T07:43:00.000Z" itemprop="datePublished">2019-09-16</time>
</a>
        
  <div class="article-category">
    <a class="article-category-link" href="../../../../categories/机器学习/">机器学习</a>
  </div>

      </div>
    

    <div class="article-entry" itemprop="articleBody">
      

      

      
        <h3 id="Programming-Exercise-7-K-means-Clustering-and-Principal-Component-Analysis"><a href="#Programming-Exercise-7-K-means-Clustering-and-Principal-Component-Analysis" class="headerlink" title=" Programming Exercise 7:   K-means Clustering and Principal Component Analysis"></a><center> <a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/text?lessonId=1053422072&amp;courseId=1004570029" target="_blank" rel="noopener">Programming Exercise 7:   K-means Clustering and Principal Component Analysis</a></center></h3><hr>
<p>环境： Python3.5 , Pycharm2019.01</p>
<p>参考资料：</p>
<ol>
<li><a href="https://www.cnblogs.com/Allen-rg/p/9606824.html" target="_blank" rel="noopener">numpy.argmin 使用</a></li>
<li><a href="https://blog.csdn.net/kingroc/article/details/73088510" target="_blank" rel="noopener">Python绘制点线</a></li>
<li><a href="https://blog.csdn.net/Cherish_x/article/details/90550001" target="_blank" rel="noopener">第七次作业：k-means算法</a></li>
<li><a href="https://www.cnblogs.com/twilight77/p/7675512.html" target="_blank" rel="noopener">Python 玩转随机数</a></li>
<li><a href="https://www.cnblogs.com/MIS-67/p/9912505.html" target="_blank" rel="noopener">K-means算法应用：图片压缩</a></li>
<li><a href="https://www.cnblogs.com/DOLFAMINGO/p/9362637.html" target="_blank" rel="noopener">吴恩达机器学习笔记 — 降维与主成分分析法(PCA)
</a></li>
<li><a href="https://blog.csdn.net/a10767891/article/details/80288463" target="_blank" rel="noopener">PCA 原理：为什么用协方差矩阵</a></li>
<li><a href="https://blog.csdn.net/babywong/article/details/50085239" target="_blank" rel="noopener">PCA为什么要用协方差矩阵？</a></li>
<li><a href="https://www.cnblogs.com/pinard/p/6251584.html" target="_blank" rel="noopener">奇异值分解(SVD)原理与在降维中的应用</a></li>
<li><a href="https://blog.csdn.net/cowry5/article/details/80503380" target="_blank" rel="noopener">吴恩达机器学习作业Python实现(七)</a></li>
</ol>
<p><a href="https://paste.ubuntu.com/p/CJncKmWdwc/" target="_blank" rel="noopener">K-means Clustering完整代码</a><br><a href="https://paste.ubuntu.com/p/Kq9F5bKmcz/" target="_blank" rel="noopener">Principal Component Analysis完整代码</a></p>
<hr>
<h3 id="Part-1-K-Means-Clustering"><a href="#Part-1-K-Means-Clustering" class="headerlink" title="Part 1:  K-Means Clustering"></a>Part 1:  K-Means Clustering</h3><h4 id="实现K-Means算法"><a href="#实现K-Means算法" class="headerlink" title="实现K-Means算法"></a>实现K-Means算法</h4><h5 id="Finding-closest-centroids"><a href="#Finding-closest-centroids" class="headerlink" title="Finding closest centroids"></a>Finding closest centroids</h5><p>根据所给出的簇中心将所有的样本点进行标记，返回一维数组代表每个样本所属的簇，簇编号从1开始。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def Find_Closets_Centroid(X,centroid):</span><br><span class="line">    idx=np.zeros(X.shape[0],dtype=int)</span><br><span class="line">    for i in range(X.shape[0]):</span><br><span class="line">        tmp_x=(X[i]-centroid)**2</span><br><span class="line">        idx[i]=np.argmin(np.sum(tmp_x,axis=1)) # 先按行相加得到[1,3]的距离数组，再找出最小值下标</span><br><span class="line">    return idx  # 下标从0开始</span><br><span class="line">    </span><br><span class="line">K=3 # 簇中心数量</span><br><span class="line">init_centroid=[[3,3],[6,2],[8,5]] </span><br><span class="line">idx=Find_Closets_Centroid(X,init_centroid)</span><br><span class="line"># print(idx[:3]) # except to see [0 2 1]</span><br></pre></td></tr></table></figure></p>
<h5 id="Computing-centroid-means"><a href="#Computing-centroid-means" class="headerlink" title="Computing centroid means"></a>Computing centroid means</h5><p>既然每个样本都有一个所属簇，那么分别计算这些不同簇的样本点集的均值作为新的簇中心。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def Computer_Centroids(X,idx,K):</span><br><span class="line">    centroids=np.zeros((K,X.shape[1]))</span><br><span class="line">    for i in range(K):</span><br><span class="line">        centroids[i]=np.mean(X[np.where(idx==i)],axis=0) # 先将样本点集区分出来成为新的二维数组，再按列取均值</span><br><span class="line">    return centroids</span><br><span class="line"></span><br><span class="line">centroids=Computer_Centroids(X,idx,K)</span><br><span class="line">print(centroids) # except to see [[2.42830111 3.15792418],[5.81350331 2.63365645],[7.11938687 3.6166844 ]]</span><br></pre></td></tr></table></figure></p>
<h4 id="K-means-on-example-dataset"><a href="#K-means-on-example-dataset" class="headerlink" title="K-means on example dataset"></a>K-means on example dataset</h4><p>手写K-Means算法，观察簇中心在数据集上的变化情况。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">def Record_Centroid(i,centroids,history_centroids): # 记录簇中心变化</span><br><span class="line">    history_centroids_tmp = centroids.copy()</span><br><span class="line">    history_centroids[0][i] = history_centroids_tmp[0]</span><br><span class="line">    history_centroids[1][i] = history_centroids_tmp[1]</span><br><span class="line">    history_centroids[2][i] = history_centroids_tmp[2]</span><br><span class="line">    return history_centroids</span><br><span class="line"></span><br><span class="line">def kMeans(X,K,init_centroid,max_iter): # K-Means算法运行过程</span><br><span class="line">    centroids=init_centroid</span><br><span class="line">    history_centroids=np.zeros((K,max_iter,X.shape[1])) # 申请三维数组记录K个簇变化</span><br><span class="line">    idx=np.zeros(X.shape[0],dtype=int)</span><br><span class="line">    for i in range(max_iter):</span><br><span class="line">        idx=Find_Closets_Centroid(X,centroids)</span><br><span class="line">        history_centroids=Record_Centroid(i,centroids,history_centroids)</span><br><span class="line">        centroids=Computer_Centroids(X,idx,K) # new centroids</span><br><span class="line">    return centroids,idx,history_centroids</span><br><span class="line"></span><br><span class="line">def Plot_kMeans_Progress(X,history_centroids,idx,K): # 观察K-Means簇中心变化</span><br><span class="line">    cluster1=X[np.where(idx==0)] # 最终所属的簇</span><br><span class="line">    cluster2 = X[np.where(idx == 1)]</span><br><span class="line">    cluster3 = X[np.where(idx == 2)]</span><br><span class="line">    plt.plot(cluster1[:,0],  cluster1[:,1],&apos;r.&apos;, label=&apos;Cluster 1&apos;)</span><br><span class="line">    plt.plot(cluster2[:, 0], cluster2[:, 1],&apos;g.&apos;, label=&apos;Cluster 2&apos;)</span><br><span class="line">    plt.plot(cluster3[:, 0], cluster3[:, 1],&apos;b.&apos;, label=&apos;Cluster 3&apos;)</span><br><span class="line">    plt.plot(history_centroids[0][:,0],  history_centroids[0][:,1],&apos;kX-&apos;,alpha=0.5) # alpha设置透明度</span><br><span class="line">    plt.plot(history_centroids[1][:, 0], history_centroids[1][:, 1],&apos;kX-&apos;,alpha=0.5)</span><br><span class="line">    plt.plot(history_centroids[2][:, 0], history_centroids[2][:, 1],&apos;kX-&apos;,alpha=0.5)</span><br><span class="line">    plt.xlim(-1,9)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.title(&apos;Iteration number 10&apos;,fontsize=12)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://www.privacypic.com/images/2019/09/16/Figure_27a503542666fa1765.png" alt="Figure_27a503542666fa1765.png"></p>
<h4 id="Random-initialization"><a href="#Random-initialization" class="headerlink" title="Random initialization"></a>Random initialization</h4><p>给定样本集和簇中心数量随机选取中心簇作为初始化。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def Random_Init_Centroids(X,K):</span><br><span class="line">    rand_idx=np.arange(X.shape[0]) # 生成排列</span><br><span class="line">    np.random.shuffle(rand_idx)  # 随机打乱，每次都不一样</span><br><span class="line">    return X[rand_idx[:K]] # 返回前K个样本点作为簇中心</span><br></pre></td></tr></table></figure></p>
<h4 id="Image-compression-with-K-means"><a href="#Image-compression-with-K-means" class="headerlink" title="Image compression with K-means"></a>Image compression with K-means</h4><p>使用K-Means算法将所给的图片进行压缩。<br>原理：</p>
<ul>
<li>关于图片：这里所使用的是png格式的彩图，即每个像素点24位，每8位分别代表RGB。像素点越多越清晰，这里图片大小为128*128，代表长宽也代表像素点个数。</li>
<li>使用plt.imread(picture_path)读入图片，将二进制信息以三维矩阵形式读出(128,128,3)。</li>
<li>重塑图片，使其次成为二维[128*128,3]，然后每行代表一个像素点，接着使用K-Means算法找到16个簇中心，然后用这些簇中心的值代替其所属簇的像素点的值。这样就将24位压缩成了16位，然后再重塑回三维[128,128,3]，显示效果对比。</li>
<li>使用不同的K，观察效果</li>
<li>A=A/255是特征缩放，以便计算距离，最终显示还要乘上255，还是24位</li>
</ul>
<p>为什么能实现压缩？</p>
<ul>
<li>之前说的像素点每个点都有一个值，范围是[0-255]，实际上压缩上述代码实现压缩之后图片大小貌似并没有变化，那么做这个意义何在？</li>
<li>其实，原图每个像素点可以看成一种颜色，我们做的事就是找出16种主要的颜色代替其余颜色。值的范围没有变，但我们就可以改变之前像素点所存储的信息了，我们先在只要4bit就可以知道这个像素点所属的簇，那么再找到这个簇的颜色就是这个像素点的颜色了。</li>
<li>故，我们需要额外的一点点空间存储簇的颜色，然后(以下面图片为例)图片大小就可以用<code>128*128*4</code>替换，而不是<code>218*128*24</code>。有种类似时间换空间的感觉。</li>
<li>下面代码也就是实现了完整的图像压缩核心部分以显示效果，并没有实际压缩图片。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">def Clustering_On_Pixels(path,K,max_iter):</span><br><span class="line">    A=plt.imread(path)</span><br><span class="line">    # print(type(A),A.shape) # except to see: &lt;class &apos;numpy.ndarray&apos;&gt; (128, 128, 3)</span><br><span class="line">    A=A/255 # Divide by 255 so that all values are in the range 0 - 1</span><br><span class="line">    X=A.reshape(A.shape[0]*A.shape[1],3)  # [128*128,3]</span><br><span class="line">    init_centroids=Random_Init_Centroids(X,K) # 初始化簇中心</span><br><span class="line">    centroids,idx,history=kMeans(X,K,init_centroids,max_iter) # 迭代找到16个簇中心</span><br><span class="line">    return A,X,centroids</span><br><span class="line"></span><br><span class="line">def Image_Compression(path):</span><br><span class="line">    K, max_iter = 16, 10  # try different values of K and max_iters here</span><br><span class="line">    A,X,centroids=Clustering_On_Pixels(path,K,max_iter)</span><br><span class="line">    idx=Find_Closets_Centroid(X,centroids)</span><br><span class="line">    X_recovered=centroids[idx] # 值代替</span><br><span class="line">    X_recovered=X_recovered.reshape(A.shape[0],A.shape[1],3) # 重塑回三维</span><br><span class="line">    plt.subplot(1,2,1)</span><br><span class="line">    plt.imshow(A*255)</span><br><span class="line">    plt.title(&apos;Original&apos;)</span><br><span class="line">    plt.subplot(1, 2, 2)</span><br><span class="line">    plt.imshow(X_recovered*255)</span><br><span class="line">    plt.title(&apos;Compressed, with %d colors&apos; %K,fontsize=10)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">Image_Compression(&apos;./ML/ex7/bird_small.png&apos;)</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/09/18/jLAaXp9EtyIukqC.png" alt="Figure_28.png"><br><img src="https://i.loli.net/2019/09/18/wDglFhRmxJnaKMq.png" alt="Figure_29.png"> </p>
<h3 id="Part2-Principal-Component-Analysis"><a href="#Part2-Principal-Component-Analysis" class="headerlink" title="Part2: Principal Component Analysis"></a>Part2: Principal Component Analysis</h3><h4 id="2-1-Example-Dataset"><a href="#2-1-Example-Dataset" class="headerlink" title="2.1 Example Dataset"></a>2.1 Example Dataset</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def  Get_data(path):</span><br><span class="line">    data=sio.loadmat(path)</span><br><span class="line">    # for key in data:</span><br><span class="line">    #     print(key)</span><br><span class="line">    return data,data[&apos;X&apos;]</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos; Visualizing example dataset for PCA &apos;&apos;&apos;</span><br><span class="line">data,X=Get_data(&apos;./ML/ex7/ex7data1.mat&apos;)</span><br><span class="line"># print(X.shape) # (50,2)</span><br><span class="line">plt.scatter(X[:,0], X[:,1], facecolors=&apos;none&apos;, edgecolors=&apos;b&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/09/18/TGYjgWk5QDCtlUq.png" alt="Figure_30.png"></p>
<h4 id="2-2-Implementing-PCA"><a href="#2-2-Implementing-PCA" class="headerlink" title="2.2 Implementing PCA"></a>2.2 Implementing PCA</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def Feature_Normalize(X):</span><br><span class="line">    means = np.mean(X, axis=0)  # the mean of the each feature</span><br><span class="line">    std = np.std(X, axis=0, ddof=1)  # 取每列标准偏差</span><br><span class="line">    return (X-means)/std,means</span><br><span class="line"></span><br><span class="line">def PCA(X):</span><br><span class="line">    sigma=(X.T@X)/X.shape[0] # covariance matrix</span><br><span class="line">    U,S,V=np.linalg.svd(sigma) # wait to learn</span><br><span class="line">    return U,S,V</span><br><span class="line"></span><br><span class="line">X_normal,means=Feature_Normalize(X) # normalize X</span><br><span class="line">U,S,V=PCA(X_normal)</span><br><span class="line"># print(means.shape,S.shape,U.shape)</span><br><span class="line"># print(U[:,0]) # except to  see: [-0.70710678 -0.70710678]</span><br><span class="line">plt.plot([means[0], means[0] + 1.5*S[0]*U[0,0]], [means[1], means[1] + 1.5*S[0]*U[0,1]],c=&apos;r&apos;, linewidth=3, label=&apos;First Principal Component&apos;)</span><br><span class="line">plt.plot([means[0], means[0] + 1.5*S[1]*U[1,0]], [means[1], means[1] + 1.5*S[1]*U[1,1]],c=&apos;g&apos;, linewidth=3, label=&apos;Second Principal Component&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/09/18/BpEvQxb6NChtocg.png" alt="Figure_31.png"> </p>
<h4 id="Dimensionality-Reduction-with-PCA"><a href="#Dimensionality-Reduction-with-PCA" class="headerlink" title="Dimensionality Reduction with PCA"></a>Dimensionality Reduction with PCA</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def Project_Data(X,U,K): # 数据投影</span><br><span class="line">    return X@U[:,:K]</span><br><span class="line">def Recover_Data(Z,U,K): # 数据重构</span><br><span class="line">    return Z@U[:,:K].T</span><br><span class="line">def Visualizing_Projections(X_normal,X_rec):</span><br><span class="line">    plt.scatter(X_normal[:, 0], X_normal[:, 1], facecolors=&apos;none&apos;, edgecolors=&apos;b&apos;, label=&apos;X_normal&apos;)</span><br><span class="line">    plt.scatter(X_rec[:, 0], X_rec[:, 1], facecolors=&apos;none&apos;, edgecolors=&apos;r&apos;, label=&apos;X_rec&apos;)</span><br><span class="line">    plt.xlim(-4, 3)</span><br><span class="line">    plt.ylim(-4, 3)</span><br><span class="line">    for i in range(X_normal.shape[0]):</span><br><span class="line">        plt.plot([X_normal[i, 0], X_rec[i, 0]], [X_normal[i, 1], X_rec[i, 1]], &apos;k--&apos;)</span><br><span class="line">    plt.legend(loc=2)</span><br><span class="line">    plt.grid(True)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos; Dimensionality Reduction with PCA &apos;&apos;&apos;</span><br><span class="line">Z=Project_Data(X_normal,U,1) # Projecting the data onto the principal components</span><br><span class="line"># print(Z.shape,Z[0]) # except to see: (50, 1) [1.48127391]</span><br><span class="line">X_rec=Recover_Data(Z,U,1)</span><br><span class="line"># print(X_rec.shape,X_rec[0]) # (50, 2) [-1.04741883 -1.04741883]</span><br><span class="line">Visualizing_Projections(X_normal,X_rec)</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/09/18/efmpULIPTAQ7Nrd.png" alt="Figure_32.png"></p>
<h4 id="Face-Image-Dataset"><a href="#Face-Image-Dataset" class="headerlink" title="Face Image Dataset"></a>Face Image Dataset</h4><h5 id="Faces-dataset"><a href="#Faces-dataset" class="headerlink" title="Faces dataset"></a>Faces dataset</h5><p><code>5000*1024</code>的灰度图像，在显示的时候重塑乘<code>32*32</code>的就行。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def Show_Face(X,num):</span><br><span class="line">    plt.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9,hspace=0.02, wspace=0.02)</span><br><span class="line">    for i in range(num):</span><br><span class="line">        plt.subplot(10,10,i+1,xticks=[], yticks=[]) # 不显示刻度</span><br><span class="line">        plt.imshow(X[i].reshape(32,32).T,cmap = &apos;Greys_r&apos;,interpolation=&apos;nearest&apos;) # 指定为灰度，默认RGB</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://i.loli.net/2019/09/18/x2pArQO7HLsEDgF.png" alt="Figure_33.png"></p>
<h5 id="PCA-on-Faces"><a href="#PCA-on-Faces" class="headerlink" title="PCA on Faces"></a>PCA on Faces</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data,X=Get_data(&apos;./ML/ex7/ex7faces.mat&apos;)</span><br><span class="line"># Show_Face(X,100)</span><br><span class="line">X_noraml,means=Feature_Normalize(X)</span><br><span class="line">U,S,V=PCA(X_noraml)</span><br><span class="line"># print(U.shape) # (1024,1024)</span><br><span class="line">Show_Face(U.T,36) # change subplot=6*6</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/09/18/iHpLlSZszIJeqW9.png" alt="Figure_34.png"></p>
<h5 id="Reconstruct-Data"><a href="#Reconstruct-Data" class="headerlink" title="Reconstruct Data"></a>Reconstruct Data</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Z=Project_Data(X_noraml,U,100) # Dimensionality Reduction</span><br><span class="line"># print(Z.shape) # (5000, 100)</span><br><span class="line">X_rec=Recover_Data(Z,U,100)</span><br><span class="line"># print(X_rec.shape) # (5000, 1024)</span><br><span class="line"># Show_Face(X_noraml,100)</span><br><span class="line">Show_Face(X_rec,100)</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/09/18/MwynF8Tb72ex5pV.png" alt="Figure_35.png"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://sfz-lyq.cn/2019/09/16/吴恩达机器学习第七次编程作业/" data-id="ck45pexbd00edsvpvuf8kmbsa" class="article-share-link">分享</a>
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../tags/作业/">作业</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../tags/机器学习/">机器学习</a></li></ul>

    </footer>

  </div>

  
    
  <nav class="article-nav">
    
      <a href="../../22/自然语言处理NLP-一/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            Python自然语言处理NLP(一)
          
        </div>
      </a>
    
    
      <a href="../../15/降维/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">降维</div>
      </a>
    
  </nav>


  

  
    
  

</article>



</section>
    </div>
    <script src="../../../../js/jquery-2.0.3.min.js"></script>
<script src="../../../../js/lazyload.min.js"></script>
<script src="../../../../js/busuanzi-2.3.pure.min.js"></script>


  <script src="../../../../fancybox/jquery.fancybox.min.js"></script>



  <script src="../../../../js/search.js"></script>


<script src="../../../../js/technology.js"></script>

  </div>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body>
</html>