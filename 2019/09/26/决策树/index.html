<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  
    <meta name="keywords" content="NO response">
  
  
    <meta name="description" content="what you will be,the world will be">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    决策树 |
    
    种花家</title>
  
    <link rel="shortcut icon" href="/images/rabbit.png">
  
  <link rel="stylesheet" href="../../../../css/style.css">
  <link rel="stylesheet" href="../../../../css/technology.css">
  
    <link rel="stylesheet" href="../../../../fancybox/jquery.fancybox.min.css">
  
  <script src="../../../../js/pace.min.js"></script>
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <section class="outer">
  <article id="post-决策树" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      决策树
    </h1>
  

      </header>
    

    
      <div class="article-meta">
        <a href class="article-date">
  <time datetime="2019-09-26T09:07:00.000Z" itemprop="datePublished">2019-09-26</time>
</a>
        
  <div class="article-category">
    <a class="article-category-link" href="../../../../categories/机器学习/">机器学习</a>
  </div>

      </div>
    

    <div class="article-entry" itemprop="articleBody">
      

      

      
        <h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://www.bilibili.com/video/av62758431" target="_blank" rel="noopener">学习地址</a></center></h3><hr>
<ol>
<li><a href="https://www.bilibili.com/video/av62758431" target="_blank" rel="noopener">理论篇</a></li>
<li><a href="https://www.bilibili.com/video/av35523476" target="_blank" rel="noopener">实战篇</a></li>
</ol>
<hr>
<h4 id="关于数据"><a href="#关于数据" class="headerlink" title="关于数据"></a><font size="5" color="red">关于数据</font></h4><p>数据集矩阵X:</p>
<ul>
<li>和之前接触到的数据集一样，m个样本（X<sup>(1)</sup>，X<sup>(2)</sup>，X<sup>(3)</sup>，…），每个样本有n个特征，即数据集是一个<code>m×n</code>的矩阵。  </li>
</ul>
<p>数据集划分：</p>
<ul>
<li>我们可以从这个原始数据集中<strong>随机</strong>选取70%作为训练集/测试集，另30%作为验证集。  </li>
</ul>
<p>决策树模型</p>
<ul>
<li>决策树模型类似一个二叉树，非叶节点都是<strong>判定点/决策点</strong>，所有样本都在叶节点，根据非叶节点划分，预测最终所属分类</li>
<li>决策树是一种基本的分类与回归方法，学习通常包含三个步骤：<strong>特征选择</strong>、<strong>决策树的生成</strong>和<strong>决策树的剪枝</strong>。    </li>
<li>一个关键的议题就是该从这n个特征中选取哪个特征作为根节点的分裂点呢？这个涉及到决策树一个很重要的一个概念：<strong>信息熵</strong></li>
</ul>
<p>输出y</p>
<ul>
<li>决策树生成并优化之后，所有样本数据就可以通过这个二叉树进行分类了，确定最终归属</li>
</ul>
<h4 id="熵理论"><a href="#熵理论" class="headerlink" title="熵理论"></a><font size="5" color="red">熵理论</font></h4><p>首先引入之前做的总结：<a href="https://sfz-lyq.cn/2019/12/05/Untitled/">浅谈互信息与熵</a>  </p>
<p><strong>熵</strong>是从热力学引入过来的，最初描述一个系统的混乱程度。在这里，可以理解为一个事件的<strong>不确定性</strong>。<br>先从<strong>信息量</strong>说起，信息量顾名思义就是一个信息或者说一个子事件(可能)的信息量，比如：马云破产、国足夺冠，这种概率很小的事件包含的信息量很大。<br>而我们要研究一整个范围称为一个事件的话，这些子事件就是若干可能的情况，比如我们研究天气，有若干种可能的情况，每种情况有一个<strong>概率</strong>，但不难理解总的概率之和为1。<br>那么，假设某种可能事件x发生的概率为p(x)，那么这个可能事件所包含的信息量的度量方式就是：<code>I(X)=-log p(x)</code>。再回到熵，我们要知道我们所研究的这整个事件的不确定性，其实称之为信息熵就是这些子事件/可能的 信息量的<strong>期望</strong>。用H(X)来度量整体事件X的信息熵，<code>H(X)=-p(x)log p(x    )</code>    </p>
<p><font size="3" color="green">条件熵</font></p>
<ul>
<li>定义H(Y|X)，也等于H(X,Y) - H(X)，其中H(X,Y)表示X，Y的联合熵，具体计算详见之前引用</li>
<li>含义：X确定的情况下，确定Y所需要的信息量，即Y的不确定性</li>
<li>需要注意的是，联合概率联合熵是对称的，而条件熵和条件概率一样是不对称的</li>
</ul>
<h4 id="决策树理论"><a href="#决策树理论" class="headerlink" title="决策树理论"></a><font size="5px" color="red">决策树理论</font></h4><ol>
<li>决策树是一种树形判定结构，每个非叶节点都是表示对一种属性(特征)的测试，每个分支代表一个测试输出，而每个叶节点代表一种类别</li>
<li>决策树学习采用自顶向下的递归方法，其基本思想是以信息熵为度量构造一颗熵值下降最快的树，到叶子节点处的熵值为零（完全确定），每个叶节点中的实例都属于同一类</li>
<li>容易理解这颗判定树越到底层不确定性就越少</li>
<li>决策树的优点是可以进行自学习</li>
</ol>
<h5 id="信息增益-ID3"><a href="#信息增益-ID3" class="headerlink" title="信息增益 - ID3"></a><font size="3px" color="red">信息增益 - ID3</font></h5><p>上图：<br><img src="https://img-blog.csdnimg.cn/20191218163122118.jpg?shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="信息增益"></p>
<h5 id="互信息"><a href="#互信息" class="headerlink" title="互信息"></a><font size="3px" color="red">互信息</font></h5><p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2ltYWdlczIwMTUuY25ibG9ncy5jb20vYmxvZy83ODg3NTMvMjAxNjEwLzc4ODc1My0yMDE2MTAyNzE1MTIxMDg0My03NDUzNDgwMjYucG5n?x-oss-process=image/format,png" alt="互信息"></p>
<h4 id="决策树标记"><a href="#决策树标记" class="headerlink" title="决策树标记"></a><font size="5px" color="red">决策树标记</font></h4><ul>
<li>记： 数据集记为D，样本数m=|D|</li>
<li>记：最终叶节点个数为K，即有K个分类C<sub>K</sub></li>
<li>记：根据不同的特征将数据集划分为{D<sub>1</sub>, … , D<sub>n</sub> }</li>
<li>记：子集D<sub>i</sub>中属于类C<sub>k</sub>的样本集合为D<sub>ik</sub></li>
<li>以上若干标记符号加绝对值符号表示取集合数量</li>
</ul>
<h4 id="ID3流程"><a href="#ID3流程" class="headerlink" title="ID3流程"></a><font size="5px" color="red">ID3流程</font></h4><ol>
<li>计算数据集D的经验熵H(D)=-&sum;(|C<sub>k</sub>| ÷ |D|) × In(|C<sub>k</sub>| ÷ |D|)</li>
<li>遍历所有特征，对于每个特征A<sub>i</sub>，计算经验条件熵H(D|A<sub>i</sub>)</li>
<li>计算信息增益G(D,A<sub>i</sub>)=H(D)-H(D|A<sub>i</sub>)</li>
<li>选择信息增益最大的当做根节点的分裂特征</li>
<li>关于经验条件熵的计算如图<br><img src="https://img-blog.csdnimg.cn/20191218204149145.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="经验条件熵"><br><img src="https://img-blog.csdnimg.cn/20191218182947620.jpg?shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="信息增益计算"></li>
</ol>
<h5 id="信息增益率C4-5-与-Gini系数"><a href="#信息增益率C4-5-与-Gini系数" class="headerlink" title="信息增益率C4.5 与 Gini系数"></a><font size="3px" color="red">信息增益率C4.5 与 Gini系数</font></h5><p>G<sub>r</sub>(D,A)=g(D,A)÷H(A)<br>Gini=&sum;p<sub>k</sub>(1-p<sub>k</sub>) = 1-&sum;p<sub>k</sub><sup>2</sup> = 1-&sum;(|C<sub>k</sub>|÷|D|)<sup>2</sup></p>
<h4 id="决策树评价"><a href="#决策树评价" class="headerlink" title="决策树评价"></a><font size="5px" color="red">决策树评价</font></h4><p><img src="https://img-blog.csdnimg.cn/20191218211751802.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="决策树评价"></p>
<h4 id="决策树实例"><a href="#决策树实例" class="headerlink" title="决策树实例"></a><font size="5px" color="red">决策树实例</font></h4><h5 id="scikit-learn建模基本流程"><a href="#scikit-learn建模基本流程" class="headerlink" title="scikit-learn建模基本流程"></a><font size="3px" color="red">scikit-learn建模基本流程</font></h5><p><img src="https://img-blog.csdnimg.cn/2019121822071283.jpg?shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="sklearn借口调用"></p>
<h5 id="DecisionTree重要参数"><a href="#DecisionTree重要参数" class="headerlink" title="DecisionTree重要参数"></a><font size="3px" color="red">DecisionTree重要参数</font></h5><ol>
<li><code>criterion</code>：用来决定计算方法，可选[<code>&quot;entropy&quot;</code>, <code>&quot;gini&quot;</code>]，<strong>默认基尼系数</strong></li>
<li>比起基尼系数，信息熵对不纯度更加敏感，对不纯度的惩罚最强。但是<strong>在实际使用中，信息熵和基尼系数的效果基本相同</strong>。信息熵的计算比基尼系数缓慢一些，因为基尼系数不涉及对数。</li>
<li><code>random_state</code>: 随机性，一旦这个参数指定值那么每次运行结果都是确定的，<strong>默认随机</strong></li>
<li><code>splitter</code>: 控制决策树中随机选项，可选[<code></code>best, <code>random</code>]，<code>best</code>表示虽然决策树在分枝时虽然是随机的但优先选取更重要的特征进行分枝（重要性可以通过属性<code>feature_importances_</code>查看），而<code>random</code>在分枝时会更随机，树可能会更深，同时也<strong>可以防止 过拟合</strong></li>
</ol>
<h5 id="建树"><a href="#建树" class="headerlink" title="建树"></a><font size="3px" color="red">建树</font></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeClassifier # 导入分类树</span><br><span class="line">from sklearn.datasets import load_wine  # 数据集</span><br><span class="line">from sklearn.model_selection import train_test_split # 区分训练集和测试集</span><br></pre></td></tr></table></figure>
<h5 id="简单实例化"><a href="#简单实例化" class="headerlink" title="简单实例化"></a><font size="3px" color="red">简单实例化</font></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"># 导入数据集</span><br><span class="line">wine=load_wine() # Load and return the wine dataset (classification)</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">wine.data 是特征</span><br><span class="line">wine.feature_names 是属性</span><br><span class="line">print(wine.feature_names) [&apos;alcohol&apos;, &apos;malic_acid&apos;, &apos;ash&apos;,..., &apos;proline&apos;] totally 13</span><br><span class="line">wine.target 是标签</span><br><span class="line">print(wine.target_names) # [&apos;class_0&apos; &apos;class_1&apos; &apos;class_2&apos;]</span><br><span class="line">print(wine.data.shape) # (178, 13)</span><br><span class="line">print(wine.target.shape) # (178,)</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line"># 完整数据集</span><br><span class="line">data=pd.concat([pd.DataFrame(wine.data),pd.DataFrame(wine.target)],axis=1)</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">print(data.shape) # (178, 14)  # DataFrame</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line"># 划分测试集和训练集，其中30%作为测试集，注意赋值顺序固定</span><br><span class="line">X_train,X_test,Y_train,Y_test=train_test_split(wine.data,wine.target,test_size=0.3)</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">print(X_train.shape) # (124, 13)</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line"># 实例化</span><br><span class="line">clf=DecisionTreeClassifier(criterion=&apos;entropy&apos;)</span><br><span class="line"># 使用训练集训练模型</span><br><span class="line">clf=clf.fit(X_train,Y_train)</span><br><span class="line"># 调用接口导出我们所需要的数据，比如精确度</span><br><span class="line">score=clf.score(X_test,Y_test)</span><br><span class="line">print(score) # 0.9259259259259259</span><br></pre></td></tr></table></figure>
<h5 id="剪枝参数"><a href="#剪枝参数" class="headerlink" title="剪枝参数"></a><font size="3px" color="red">剪枝参数</font></h5><p>为了让决策树有更好的泛化性（在测试集上表现同样好），需要对决策树进行剪枝。剪枝策略对决策树影响巨大，正确的剪枝策略是优化决策树算法的核心。</p>
<ol>
<li><code>max_depth=</code>: 限制树的最大深度，超过设定深度的树枝全部剪掉</li>
<li><code>min_samples_leaf=</code>和<code>min_samples_split=</code>: 前者限定一个节点在分枝后每个子节点都必须包含至少min_samples_leaf个训练样本，否则将会被剪掉；后者限定一个节点至少包含min_samples_split个训练样本，这个节点才会被分枝，否者分枝不会被发生</li>
<li><code>max_features=</code>和<code>min_impurity_decrease=</code>: 前者限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃，用来限制高维度过拟合，但如果不知道属性重要性强行剪去特征可能会导致学习不足；后者限制信息增益的大小，信息增益小于设定数值分枝不会发生</li>
</ol>
<h5 id="超参数曲线"><a href="#超参数曲线" class="headerlink" title="超参数曲线"></a><font size="3px" color="red">超参数曲线</font></h5><p>实际就是根据调参将结果绘制出来，衡量模型表现<br>例如：以深度作为参数，精确度作为衡量指标<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeClassifier # 导入分类树</span><br><span class="line">from sklearn.datasets import load_wine  # 数据集</span><br><span class="line">from sklearn.model_selection import train_test_split # 区分训练集和测试集</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"># 导入数据集</span><br><span class="line">wine=load_wine() # Load and return the wine dataset (classification)</span><br><span class="line"># 划分数据集</span><br><span class="line">X_train,X_test,Y_train,Y_test=train_test_split(wine.data,wine.target,test_size=0.3)</span><br><span class="line"></span><br><span class="line">score=[]</span><br><span class="line">for i in range(10):</span><br><span class="line">    clf = DecisionTreeClassifier(max_depth=i+1</span><br><span class="line">                                 ,criterion=&apos;entropy&apos;</span><br><span class="line">                                 ,random_state=30</span><br><span class="line">                                )</span><br><span class="line">    clf = clf.fit(X_train, Y_train)</span><br><span class="line">    score .append(clf.score(X_test, Y_test))</span><br><span class="line"></span><br><span class="line">plt.plot(range(1,11),score,&apos;r&apos;,label=&quot;max_depth&quot;)</span><br><span class="line">plt.legend(loc=&quot;best&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://img-blog.csdnimg.cn/20191219140332401.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="超参数曲线"><br>根据上图可以明显地观察到<code>max_depth</code>取值为3就可以了</p>
<h5 id="重要属性和接口"><a href="#重要属性和接口" class="headerlink" title="重要属性和接口"></a><font size="3px" color="red">重要属性和接口</font></h5><p>这里的属性是指算法模型训练之后能够查看的各种模型的性质</p>
<ol>
<li><code>clf.apply(X_test)</code>：返回每个测试样本所在叶子节点的索引</li>
<li><code>clf.predict(X_test)</code>：返回每个测试样本的预测结果 </li>
</ol>
<h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p><img src="https://img-blog.csdnimg.cn/20191219141024408.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="总结"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://sfz-lyq.cn/2019/09/26/决策树/" data-id="ck4nyprmu00fb9mpvzx0al9ju" class="article-share-link">分享</a>
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../tags/ML/">ML</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../tags/机器学习/">机器学习</a></li></ul>

    </footer>

  </div>

  
    
  <nav class="article-nav">
    
      <a href="../../../10/11/Python面向对象/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            Python面向对象
          
        </div>
      </a>
    
    
      <a href="../Python自然语言处理NLP-六/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">Python自然语言处理NLP(六)</div>
      </a>
    
  </nav>


  

  
    
  

</article>



</section>
    </div>
    <script src="../../../../js/jquery-2.0.3.min.js"></script>
<script src="../../../../js/lazyload.min.js"></script>
<script src="../../../../js/busuanzi-2.3.pure.min.js"></script>


  <script src="../../../../fancybox/jquery.fancybox.min.js"></script>



  <script src="../../../../js/search.js"></script>


<script src="../../../../js/technology.js"></script>

  </div>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body>
</html>