<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  
    <meta name="keywords" content="NO response">
  
  
    <meta name="description" content="what you will be,the world will be">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    浅谈互信息与熵 |
    
    种花家</title>
  
    <link rel="shortcut icon" href="/images/rabbit.png">
  
  <link rel="stylesheet" href="../../../../css/style.css">
  <link rel="stylesheet" href="../../../../css/technology.css">
  
    <link rel="stylesheet" href="../../../../fancybox/jquery.fancybox.min.css">
  
  <script src="../../../../js/pace.min.js"></script>
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <section class="outer">
  <article id="post-浅谈互信息与熵" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      浅谈互信息与熵
    </h1>
  

      </header>
    

    
      <div class="article-meta">
        <a href class="article-date">
  <time datetime="2019-12-05T14:38:00.000Z" itemprop="datePublished">2019-12-05</time>
</a>
        
  <div class="article-category">
    <a class="article-category-link" href="../../../../categories/机器学习/">机器学习</a> / <a class="article-category-link" href="../../../../categories/机器学习/信息论/">信息论</a>
  </div>

      </div>
    

    <div class="article-entry" itemprop="articleBody">
      

      

      
        <h4 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h4><p>最近在做有关曲线相关/似性的方法实现，找到了基于点、形状、分段的方法，考虑到实际研究问题我们选用了基于点的三种方法：DTW（参考<a href="https://blog.csdn.net/qcyfred/article/details/53824507" target="_blank" rel="noopener">1</a>、<a href="https://www.cnblogs.com/luxiaoxun/archive/2013/05/09/3069036.html" target="_blank" rel="noopener">2</a>、<a href="https://blog.csdn.net/raym0ndkwan/article/details/45614813" target="_blank" rel="noopener">3</a>）；LCSS（参考<a href="https://www.cnblogs.com/hugechuanqi/p/10642684.html" target="_blank" rel="noopener">1</a>、<a href="https://blog.csdn.net/NYIST_TC_LYQ/article/details/50826231" target="_blank" rel="noopener">2</a>、<a href="https://www.jianshu.com/nb/33818152" target="_blank" rel="noopener">3</a>）、EDR（参考<a href="https://blog.csdn.net/baodream/article/details/80417695" target="_blank" rel="noopener">1</a>、<a href="https://www.jianshu.com/p/f8f08621fcb5" target="_blank" rel="noopener">2</a>、<a href="https://dl.acm.org/citation.cfm?id=1066213" target="_blank" rel="noopener">3</a>）。<a href="https://paste.ubuntu.com/p/n5KpCXBbSH/" target="_blank" rel="noopener">有关源码</a>   ，非终极版<br>但上述方法还是存在一定的缺陷(如何证明不是自己一厢情愿)，老板又给了一种方法：<a href="https://blog.csdn.net/u011730199/article/details/82935792" target="_blank" rel="noopener">互信息</a><br>本文基于基于互信息，简单介绍一下自己这几天有关熵的补充和理解。</p>
<h4 id="信息量"><a href="#信息量" class="headerlink" title="信息量"></a>信息量</h4><p>就是描述一个信息所需要的数据量。而信息量的大小和这件事的概率有关。简单来说，概率越小，那么信息量越大，比如国足世界杯夺冠（手动狗头）。信息量也可以说是一个事件的不确定性，如果一个事件或者说一个随机变量所有可能的概率之和为1，我这里将一个事件的某种可能记为一条信息，那么这条信息的信息量用I(x<sub>i</sub>)表示。那么<br><img src="https://img-blog.csdnimg.cn/20191205212606736.jpg" alt="信息量"><br>这便是信息量的计算定义，<code>log</code>函数应用非常巧妙，在实际应用中也非常广泛。<br>有了信息量就可以引出信息熵了</p>
<h4 id="信息熵-information-entropy"><a href="#信息熵-information-entropy" class="headerlink" title="信息熵 (information entropy)"></a>信息熵 (information entropy)</h4><p>熵这个概念本质来说我认为是一个期望值，即随机变量所有可能的信息量的期望。通常用H(X)来表示，那么<br><img src="https://img-blog.csdnimg.cn/20191205213920357.png" alt="信息熵"></p>
<h4 id="联合熵-Joint-entropy"><a href="#联合熵-Joint-entropy" class="headerlink" title="联合熵 (Joint entropy)"></a>联合熵 (Joint entropy)</h4><p>联合熵实际是信息熵的推广，如果将一维随机变量推广到多维，以二维为例，那么<br><img src="https://img-blog.csdnimg.cn/20191205214436127.png" alt="联合熵"><br>其中，P(x<sub>i</sub>, y<sub>i</sub>)为联合概率</p>
<h4 id="条件熵-Conditional-entropy"><a href="#条件熵-Conditional-entropy" class="headerlink" title="条件熵  (Conditional entropy)"></a>条件熵  (Conditional entropy)</h4><p>条件熵和条件概率类似，H(Y|X)就表示随机变量X已知的情况下随机变量Y的不确定性，X是随机的故有多种可能，那么条件熵就是在X取遍所有可能的期望<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZXMyMDE4LmNuYmxvZ3MuY29tL2Jsb2cvMTM2MTA0Mi8yMDE4MDQvMTM2MTA0Mi0yMDE4MDQwMzA4NTg0NzgzOC0xMzAxNTI4ODYucG5n?x-oss-process=image/format,png" alt="条件熵"><br>其中，P(y|x)为条件概率<br>当然，公式还可以化简：<strong>H(Y|X)=H(X,Y)−H(X)</strong>，具体推导详见参考</p>
<h4 id="相对熵-Relative-entropy-这部分理解不是很懂"><a href="#相对熵-Relative-entropy-这部分理解不是很懂" class="headerlink" title="相对熵 (Relative entropy) [这部分理解不是很懂]"></a>相对熵 (Relative entropy) [这部分理解不是很懂]</h4><p>相对熵<strong>也称KL散度 (Kullback–Leibler divergence)</strong>，用来刻画两个分布之间的差异性，设 p(x)、q(x) 是 离散随机变量 X 中取值的两个概率分布，则 p 对 q 的相对熵是：<br><img src="https://img-blog.csdnimg.cn/20191205220707986.png" alt="相对熵"><br>相对熵的值越小，表示q分布和p分布越接近。<br>这个并不像联合熵那样具有对称性。</p>
<h4 id="交叉熵-（不懂，略）"><a href="#交叉熵-（不懂，略）" class="headerlink" title="交叉熵 （不懂，略）"></a>交叉熵 （不懂，略）</h4><h4 id="互信息"><a href="#互信息" class="headerlink" title="互信息"></a>互信息</h4><p>一个随即变量中包含的关于另一个随机变量的信息量，或者说知道X，给Y的信息量带来多少损失（或者反过来），关于这损失也可以说是另一个事件的确定性的增加。可以用来度量两个随机变量的相互依赖性。<br>定义如下：<br><img src="https://img-blog.csdn.net/20180519144113545" alt="互信息"><br>其中，<em>p(x)、p(y)</em>为边缘概率<br>简化推导：<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9nc3MwLmJkc3RhdGljLmNvbS85NG8zZFNhZ194STRraEdrcG9XSzFIRjZoaHkvYmFpa2UvcGljL2l0ZW0vZDc4OGQ0M2Y4Nzk0YTRjMmI1YWJkN2I0MGFmNDFiZDVhYzZlMzlhMC5qcGc?x-oss-process=image/format,png" alt="互信息简化"><br>互信息具有对称性</p>
<h4 id="互信息、条件熵与联合熵联系"><a href="#互信息、条件熵与联合熵联系" class="headerlink" title="互信息、条件熵与联合熵联系"></a>互信息、条件熵与联合熵联系</h4><p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2ltYWdlczIwMTUuY25ibG9ncy5jb20vYmxvZy83ODg3NTMvMjAxNjEwLzc4ODc1My0yMDE2MTAyNzE1MTIxMDg0My03NDUzNDgwMjYucG5n?x-oss-process=image/format,png" alt="联系"></p>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><ol>
<li><a href="https://www.cnblogs.com/kyrieng/p/8694705.html" target="_blank" rel="noopener">详解机器学习中的熵、条件熵、相对熵和交叉熵</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/26486223" target="_blank" rel="noopener">通俗理解信息熵</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/26551798" target="_blank" rel="noopener">通俗理解条件熵</a></li>
<li><a href="https://blog.csdn.net/weixin_41806692/article/details/82463577" target="_blank" rel="noopener">关于熵的知识——信息论基本概念</a></li>
<li><a href="https://www.jianshu.com/p/c0e05f042b01" target="_blank" rel="noopener">最大熵模型</a> ，有关条件熵推导公式有误，缺少一个负号</li>
<li><a href="https://blog.csdn.net/u010041824/article/details/70224597" target="_blank" rel="noopener">条件熵的定义</a></li>
<li><a href="https://www.jianshu.com/p/541aa736195b" target="_blank" rel="noopener">自信息和互信息、信息熵</a></li>
<li><a href="https://blog.csdn.net/ranghanqiao5058/article/details/78458815" target="_blank" rel="noopener">信息熵，条件熵，互信息的通俗理解</a></li>
<li><a href="https://blog.csdn.net/github_36299736/article/details/74278803" target="_blank" rel="noopener">互信息——事件相关性度量</a></li>
<li><a href="https://blog.csdn.net/u011322987/article/details/83751213" target="_blank" rel="noopener">互信息、条件互信息</a></li>
<li><a href="https://blog.csdn.net/u011730199/article/details/82935792" target="_blank" rel="noopener">互信息（Mutual Information）的介绍</a></li>
<li><a href="https://baike.baidu.com/item/%E4%BA%92%E4%BF%A1%E6%81%AF/7423853?fr=aladdin" target="_blank" rel="noopener">百度百科-互信息</a></li>
<li><a href="https://baike.baidu.com/item/%E8%81%94%E5%90%88%E7%86%B5/22709235?fr=aladdin" target="_blank" rel="noopener">百度百科-联合熵</a></li>
<li><a href="https://www.douban.com/note/621588501/" target="_blank" rel="noopener">关于互信息的一些注记</a></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://sfz-lyq.cn/2019/12/05/浅谈互信息与熵/" data-id="ck4grj0mf007pjdpvccjeq2s2" class="article-share-link">分享</a>
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../tags/ML/">ML</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../tags/信息论/">信息论</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../tags/机器学习/">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../tags/概率论/">概率论</a></li></ul>

    </footer>

  </div>

  
    
  <nav class="article-nav">
    
      <a href="../../17/大规模机器学习/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            大规模机器学习
          
        </div>
      </a>
    
    
      <a href="../../03/条件概率-全概率-贝叶斯公式-个人理解/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">条件概率/全概率/贝叶斯公式 个人理解</div>
      </a>
    
  </nav>


  

  
    
  

</article>



</section>
    </div>
    <script src="../../../../js/jquery-2.0.3.min.js"></script>
<script src="../../../../js/lazyload.min.js"></script>
<script src="../../../../js/busuanzi-2.3.pure.min.js"></script>


  <script src="../../../../fancybox/jquery.fancybox.min.js"></script>



  <script src="../../../../js/search.js"></script>


<script src="../../../../js/technology.js"></script>

  </div>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body>
</html>