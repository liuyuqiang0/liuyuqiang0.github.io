<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>XGBoost</title>
    <url>/2019/12/27/XGBoost/</url>
    <content><![CDATA[<h3 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a><table><td bgcolor="gren"><font size="5px" color="b">XGBoost</font></td></table></h3><p>参考</p>
<ol>
<li><a href="https://cosx.org/2015/03/xgboost/" target="_blank" rel="noopener">xgboost: 速度快效果好的 boosting 模型</a></li>
<li><a href="https://www.bilibili.com/video/av43162159?p=2" target="_blank" rel="noopener">菜菜的sklearn课堂11 -XGBoost</a></li>
<li><a href="http://file.sh.peixun.net/file/201902/11/201902110008wdrryyy669.pdf" target="_blank" rel="noopener">课件</a></li>
<li></li>
</ol>
<hr>
<h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a><font size="4px" color="red">概述</font></h4><p><code>XGBoost</code>全称是eXtreme Gradient Boosting，可译为<strong>极限梯度提升算法</strong>。<strong>它由<a href="https://cosx.org/2015/06/interview-of-tianqi/" target="_blank" rel="noopener">陈天奇</a>所设计，致力于让提升树突破自身的计算极限，以实现运算快速，性能优秀的工程目标</strong>。和传统的梯度提升算法相比，XGBoost进行了许多改进，它能够比其他使用梯度提升的集成算法更加快速，并且已经被认为是在分类和回归上都拥有超高性能的先进评估器。</p>
<p>从2016年开始，各大竞赛平台排名前列的解决方案逐渐由XGBoost算法统治，业界甚至将其称之为“机器学习竞赛的胜利女神”。</p>
<h4 id="xgboost库与XGB的sklearn-API"><a href="#xgboost库与XGB的sklearn-API" class="headerlink" title="xgboost库与XGB的sklearn API"></a><font size="4px" color="red">xgboost库与XGB的sklearn API</font></h4><ol>
<li><code>xgboost</code>是一个独立的，开源的，专门提供梯度提升树以及XGBoost算法应用的算法库。它和sklearn类似，有一个详细的官方网站可以供我们查看，并且可以与C，Python，R，Julia等语言连用，但需要我们单独安装和下载<br>官方文档：<a href="https://xgboost.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">xgboost documents</a><br>xgboost建模流程：对数据格式有特定要求，参数需要提前设置(字典格式)，<br><img src="https://img-blog.csdnimg.cn/20191227171149260.jpg" alt="xgboost建模"></li>
<li><code>sklearn的API</code>，建模流程和其他的sklearn算法类似，属性和接口也类似<br>比如：<code>class xgboost.XGBRegressor (max_depth=3, learning_rate=0.1, n_estimators=100, silent=True,objective=&#39;reg:linear&#39;, booster=&#39;gbtree&#39;, n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, importance_type=&#39;gain&#39;, **kwargs)</code></li>
</ol>
<h4 id="XGBoost的三大板块"><a href="#XGBoost的三大板块" class="headerlink" title="XGBoost的三大板块"></a><font size="4px" color="red">XGBoost的三大板块</font></h4><p>XGBoost本身的核心是基于梯度提升树实现的集成算法，整体来说可以有三个核心部分：集成算法本身，用于集成的<br>弱评估器，以及应用中的其他过程。</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>集成算法</th>
<th>弱评估器</th>
<th>其他过程</th>
</tr>
</thead>
<tbody>
<tr>
<td>n_estimators</td>
<td>√</td>
<td></td>
<td></td>
</tr>
<tr>
<td>learning_rate</td>
<td>√</td>
<td></td>
<td></td>
</tr>
<tr>
<td>silent</td>
<td>√</td>
<td></td>
<td></td>
</tr>
<tr>
<td>subsample</td>
<td>√</td>
<td></td>
<td></td>
</tr>
<tr>
<td>max_depth</td>
<td></td>
<td>√</td>
<td></td>
</tr>
<tr>
<td>objective</td>
<td></td>
<td>√</td>
<td></td>
</tr>
<tr>
<td>booster</td>
<td></td>
<td>√</td>
<td></td>
</tr>
<tr>
<td>gamma</td>
<td></td>
<td>√</td>
<td></td>
</tr>
<tr>
<td>min_child_weight</td>
<td></td>
<td>√</td>
<td></td>
</tr>
<tr>
<td>max_delta_step</td>
<td></td>
<td>√</td>
<td></td>
</tr>
<tr>
<td>colsample_bytree</td>
<td></td>
<td>√</td>
<td></td>
</tr>
<tr>
<td>colsample_bylevel</td>
<td></td>
<td>√</td>
<td></td>
</tr>
<tr>
<td>reg_alpha</td>
<td></td>
<td>√</td>
<td></td>
</tr>
<tr>
<td>reg_lambda</td>
<td></td>
<td>√</td>
<td></td>
</tr>
<tr>
<td>nthread</td>
<td></td>
<td></td>
<td>√</td>
</tr>
<tr>
<td>n_jobs</td>
<td></td>
<td></td>
<td>√</td>
</tr>
<tr>
<td>scale_pos_weight</td>
<td></td>
<td></td>
<td>√</td>
</tr>
<tr>
<td>base_score</td>
<td></td>
<td></td>
<td>√</td>
</tr>
<tr>
<td>seed</td>
<td></td>
<td></td>
<td>√</td>
</tr>
<tr>
<td>random_state</td>
<td></td>
<td></td>
<td>√</td>
</tr>
<tr>
<td>missing</td>
<td></td>
<td></td>
<td>√</td>
</tr>
<tr>
<td>importance_type</td>
<td></td>
<td></td>
<td>√</td>
</tr>
</tbody>
</table>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ML</tag>
        <tag>XGBoost</tag>
        <tag>数据竞赛</tag>
      </tags>
  </entry>
  <entry>
    <title>朴素贝叶斯</title>
    <url>/2019/12/26/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/</url>
    <content><![CDATA[<h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a><table><td bgcolor="gren"><center><font size="5px" color="b">朴素贝叶斯</font></center></td></table></h3><p>参考：</p>
<ol>
<li><a href="https://www.bilibili.com/video/av40337118" target="_blank" rel="noopener">菜菜的sklearn课堂10 - 朴素贝叶斯</a>  </li>
<li><a href="http://file.sh.peixun.net/file/201901/10/201901100005ts1yyj881y.pdf" target="_blank" rel="noopener">课件</a>  </li>
<li><a href="https://sfz-lyq.cn/2019/12/03/%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87-%E5%85%A8%E6%A6%82%E7%8E%87-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F-%E4%B8%AA%E4%BA%BA%E7%90%86%E8%A7%A3/">条件概率/全概率/贝叶斯公式 个人理解</a></li>
</ol>
<hr>
<h4 id="真正的概率分类器"><a href="#真正的概率分类器" class="headerlink" title="真正的概率分类器"></a><font size="4px" color="red">真正的概率分类器</font></h4><p>为什么称之为<strong>真正</strong>?  </p>
<ul>
<li>在之前学习的算法中，我们都明白只根据训练集训练的模型是无法给出<strong>肯定的答案的</strong>，它只能尽量预测正确。而这个尽量，是人为强行定义的。比如逻辑回归的sigmoid函数，只要大于0.5我们就将其分为1。  <ul>
<li>朴素贝叶斯是一种直接衡量标签和特征之间的概率关系的有监督算法，它既可以做回归也可以分类，不过多是用于分类之中。朴素贝叶斯的算法根源就是基于概率论和数理统计的贝叶斯理论，因此它是根正苗红的概率模型。也就是说它是真正基于概率的算法，真正计算的概率。</li>
</ul>
</li>
</ul>
<p>关于贝叶斯公式详见参考：<br><img src="https://img-blog.csdnimg.cn/20191227132950730.jpg" alt="贝叶斯公式"><br>这即是一切贝叶斯算法的根源理论。</p>
<h5 id="后验概率"><a href="#后验概率" class="headerlink" title="后验概率"></a><font size="3px" color="red">后验概率</font></h5><p>把特征X当成条件事件，要求解的标签Y当成是被满足条件后会被影响的结果，两者之间的概率关系就是P(Y|X)，在机器学习中这个概率公式称之为是标签的<strong>后验概率</strong>（posterior probability），即是说我们先知道了条件，再去求解结果</p>
<h5 id="先验概率"><a href="#先验概率" class="headerlink" title="先验概率"></a><font size="3px" color="red">先验概率</font></h5><p>标签Y在没有任何条件限制下取值为某个值的概率记作P(Y)，称之为标签的<strong>先验概率</strong>（prior probability）</p>
<h5 id="为什么叫朴素"><a href="#为什么叫朴素" class="headerlink" title="为什么叫朴素"></a><font size="3px" color="red">为什么叫朴素</font></h5><p>在机器学习中，对于每一个样本，都可能是包含n个特征的特征向量<strong>X</strong>={X<sub>1</sub>, X<sub>2</sub>, …. , X<sub>n</sub>}，那么：<br><img src="https://img-blog.csdnimg.cn/20191227151412857.jpg" alt="后验概率"><br>如果特征之间是<strong>独立的</strong>，那么可以结合条件概率与全概率化简进而求得P(X)，P(X=X<sub>1,2,3…</sub>|Y):<br><img src="https://img-blog.csdnimg.cn/20191227151833976.jpg" alt><br>简单来说，如果特征之间是有条件独立的，可以解决众多问题也可以简化很多计算，这就是朴素贝叶斯被称为“朴素”的理由。</p>
<p><strong>朴素贝叶斯是一个不建模的算法</strong>，所需要的样本量比较少。当然，如果样本量少于特征数目，贝叶斯的效果就会被削弱。</p>
<h4 id="sklearn中的朴素贝叶斯"><a href="#sklearn中的朴素贝叶斯" class="headerlink" title="sklearn中的朴素贝叶斯"></a><font size="4px" color="red">sklearn中的朴素贝叶斯</font></h4><table>
<thead>
<tr>
<th>类</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>naive_bayes.BernoulliNB</td>
<td>伯努利分布下的朴素贝叶斯</td>
</tr>
<tr>
<td>naive_bayes.GaussianNB</td>
<td>高斯分布下的朴素贝叶斯</td>
<td></td>
</tr>
<tr>
<td>naive_bayes.MultinomialNB</td>
<td>多项式分布下的朴素贝叶斯</td>
</tr>
<tr>
<td>naive_bayes.ComplementNB</td>
<td>补充朴素贝叶斯</td>
</tr>
<tr>
<td>linear_model.BayesianRidge</td>
<td>贝叶斯岭回归，在参数估计过程中使用贝叶斯回归技术来包括正则化参数</td>
</tr>
</tbody>
</table>
<h4 id="高斯朴素贝叶斯GaussianNB"><a href="#高斯朴素贝叶斯GaussianNB" class="headerlink" title="高斯朴素贝叶斯GaussianNB"></a><font size="4px" color="red">高斯朴素贝叶斯GaussianNB</font></h4><ul>
<li><code>class sklearn.naive_bayes.GaussianNB (priors=None, var_smoothing=1e-09)</code><br>通过假设某一类的特征服从正态分布，来估计每个特征下每个类别上的条件概率<br><img src="https://img-blog.csdnimg.cn/20191227155254627.jpg" alt="高斯贝叶斯"></li>
</ul>
<table>
<thead>
<tr>
<th>参数</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>prior</td>
<td>可输入任何类数组结构，形状为（n_classes，）表示类的先验概率。如果指定，则不根据数据调整先验，如果不指定，则自行根据数据计算先验概率 P(Y)</td>
</tr>
<tr>
<td>var_smoothing</td>
<td>浮点数，可不填（默认值= 1e-9）在估计方差时，为了追求估计的稳定性，将所有特征的方差中最大的方差以某个比例添加到估计的方差中。这个比例，由var_smoothing参数控制。</td>
</tr>
</tbody>
</table>
<p>实例：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.naive_bayes import GaussianNB</span><br><span class="line">from sklearn.datasets import load_digits</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">digits = load_digits()</span><br><span class="line">X, y = digits.data, digits.target</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest = train_test_split(X,y,test_size=0.3,random_state=420)</span><br><span class="line"></span><br><span class="line">gnb = GaussianNB().fit(Xtrain,Ytrain) # 建模</span><br><span class="line"></span><br><span class="line">#查看分数</span><br><span class="line">acc_score = gnb.score(Xtest,Ytest) # 预测</span><br><span class="line"></span><br><span class="line">#查看预测结果</span><br><span class="line">Y_pred = gnb.predict(Xtest)  # 返回每个样本的预测分类</span><br><span class="line"></span><br><span class="line">#查看预测的概率结果</span><br><span class="line">prob = gnb.predict_proba(Xtest) # 返回一个矩阵，每个样本所属每个分类的概率</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ML</tag>
        <tag>贝叶斯</tag>
        <tag>Sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title>sklearn中的线性回归</title>
    <url>/2019/12/26/sklearn%E4%B8%AD%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h3 id="sklearn中的线性回归"><a href="#sklearn中的线性回归" class="headerlink" title="sklearn中的线性回归"></a><table><tr><td bgcolor="greey"><center><font size="5px" color="b">sklearn中的线性回归</font></center></td></tr></table></h3><p>参考</p>
<ol>
<li><a href="https://www.bilibili.com/video/av39811682" target="_blank" rel="noopener">菜菜的sklearn课堂09 - 线性回归和回归类模型的评估指标（非完整版）</a></li>
<li><a href="http://file.sh.peixun.net/file/201901/02/201901022212bcpxh53gnd.pdf" target="_blank" rel="noopener">课件</a></li>
</ol>
<hr>
<h4 id="sklearn中的线性回归-1"><a href="#sklearn中的线性回归-1" class="headerlink" title="sklearn中的线性回归"></a><font size="4px" color="red">sklearn中的线性回归</font></h4><ul>
<li>普通线性回归<code>LinearRegression</code></li>
</ul>
<table>
<thead>
<tr>
<th>类/函数</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>linear_model.LinearRegression</td>
<td>使用普通最小二乘法的线性回归</td>
</tr>
</tbody>
</table>
<ul>
<li>岭回归<code>Ridge</code></li>
</ul>
<table>
<thead>
<tr>
<th>类/函数</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>linear_model.Ridge</td>
<td>岭回归，一种将L2作为正则化工具的线性最小二乘回归</td>
</tr>
<tr>
<td>linear_model.RidgeCV</td>
<td>带交叉验证的岭回归</td>
</tr>
<tr>
<td>linear_model.RidgeClassifier</td>
<td>岭回归的分类器</td>
</tr>
<tr>
<td>linear_model.RidgeClassifierCV</td>
<td>带交叉验证的岭回归的分类器</td>
</tr>
<tr>
<td>linear_model.ridge_regression</td>
<td>【函数】用正太方程法求解岭回归</td>
</tr>
</tbody>
</table>
<ul>
<li>LASSO</li>
</ul>
<table>
<thead>
<tr>
<th>类/函数</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>linear_model.Lasso</td>
<td>Lasso，使用L1作为正则化工具来训练的线性回归模型</td>
</tr>
<tr>
<td>linear_model.LassoCV</td>
<td>带交叉验证和正则化迭代路径的Lasso</td>
</tr>
<tr>
<td>linear_model.LassoLars</td>
<td>使用最小角度回归求解的Lasso</td>
</tr>
<tr>
<td>linear_model.LassoLarsCV</td>
<td>带交叉验证的使用最小角度回归求解的Lasso</td>
</tr>
<tr>
<td>linear_model.LassoLarsIC</td>
<td>使用BIC或AIC进行模型选择的，使用最小角度回归求解的Lasso</td>
</tr>
<tr>
<td>linear_model.MultiTaskLasso</td>
<td>使用L1 / L2混合范数作为正则化工具训练的多标签Lasso</td>
</tr>
<tr>
<td>linear_model.MultiTaskLassoCV</td>
<td>使用L1 / L2混合范数作为正则化工具训练的，带交叉验证的多标签Lasso</td>
</tr>
<tr>
<td>linear_model.lasso_path</td>
<td>【函数】用坐标下降计算Lasso路径</td>
</tr>
</tbody>
</table>
<ul>
<li>弹性网</li>
</ul>
<table>
<thead>
<tr>
<th>类/函数</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>linear_model.ElasticNet</td>
<td>弹性网，一种将L1和L2组合作为正则化工具的线性回归</td>
</tr>
<tr>
<td>linear_model.ElasticNetCV</td>
<td>带交叉验证和正则化迭代路径的弹性网</td>
</tr>
<tr>
<td>linear_model.MultiTaskElasticNet</td>
<td>多标签弹性网</td>
</tr>
<tr>
<td>linear_model.MultiTaskElasticNetCV</td>
<td>带交叉验证的多标签弹性网</td>
</tr>
<tr>
<td>linear_model.enet_path</td>
<td>【函数】用坐标下降法计算弹性网的路径</td>
</tr>
</tbody>
</table>
<ul>
<li>最小角度回归</li>
</ul>
<table>
<thead>
<tr>
<th>类/函数</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>linear_model.Lars</td>
<td>最小角度回归（Least Angle Regression，LAR）</td>
</tr>
<tr>
<td>linear_model.LarsCV</td>
<td>带交叉验证的最小角度回归模型</td>
</tr>
<tr>
<td>linear_model.lars_path</td>
<td>【函数】使用LARS算法计算最小角度回归路径或Lasso的路径</td>
</tr>
</tbody>
</table>
<ul>
<li>正交匹配追踪</li>
</ul>
<table>
<thead>
<tr>
<th>类/函数</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>linear_model.OrthogonalMatchingPursuit</td>
<td>正交匹配追踪模型（OMP）</td>
</tr>
<tr>
<td>linear_model.OrthogonalMatchingPursuitCV</td>
<td>交叉验证的正交匹配追踪模型（OMP）</td>
</tr>
<tr>
<td>linear_model.orthogonal_mp</td>
<td>【函数】正交匹配追踪（OMP）</td>
</tr>
<tr>
<td>linear_model.orthogonal_mp_gram</td>
<td>【函数】Gram正交匹配追踪（OMP）</td>
</tr>
</tbody>
</table>
<ul>
<li>贝叶斯回归</li>
</ul>
<table>
<thead>
<tr>
<th>类/函数</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>linear_model.ARDRegression</td>
<td>贝叶斯ARD回归。ARD是自动相关性确定回归 （Automatic Relevance Determination Regression）， 是一种类似于最小二乘的，用来计算参数向量的数学方 法。</td>
</tr>
<tr>
<td>linear_model.BayesianRidge</td>
<td>贝叶斯岭回归</td>
</tr>
</tbody>
</table>
<ul>
<li>其他回归</li>
</ul>
<table>
<thead>
<tr>
<th>类/函数</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>linear_model.PassiveAggressiveClassifier</td>
<td>被动攻击性分类器</td>
</tr>
<tr>
<td>linear_model.PassiveAggressiveRegressor</td>
<td>被动攻击性回归</td>
</tr>
<tr>
<td>linear_model.Perceptron</td>
<td>感知机</td>
</tr>
<tr>
<td>linear_model.RANSACRegressor</td>
<td>RANSAC（RANdom SAmple Consensus）算法。</td>
</tr>
<tr>
<td>linear_model.HuberRegressor</td>
<td>胡博回归，对异常值具有鲁棒性的一种线性回归模型</td>
</tr>
<tr>
<td>linear_model.SGDRegressor</td>
<td>通过最小化SGD的正则化损失函数来拟合线性模型</td>
</tr>
<tr>
<td>linear_model.TheilSenRegressor</td>
<td>Theil-Sen估计器，一种鲁棒的多元回归模型</td>
</tr>
</tbody>
</table>
<h4 id="多元线性回归LinearRegression"><a href="#多元线性回归LinearRegression" class="headerlink" title="多元线性回归LinearRegression"></a><font size="4px" color="red">多元线性回归LinearRegression</font></h4><p>虽然之前的吴恩达视频中已经有了关于单变量/多变量线性回归的简单讲解，不过在具体应用上使用的梯度下降法结合<code>scipy.optimize</code>中的优化器，但是尚不熟悉。这里为了了解sklearn，又重新作补充。</p>
<p>回归问题的经典案例是<strong>房价预测问题</strong>，这个预测函数可以表示成参数向量和特征的简单线性组合。本质则是通过损失函数的定义求得最佳参数向量。</p>
<p>简单的L2范数形式的损失函数：不带正则项<br><img src="https://img-blog.csdnimg.cn/20191226152826165.jpg" alt="损失函数"><br>我们往往称呼这个上式为<strong>SSE</strong>（Sum of Sqaured Error，误差平方和）或者<strong>RSS</strong>（Residual Sum of Squares 残差平方和）。<strong>在sklearn所有官方文档和网页上都称之为RSS残差平方和</strong>。</p>
<p>关于参数的矩阵形式求导（算法工程师基本要求）： <a href="https://en.wikipedia.org/wiki/Matrix_calculus" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Matrix_calculus
</a></p>
<h4 id="linear-model-LinearRegression"><a href="#linear-model-LinearRegression" class="headerlink" title=" linear_model.LinearRegression"></a><font size="4px" color="red"> linear_model.LinearRegression</font></h4><ul>
<li><code>class sklearn.linear_model.LinearRegression (fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)</code></li>
<li>参数解释</li>
</ul>
<table>
<thead>
<tr>
<th>参数</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>fit_intercept</td>
<td>布尔值，可不填，默认为True 是否计算此模型的截距。如果设置为False，则不会计算截距。</td>
</tr>
<tr>
<td>normalize</td>
<td>布尔值，可不填，默认为False。当fit_intercept设置为False时，将忽略此参数。如果为True，则特征矩阵X在进入回 之前 将会被减去均值（中心化）并除以L2范式（缩放）。如果你希望进行标准化，请在fit数据 前使用preprocessing模块中的标准化专用类StandardScaler。</td>
</tr>
<tr>
<td>copy_X</td>
<td>b布尔值，可不填，默认为True。如果为真，将在X.copy()上进行操作，否则的话原本的特征矩阵X可能被线性回归影响并覆盖。</td>
</tr>
<tr>
<td>n_jobs</td>
<td>整数或者None，可不填，默认为None 用于计算的作业数。只在多标签的回归和数据量足够大的时候才生效。除非None在joblib.parallel_backend上下文中，否则None统一表示为1。如果输入 -1，则表示使用全 部的CPU来进行计算。更多详细内容，请参阅词汇表：<a href="https://scikit-learn.org/stable/glossary.html#term-n-jobs" target="_blank" rel="noopener">https://scikit-learn.org/stable/glossary.html#term-n-jobs</a></td>
</tr>
</tbody>
</table>
<h4 id="实践"><a href="#实践" class="headerlink" title="实践"></a><font size="4px" color="red">实践</font></h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from sklearn.linear_model import LinearRegression as LR</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line">from sklearn.datasets import fetch_california_housing as fch #加利福尼亚房屋价值数据集</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">housevalue = fch() #没有下载过需要下载</span><br><span class="line"></span><br><span class="line">X=housevalue.data</span><br><span class="line">y=housevalue.target</span><br><span class="line">print(X.shape,y.shape)</span><br><span class="line">print(housevalue.feature_names) # 输出所有特征名称</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">(20640, 8) (20640,)</span><br><span class="line">[&apos;MedInc&apos;, &apos;HouseAge&apos;, &apos;AveRooms&apos;, &apos;AveBedrms&apos;, &apos;Population&apos;, &apos;AveOccup&apos;, &apos;Latitude&apos;, &apos;Longitude&apos;] </span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line"># 分训练集和测试集</span><br><span class="line">Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,y,test_size=0.3,random_state=420)</span><br><span class="line"></span><br><span class="line"># 建模</span><br><span class="line">reg = LR().fit(Xtrain, Ytrain) # 实例化+ fit来训练模型</span><br><span class="line">yhat = reg.predict(Xtest) # 接口</span><br><span class="line"></span><br><span class="line"># 探索建好的模型-属性</span><br><span class="line">print(reg.coef_) # 参数向量</span><br><span class="line"># *zip(Xtrain.columns,reg.coef_) # *zip 表示将数据打包变成一组一组的形式，可以转化成列表用tolist，或者[*]</span><br><span class="line">print([*zip(housevalue.feature_names,reg.coef_)]) # 这个含义是特征对应的参数</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">[ 4.37358931e-01  1.02112683e-02 -1.07807216e-01  6.26433828e-01 5.21612535e-07 -3.34850965e-03 -4.13095938e-01 -4.26210954e-01]</span><br><span class="line">[(&apos;MedInc&apos;, 0.4373589305968407), (&apos;HouseAge&apos;, 0.01021126829449407), (&apos;AveRooms&apos;, -0.10780721617317679), (&apos;AveBedrms&apos;, 0.6264338275363791), </span><br><span class="line">(&apos;Population&apos;, 5.216125353556256e-07), (&apos;AveOccup&apos;, -0.003348509646333535), (&apos;Latitude&apos;, -0.41309593789477095), (&apos;Longitude&apos;, -0.42621095362084693)]</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>属性</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>coef_</td>
<td>参数数组，形状为 (n_features, )或者(n_targets, n_features) 线性回归方程中估计出的系数。如果在fit中传递多个标签（当y为二维或以上的时候），则返回的系数是形状为（n_targets，n_features）的二维数组，而如果仅传递一个标签，则返回 的系数是长度为n_features的一维数组或说向量</td>
</tr>
<tr>
<td>intercept_</td>
<td>数组，线性回归中的截距项</td>
</tr>
</tbody>
</table>
<h4 id="多元线性回归的模型评估指标"><a href="#多元线性回归的模型评估指标" class="headerlink" title="多元线性回归的模型评估指标"></a><font size="4px" color="red">多元线性回归的模型评估指标</font></h4><p>回归类算法的模型评估一直都是回归算法中的一个难点，但不像我们曾经讲过的无监督学习算法中的轮廓系数等等评估指标，回归类与分类型算法的模型评估其实是相似的法则——找真实标签和预测值的差异。只不过在分类型算法中，这个差异只有一种角度来评判，那就是是否预测到了正确的分类，而在我们的回归类算法中，我们有两种不同的角度来看待回归的效果：</p>
<ul>
<li>我们是否预测到了正确的数值。</li>
<li>我们是否拟合到了足够的信息。</li>
</ul>
<p>这两种角度，分别对应着不同的模型评估指标</p>
<h5 id="是否预测了正确的数值"><a href="#是否预测了正确的数值" class="headerlink" title="是否预测了正确的数值"></a><font size="3px" color="red">是否预测了正确的数值</font></h5><p>RSS残差平方和，它的本质是我们的预测值与真实值之间的差异，也就是从第一种角度来评估我们回归的效力，所以RSS既是我们的损失函数，也是我们回归类模型的模型评估指标之一。但是，RSS有着致命的缺点：<strong>它是一个无界的和，可以无限地大</strong>。我们希望这个RSS越小越好，但并没有一个概念究竟多小才好。</p>
<p>为了应对这种状况，sklearn中使用RSS的变体，均方误差MSE（mean squared error）来衡量我们的预测值和真实值的差异：<br><img src="https://img-blog.csdnimg.cn/20191226160829831.jpg" alt="均方误差"><br>均方误差，本质是在RSS的基础上除以了样本总量，得到了每个样本量上的平均误差<br>在sklearn当中，我们有两种方式调用这个评估指标，一种是使用sklearn专用的模型评估模块<strong>metrics</strong>里的类<strong>mean_squared_error</strong>，另一种是调用交叉验证的类<strong>cross_val_score</strong>并使用里面的<strong>scoring参数</strong>来设置使用均方误差。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from sklearn.metrics import mean_squared_error as MSE</span><br><span class="line">print(MSE(yhat,Ytest),y.max(),y.min()) # 0.5309012639324554 5.00001 0.14999</span><br><span class="line"># 发现平均误差可到0.5，然而实际数据最小值才0.14，这个误差50%就有点大了，这个模型可能就不太好</span><br><span class="line"></span><br><span class="line">print(cross_val_score(reg,X,y,cv=10,scoring=&quot;neg_mean_squared_error&quot;)) # 返回每次交叉验证的均方误差</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">[-0.48922052 -0.43335865 -0.8864377  -0.39091641 -0.7479731  -0.52980278</span><br><span class="line"> -0.28798456 -0.77326441 -0.64305557 -0.3275106 ]</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure></p>
<p>说明：</p>
<ol>
<li><code>sklearn</code>中评估指标并没有<code>mean_squared_error</code>，而是<code>neg_mean_squared_error</code></li>
<li>但是返回的10个交叉验证的误差为什么为负数？<br>均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)。在sklearn当中，所有的损失都使用负数表示，因此均方误差也被显示为负数了。真正的均方误差MSE的数值，其实就 是neg_mean_squared_error去掉负号的数字</li>
<li>类似均方误差MSE，还有一种<strong>绝对值误差MAE</strong>（Mean absolute error）<br>其表达的概念与均方误差完全一致，不过在真实标签和预测值之间的差异外我们使用的是L1范式（绝对值）。现实使用中，MSE和MAE选一个来使用就好了。在sklearn当中，我们使用命令<code>from sklearn.metrics importmean_absolute_error</code>来调用MAE，同时，我们也可以使用交叉验证中的<code>scoring = &quot;neg_mean_absolute_error&quot;</code>，以此在交叉验证时调用MAE。<br><img src="https://img-blog.csdnimg.cn/20191226172512276.jpg" alt="MAE"></li>
</ol>
<h5 id="是否拟合了足够的信息"><a href="#是否拟合了足够的信息" class="headerlink" title="是否拟合了足够的信息"></a><font size="3px" color="red">是否拟合了足够的信息</font></h5><p>均方形式化的误差的一个缺陷是平均之后并无法反映到底正确拟合了多少数据信息。</p>
<p>为了衡量模型对数据上的信息量的捕捉，我们定义了R<sup>2</sup>和<strong>可解释性方差分数</strong>(explained_variance_score，EVS)来帮助我们：<br><img src="https://img-blog.csdnimg.cn/20191226172715234.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt><br>所以两者都衡量 1 - 我们的模型没有捕获到的信息量占真实标签中所带的信息量的比例，所以，两者都是越接近1越好。</p>
<ul>
<li><p>R<sup>2</sup>：我们可以使用三种方式来调用，一种是直接从metrics中导入r2_score，输入预测值和真实值后打分。第二种是直接从线性回归LinearRegression的接口score来进行调用。第三种是在交叉验证中，输入”r2”来调用。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from sklearn.metrics import r2_score</span><br><span class="line">print(r2_score(Ytest,yhat))  # 注意参数顺序，第一个参数需要是真实值</span><br><span class="line">r2 = reg.score(Xtest,Ytest)</span><br><span class="line">print(cross_val_score(reg,X,y,cv=10,scoring=&quot;r2&quot;).mean())</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>EVS</strong>有两种调用方法：可以从<code>metrics</code>中导入，也可以在交叉验证中输入<code>”explained_variance“</code>来调用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from sklearn.metrics import explained_variance_score as EVS</span><br><span class="line">print(EVS(Ytest,yhat))</span><br><span class="line">print(cross_val_score(reg,X,y,cv=10,scoring=&quot;explained_variance&quot;))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>注意：</p>
<ol>
<li>R<sup>2</sup>可能为负</li>
<li>当R<sup>2</sup><strong>为负时说明模型对数据拟合非常糟糕，模型完全无法使用</strong></li>
</ol>
<p>补充：</p>
<ol>
<li>解释平方和<strong>ESS</strong>（Explained Sum of Squares，也叫做SSR回归平方和）</li>
<li>总离差平方和<strong>TSS</strong>（Total Sum of Squares，也叫做SST总离差平方和）。</li>
<li>解释平方和ESS定义了我们的<strong>预测值和样本均值之间的差异</strong>，而<strong>总离差平方和定义了真实值和样本均值之间的差异</strong>，两个指标分别写作：<br><img src="https://img-blog.csdnimg.cn/20191226191716194.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt></li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ML</tag>
        <tag>Sklearn</tag>
        <tag>线形回归</tag>
      </tags>
  </entry>
  <entry>
    <title>主成分分析PCA与奇异值分解SVD</title>
    <url>/2019/12/22/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90PCA%E4%B8%8E%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3SVD/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://www.bilibili.com/video/av36965169" target="_blank" rel="noopener">学习地址</a></center></h3><p><a href="http://file.sh.peixun.net/file/201811/28/201811282159x84x0o4puv.pdf" target="_blank" rel="noopener">课件</a></p>
<hr>
<h4 id="关于维度"><a href="#关于维度" class="headerlink" title="关于维度"></a><font size="5px" color="red">关于维度</font></h4><p>对于数组和Series来说，维度就是功能shape返回的结果，shape中返回了几个数字，就是几维。对于numpy类型数据来说，可以使用<code>numpy.ndim</code>接口来查看维度。<br>数组中的每一张表，都可以是一个特征矩阵或一个DataFrame，这些结构永远只有一张表，所以一定有行列，其中行是样本，列是特征。针对每一张表，<strong>维度指的是样本的数量或特征的数量</strong>，一般无特别说明，指的都是特征的数量<br><strong>对图像来说，维度就是图像中特征向量的数量</strong>  </p>
<p><strong>降维算法中的”降维“，指的是降低特征矩阵中特征的数量</strong>，降维的目的是为了让算法运算更快，效果更好，但其实还有另一种需求：<strong>数据可视化</strong>。三维以上特征矩阵的则不能被可视化，数据的性质也就比较难理解。  </p>
<p><strong>降维</strong>能够即减少特征的数量，又保留大部分有效信息——将那些带有重复信息的特征合并，并删除那些带无效信息的特征等等——逐渐创造出能够代表原特征矩阵大部分信息的，特征更少的，新特征矩阵</p>
<h4 id="sklearn中的降维算法"><a href="#sklearn中的降维算法" class="headerlink" title="sklearn中的降维算法"></a><font size="5px" color="red">sklearn中的降维算法</font></h4><p><strong>sklearn</strong>中降维算法都被包括在模块<strong>decomposition</strong>中，这个模块本质是一个矩阵分解模块。</p>
<p><img src="https://img-blog.csdnimg.cn/20191222143508981.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="sklearn中的降维算法"></p>
<h4 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a><font size="5px" color="red">PCA</font></h4><p><strong>一种重要的特征选择方法：<font color="red">方差过滤</font></strong></p>
<ul>
<li>如果一个特征的方差很小，则意味着这个特征上很可能有大量取值都相同，那这一个特征的取值对样本而言就没有区分度，这种特征就不带有有效信息。从方差的这种应用就可以推断出，如果一个特征的方差很大，则说明这个特征上带有大量的信息。  </li>
</ul>
<p>在降维中，<strong>PCA使用的信息量衡量指标，就是样本方差，又称可解释性方差，方差越大，特征所带的信息量越多</strong><br><img src="https://img-blog.csdnimg.cn/20191222144146538.jpg" alt="方差"><br>Var代表一个特征的方差，n代表样本量，x<sub>i</sub>代表一个特征中的每个样本取值，xhat代表这一列样本的均值。  </p>
<p><font color="red" size="4px">面试高危问题</font>：方差计算公式中为什么除数是n-1? 这是为了得到样本方差的无偏估计</p>
<p>降维过程中，几个重要的步骤</p>
<table>
<thead>
<tr>
<th>过程</th>
<th>二维特征矩阵</th>
<th>n维特征矩阵</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>输入原数据，结构为 (3,2) 找出原本的2个特征对应的直角坐标系，本质 是找出这2个特征构成的2维平面</td>
<td>输入原数据，结构为 (m,n) 找出原本的n个特征向量构成的n维空间</td>
</tr>
<tr>
<td>2</td>
<td>2 决定降维后的特征数量：1</td>
<td>决定降维后的特征数量：k</td>
</tr>
<tr>
<td>3</td>
<td>旋转，找出一个新坐标系本质是找出2个新的特征向量，以及它们构成的新2维平面新特征向量让数据能够被压缩到少数特征上，并且总信息量不损失太多</td>
<td>通过某种变化，找出n个新的特征向量，以及它们构成的新n维空间</td>
</tr>
<tr>
<td>4</td>
<td>找出数据点在新坐标系上，2个新坐标轴上的坐标</td>
<td>找出原始数据在新特征空间V中的n个新特征向量上对应的值，即“将数据映射到新空间中”</td>
</tr>
<tr>
<td>5</td>
<td>选取第1个方差最大的特征向量，删掉没有被选中的特征，成功将2维平面降为1维</td>
<td>选取前k个信息量最大的特征，删掉没有被选中的特征，成功将n维空间V降为k维</td>
</tr>
</tbody>
</table>
<ul>
<li>在步骤3当中，我们用来找出n个新特征向量，让数据能够被压缩到少数特征上并且总信息量不损失太多的技术就是<strong>矩阵分解</strong>。PCA和SVD是两种不同的降维算法，但他们都遵从上面的过程来实现降维，只是两种算法中矩阵分解的方法不同，信息量的衡量指标不同罢了。<strong>PCA使用方差作为信息量的衡量指标</strong>，并且用特征值分解来找出空间V。</li>
</ul>
<p>降维完成之后，PCA找到的每个新特征向量就叫做“主成分”，而被丢弃的特征向量被认为信息量很少，这些信息很可能就是噪音。<br><img src="https://img-blog.csdnimg.cn/20191222151921488.jpg" alt="PCA矩阵分解"><br>而SVD使用奇异值分解来找出空间V，其中Σ也是一个对角矩阵，不过它对角线上的元素是奇异值，这也是SVD中用来衡量特征上的信息量的指标<br><img src="https://img-blog.csdnimg.cn/20191222151950978.jpg" alt="奇异值分解"><br>以上两种分解一种称为特征矩阵分解，一种为奇异值分解，在线性代数与矩阵论中会有详细讲解。<br>其中涉及的矩阵运算无论使用任何语言都不可避免的需要庞大的计算过程，因此算法运行较缓慢。但目前它的功能暂时无法替代，依然是机器学习领域的宠儿。</p>
<h4 id="特征工程中的降维和特征选择"><a href="#特征工程中的降维和特征选择" class="headerlink" title="特征工程中的降维和特征选择"></a><font size="5px" color="red">特征工程中的降维和特征选择</font></h4><p>特征工程中有三种方式：<strong>特征提取</strong>，<strong>特征创造</strong>和<strong>特征选择</strong>  </p>
<ul>
<li><strong>特征选择</strong>是从已存在的特征中选取携带<strong>信息最多</strong>的，选完之后的特征依然具有可解释性，我们依然知道这个特征在原数据的哪个位置，代表着原数据上的什么含义</li>
<li>而降维算法，是将已存在的特征进行压缩，降维完毕后的特征不是原本的特征矩阵中的任何一个特征，而是通过某些方式组合起来的新特征。通常来说，在新的特征矩阵生成之前，我们无法知晓降维算法们都建立了怎样的新特征向量，新特征矩阵生成之后也不具有可读性，我们无法判断新特征矩阵的特征是从原数据中的什么特征组合而来，新特征虽然带有原始数据的信息，却已经不是原数据上代表着的含义了。降维算法因此是<strong>特征创造</strong>（feature creation，或feature construction）的一种  </li>
</ul>
<h4 id="PCA重要参数"><a href="#PCA重要参数" class="headerlink" title="PCA重要参数"></a><font size="5px" color="red">PCA重要参数</font></h4><ul>
<li><font size="3px" color="red">n_components</font><br><code>n_components</code>是我们降维后需要的维度，即降维后需要保留的特征数量，降维流程中第二步里需要确认的k值，一般输入[0, min(X.shape)]范围中的整数。不过，为了方便可视化，一般将数据维度降到3维以下</li>
</ul>
<h5 id="案例"><a href="#案例" class="headerlink" title="案例"></a><font size="3px" color="red">案例</font></h5><ul>
<li><p>实例化</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.decomposition import PCA</span><br><span class="line"></span><br><span class="line">iris=load_iris()</span><br><span class="line">X=iris.data</span><br><span class="line">y=iris.target</span><br><span class="line">print(X.shape,y.shape) # (150, 4) (150,)</span><br><span class="line">print(iris.target_names) # [&apos;setosa&apos; &apos;versicolor&apos; &apos;virginica&apos;]</span><br><span class="line"># .target_names属性分别对应iris.target中的数字</span><br><span class="line">print(set(y)) # &#123;0, 1, 2&#125;</span><br><span class="line"># .target_names[i]就表示i对应的花的名称</span><br><span class="line"></span><br><span class="line">pca=PCA(n_components=2) # 实例化</span><br><span class="line">X_=pca.fit_transform(X) # 一步实现，同之前的预处理类似也可以拆分成.fit和.transform</span><br><span class="line">print(X_.shape) # (150, 2)</span><br></pre></td></tr></table></figure>
</li>
<li><p>可视化</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">plt.scatter(X_[y==0,0],X_[y==0,1],c=&apos;red&apos;,label=iris.target_names[0])  # 对应布尔索引</span><br><span class="line">plt.scatter(X_[y==1,0],X_[y==1,1],c=&apos;black&apos;,label=iris.target_names[1])</span><br><span class="line">plt.scatter(X_[y==2,0],X_[y==2,1],c=&apos;orange&apos;,label=iris.target_names[2])</span><br><span class="line">plt.legend(loc=&quot;best&quot;)</span><br><span class="line">plt.title(&apos;PCA of IRIS dataset&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/2019122215593927.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="PCA of IRIS dataset"></p>
<h5 id="降维后的信息属性"><a href="#降维后的信息属性" class="headerlink" title="降维后的信息属性"></a><font size="3px" color="red">降维后的信息属性</font></h5><ul>
<li><p>属性<code>explained_variance_</code>，查看降维后每个新特征向量上所带的信息量大小（可解释性方差的大小）<br>·pca.explained_variance_·</p>
</li>
<li><p>属性<code>explained_variance_ratio_</code>，查看降维后每个新特征向量所占的信息量占原始数据总信息量的百分比，又叫做可解释方差贡献率<br><code>pca.explained_variance_ratio_</code> ，大部分信息都被有效地集中在了第一个特征上</p>
</li>
<li><p><code>pca.explained_variance_ratio_.sum()</code>，表示<code>explained_variance_ratio_</code>的数字之和，即降维后保留的数据信息占总信息的百分比</p>
</li>
</ul>
<h5 id="如何选择n-components"><a href="#如何选择n-components" class="headerlink" title="如何选择n_components"></a><font size="3px" color="red">如何选择n_components</font></h5><ul>
<li>当参数components中不填写任何值，则默认返回min(X.shape)个特征，一般来说，样本量都会大于特征数目，所以什么都不填就相当于转换了新特征空间，但没有减少特征的个数。但我们可以使用这个计算出累计方差贡献率曲线。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pca_line=PCA() # 默认选择min(X.shape)，虽然维度不变但还是会改变特征维度空间</span><br><span class="line">pca_line=pca_line.fit(X) # 这里就把特征的信息量占比区分并排序出来了</span><br><span class="line"># 通过累计图查找总信息量突然变平滑的点就是我们要的最佳维度</span><br><span class="line">plt.plot([1,2,3,4],np.cumsum(pca_line.explained_variance_ratio_))</span><br><span class="line">plt.xticks([1,2,3,4])</span><br><span class="line">plt.xlabel(&quot;number of components after dimension reduction&quot;)</span><br><span class="line">plt.ylabel(&quot;cumulative explained variance&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20191222162317337.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="累计方差贡献率"></p>
<ul>
<li>最大似然估计自动选择超参数<br><strong>极大似然估计</strong> ： 也叫最大似然估计(maximum likelihoodestimation)<br>在实例化时输入<code>mle</code>作为<code>n_components</code>的参数即可。</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ML</tag>
        <tag>PCA</tag>
        <tag>SVD</tag>
      </tags>
  </entry>
  <entry>
    <title>数据预处理与特征工程</title>
    <url>/2019/12/21/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://www.bilibili.com/video/av36467376/" target="_blank" rel="noopener">学习地址</a></center></h3><p><a href="http://file.sh.peixun.net/file/201811/21/20181121222491i0fm21a9.pdf" target="_blank" rel="noopener">课件</a><br><a href="https://sklearn.apachecn.org/docs/0.21.3/" target="_blank" rel="noopener">scikit-learn (sklearn) 官方文档中文版</a><br><a href="https://www.cntofu.com/book/170/readme.html" target="_blank" rel="noopener">scikit-learn (sklearn) 官方文档中文版</a><br><a href="https://www.cnblogs.com/HuZihu/p/9761161.html" target="_blank" rel="noopener">特征缩放（Feature Scaling）</a><br><a href="https://www.cnblogs.com/jasonfreak/p/5448385.html" target="_blank" rel="noopener">使用sklearn做单机特征工程</a><br><a href="https://www.zhihu.com/question/29316149" target="_blank" rel="noopener">特征工程到底是什么？</a></p>
<hr>
<h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a><font size="5px" color="red">概述</font></h4><p><strong>数据不给力，再高级的算法能无能为力。</strong>  </p>
<p>数据挖掘的五大流程：</p>
<ol>
<li><strong>获取数据</strong></li>
<li><strong>数据预处理</strong><br>数据预处理是从数据中检测，纠正或删除损坏，不准确或不适用于模型的记录的过程<br>可能面对的问题有：数据类型不同，比如有的是文字，有的是数字，有的含时间序列，有的连续，有的间断。也可能，数据的质量不行，有噪声，有异常，有缺失，数据出错，量纲不一，有重复，数据是偏态，数据量太大或太小<br>数据预处理的<strong>目的</strong>：让数据适应模型，匹配模型的需求</li>
<li><strong>特征工程</strong>：<br>特征工程是将原始数据转换为更能代表预测模型的潜在问题的特征的过程，可以通过挑选最相关的特征，提取特征以及创造特征来实现。其中创造特征又经常以降维算法的方式实现。<br>可能面对的问题有：特征之间有相关性，特征和标签无关，特征太多或太小，或者干脆就无法表现出应有的数据现象或无法展示数据的真实面貌<br>特征工程的<strong>目的</strong>：1) 降低计算成本，2) 提升模型上限</li>
<li>建模，测试模型并预测出结果</li>
<li>上线，验证模型效果</li>
</ol>
<h4 id="sklearn中的数据预处理和特征工程"><a href="#sklearn中的数据预处理和特征工程" class="headerlink" title="sklearn中的数据预处理和特征工程"></a><font size="5px" color="red">sklearn中的数据预处理和特征工程</font></h4><ul>
<li><code>sklearn.preprocessing</code>模块：几乎包含数据预处理的所有内容</li>
<li><code>sklearn.impute</code>模块：填补缺失值专用</li>
<li><code>sklearn.feature_selection</code>模块：包含特征选择的各种方法的实践</li>
<li><code>sklearn.decomposition</code>模块：包含降维算法</li>
</ul>
<h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a><font size="5px" color="red">数据预处理</font></h4><h5 id="数据无量纲化"><a href="#数据无量纲化" class="headerlink" title="数据无量纲化"></a><font size="3px" color="red">数据无量纲化</font></h5><p>在机器学习算法实践中，我们往往有着<strong>将不同规格的数据转换到同一规格</strong>，或不同分布的数据转换到某个特定分布的需求，这种需求统称为将数据“无量纲化”。<br>无量纲化可以加快求解速度，可以帮我们提升模型精度，避免某一个取值范围特别大的特征对距离计算造成影响。  </p>
<p>数据的无量纲化可以是线性的，也可以是非线性的。线性的无量纲化包括<strong>中心化</strong>（Zero-centered或者Meansubtraction）处理和<strong>缩放处理</strong>（Scale）。中心化的本质是让所有记录减去一个固定值，即让数据样本数据平移到某个位置。缩放的本质是通过除以一个固定值，将数据固定在某个范围之中，取对数也算是一种缩放处理。</p>
<ul>
<li><font size="5px" color="greey">preprocessing.MinMaxScaler</font><br>当数据(x)按照最小值中心化后，再按极差（最大值 - 最小值）缩放，数据移动了最小值个单位，并且会被收敛到<br>[0,1]之间，而这个过程，就叫做<strong>数据归一化</strong>(Normalization，又称Min-Max Scaling)。注意，Normalization是归<br>一化，不是正则化，真正的正则化是regularization，不是数据预处理的一种手段。归一化之后的数据服从正态分布<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWcyMDE4LmNuYmxvZ3MuY29tL2Jsb2cvMTI0NzU3MC8yMDE5MDEvMTI0NzU3MC0yMDE5MDEwMzE2Mjc1MjU1Ny03MTEwNjI5OTMucG5n?x-oss-process=image/format,png" alt="归一化"><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn.preprocessing import MinMaxScaler</span><br><span class="line"></span><br><span class="line">data=np.array([[1,3],[2,4],[4,1],[2,5]])</span><br><span class="line">print(data.shape) # (4,2)</span><br><span class="line"></span><br><span class="line"># 实现归一化</span><br><span class="line">scaler=MinMaxScaler() # 实例化，默认范围[0-1]</span><br><span class="line">result=scaler.fit_transform(data) # 直接生成结果</span><br><span class="line">print(result)</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">[[0.         0.5       ]</span><br><span class="line"> [0.33333333 0.75      ]</span><br><span class="line"> [1.         0.        ]</span><br><span class="line"> [0.33333333 1.        ]]</span><br><span class="line"># 实际上上述步骤可拆分成.fit()和.transform()两步，fit()本质是生成min(x)和max(x)</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line"># 结果逆转，生成原始数据</span><br><span class="line">data=scaler.inverse_transform(result)</span><br><span class="line">print(data)</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">[[1. 3.]</span><br><span class="line"> [2. 4.]</span><br><span class="line"> [4. 1.]</span><br><span class="line"> [2. 5.]]</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line"># 归一化到指定范围</span><br><span class="line">data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]</span><br><span class="line">scaler=MinMaxScaler(feature_range=[5,10])</span><br><span class="line">result=scaler.fit_transform(data)</span><br><span class="line">print(result)</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">[[ 5.    5.  ]</span><br><span class="line"> [ 6.25  6.25]</span><br><span class="line"> [ 7.5   7.5 ]</span><br><span class="line"> [10.   10.  ]]</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>注： 如果待预处理数据X中特征非常多，使用<code>.fit</code>接口可能会报错，因为数据量太大无法计算，可以使用<code>.partial_fit</code>接口实现同样功能。</p>
<p>当然，已知计算公式，也可以借助<code>numpy</code>利用矩阵运算对数据进行归一化处理。</p>
<ul>
<li><font size="5px" color="greey">preprocessing.StandardScaler</font><br>当数据(x)按均值(μ)中心化后，再按标准差(σ)缩放，数据就会服从为均值为0，方差为1的正态分布（即标准正态分布），而这个过程，就叫做<strong>数据标准化</strong>(Standardization，又称Z-score normalization)<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWcyMDE4LmNuYmxvZ3MuY29tL2Jsb2cvMTI0NzU3MC8yMDE5MDEvMTI0NzU3MC0yMDE5MDEwMzE2MjkwMzQzNC0xNDA2ODAxNzc0LnBuZw?x-oss-process=image/format,png" alt="标准化"><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn.preprocessing import MinMaxScaler</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line"></span><br><span class="line">data=np.array([[1,3],[2,4],[4,1],[2,5]])</span><br><span class="line">scaler=StandardScaler() # 实例化</span><br><span class="line">scaler=scaler.fit(data) # 本质是生成均值和方差，按列</span><br><span class="line">print(scaler.mean_) # 查看均值 [2.25 3.25]</span><br><span class="line">print(scaler.var_) # 查看方差 [1.1875 2.1875]</span><br><span class="line"></span><br><span class="line">X_std=scaler.transform(data)</span><br><span class="line">print(X_std)</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">[[-1.14707867 -0.16903085]</span><br><span class="line"> [-0.22941573  0.50709255]</span><br><span class="line"> [ 1.60591014 -1.52127766]</span><br><span class="line"> [-0.22941573  1.18321596]]</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"># result=scaler.fit_transform(data) # 一步达成结果</span><br><span class="line"># datascaler.inverse_transform(result)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>注意：对于<code>StandardScaler</code>和<code>MinMaxScaler</code>来说，空值<code>NaN</code>会被当做是缺失值，在fit的时候忽略，在transform的时候<br><strong>保持缺失NaN</strong>的状态显示。<br>并且，尽管去量纲化过程不是具体的算法，但在fit接口中，依然<strong>只允许导入至少二维数组，一维数组导入会报错</strong></p>
<ul>
<li><font size="4px" color="red">StandardScaler和MinMaxScaler选哪个？</font><br>大多数机器学习算法中，会选择StandardScaler来进行特征缩放，因为<strong>MinMaxScaler对异常值非常敏感</strong>。在PCA，聚类，逻辑回归，支持向量机，神经网络这些算法中，StandardScaler往往是最好的选择<br>MinMaxScaler在不涉及距离度量、梯度、协方差计算以及数据需要被压缩到特定区间时使用广泛，比如数字图像处理中量化像素强度时，都会使用MinMaxScaler将数据压缩于[0,1]区间之中。<br><img src="https://img-blog.csdnimg.cn/20191221162515520.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="各种缩放"></li>
</ul>
<h5 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a><font size="3px" color="red">缺失值处理</font></h5><ul>
<li><font size="4px" color="greey">impute.SimpleImputer</font><br>这个类是专门用来填补缺失值的。它包括四个重要参数：</li>
</ul>
<table>
<thead>
<tr>
<th>参数</th>
<th>含义&amp;输入</th>
</tr>
</thead>
<tbody>
<tr>
<td>missing_values</td>
<td>告诉SimpleImputer，数据中的缺失值长什么样，默认空值np.nan</td>
</tr>
<tr>
<td>strategy</td>
<td>我们填补缺失值的策略，默认均值“mean”使用均值填补）（仅对数值型特征可用），“median”用中值填补（ 对数值型特征可用），”most_frequent”用众数填补（对数值型和字符型特征都可用），“constant”表示请参考参 “fill_value”中的值（对数值型和字符型特征都可用</td>
</tr>
<tr>
<td>fill_value</td>
<td>当参数startegy为”constant”的时候可用，可输入字符串或数字表示要填充的值，常用0</td>
</tr>
<tr>
<td>copy</td>
<td>默认为True，将创建特征矩阵的副本，反之则会将缺失值填补到原本的特征矩阵中去</td>
</tr>
</tbody>
</table>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">from sklearn.impute import SimpleImputer</span><br><span class="line"></span><br><span class="line">data=pd.read_csv(&apos;Data/Narrativedata.csv&apos;,index_col=0) # 指定第一列为索引读取数据</span><br><span class="line">print(data.head())</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">    Age     Sex Embarked Survived</span><br><span class="line">0  22.0    male        S       No</span><br><span class="line">1  38.0  female        C      Yes</span><br><span class="line">2  26.0  female        S      Yes</span><br><span class="line">3  35.0  female        S      Yes</span><br><span class="line">4  35.0    male        S       No</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line">print(data.info())</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;</span><br><span class="line">Int64Index: 891 entries, 0 to 890</span><br><span class="line">Data columns (total 4 columns):</span><br><span class="line">Age         714 non-null float64</span><br><span class="line">Sex         891 non-null object</span><br><span class="line">Embarked    889 non-null object</span><br><span class="line">Survived    891 non-null object</span><br><span class="line">dtypes: float64(1), object(3)</span><br><span class="line">memory usage: 34.8+ KB</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"># 发现 Age、sex和Embarked具有空值</span><br><span class="line"></span><br><span class="line">Age=data.loc[:,&apos;Age&apos;].values.reshape(-1,1) # 将series的值取出并生成二维的</span><br><span class="line">_mean=SimpleImputer() # 实例化默认均值填充</span><br><span class="line">_median=SimpleImputer(strategy=&apos;median&apos;) # 用中位数填充</span><br><span class="line">_zero=SimpleImputer(strategy=&apos;constant&apos;,fill_value=0) # 用常数0填充，需要指定填充值</span><br><span class="line">_most=SimpleImputer(strategy=&apos;most_frequent&apos;) # 使用众数填充</span><br><span class="line"></span><br><span class="line">_mean=_mean.fit_transform(Age) # 一步完成调取结果</span><br><span class="line">_median=_median.fit_transform(Age)</span><br><span class="line">_zero=_zero.fit_transform(Age)</span><br><span class="line">_most=_most.fit_transform(Age)</span><br><span class="line"></span><br><span class="line">data.loc[:,&apos;Age&apos;]=_median # 使用众数填充，也可以用填充好的_median、_zero或_most</span><br><span class="line">print(data.info())</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;</span><br><span class="line">Int64Index: 891 entries, 0 to 890</span><br><span class="line">Data columns (total 4 columns):</span><br><span class="line">Age         891 non-null float64</span><br><span class="line">Sex         891 non-null object</span><br><span class="line">Embarked    889 non-null object</span><br><span class="line">Survived    891 non-null object</span><br><span class="line">dtypes: float64(1), object(3)</span><br><span class="line">memory usage: 34.8+ KB</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"># 发现Age字段的值全部非空，填充成功</span><br></pre></td></tr></table></figure>
<p>当然，直接使用<code>numpy</code>和<code>pandas</code>填充也十分方便</p>
<h5 id="处理分类型特征：编码与哑变量"><a href="#处理分类型特征：编码与哑变量" class="headerlink" title="处理分类型特征：编码与哑变量"></a><font size="3px" color="red">处理分类型特征：编码与哑变量</font></h5><p>在机器学习中，大多数算法，譬如逻辑回归，支持向量机SVM，k近邻算法等都只能够处理数值型数据，不能处理文字，在sklearn当中，除了专用来处理文字的算法，其他算法在fit的时候全部要求输入数组或矩阵，也不能够导入文字型数据。为了让数据适应算法和库，我们必须<strong>将数据进行编码</strong>，即是说，将<strong>文字型数据转换为数值型</strong></p>
<ul>
<li><font size="4px" color="greey">preprocessing.LabelEncoder</font>: <strong>标签专用</strong>，能够将分类转换为分类数值<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">from sklearn.preprocessing import LabelEncoder</span><br><span class="line"></span><br><span class="line">data=pd.read_csv(&apos;Data/Narrativedata.csv&apos;,index_col=0) # 指定第一列为索引读取数据</span><br><span class="line"></span><br><span class="line">y=data.iloc[:,-1] # 取最后一列，标签允许一维</span><br><span class="line">le=LabelEncoder()  # 实例化</span><br><span class="line">lable=le.fit_transform(y) # 一步生成，也可以拆分</span><br><span class="line"># y=le.inverse_transform(lable) # 也可以逆转</span><br><span class="line">data.iloc[:,-1]=lable</span><br><span class="line">print(data.head())</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">    Age     Sex Embarked  Survived</span><br><span class="line">0  22.0    male        S         0</span><br><span class="line">1  38.0  female        C         2</span><br><span class="line">2  26.0  female        S         2</span><br><span class="line">3  35.0  female        S         2</span><br><span class="line">4  35.0    male        S         0</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"># data.iloc[:,-1] = LabelEncoder().fit_transform(data.iloc[:,-1]) # 熟悉之后也可以这样一步操作</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>在<code>fiit</code>之后和<code>.transform</code>之前也可以用实例属性<code>le.classes_</code>查看标签中究竟有多少类别</p>
<ul>
<li><font size="4px" color="greey">preprocessing.OrdinalEncoder</font>: <strong>特征专用</strong>，能够将分类特征转换为分类数值<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 以下代码基于上述代码，即将空值全部填充完毕，否则报错</span><br><span class="line">_data=data.copy()</span><br><span class="line">_data.iloc[:,1:-1]=OrdinalEncoder().fit_transform(_data.iloc[:,1:-1])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>这个类的用法和标签编码的用法基本一样的，但是如果要查看类别需要在<code>.fit</code>之后使用实例属性<code>.categories_</code>查看，注意同样需要在<code>.transform</code>之前查看，如果不需要查看直接使用<code>.fir_transform</code>一步生成即可</p>
<ul>
<li><font size="4px" color="greey">preprocessing.OneHotEncoder</font>: 独热编码，创建哑变量<br>把分类转换成数字的时候，忽略了数字中自带的数学性质，所以给算法传达了一些不准确的信息，而这会影响我们的建模。<br><strong>独热编码one-hot</strong>: 将特征转换为哑变量<br><img src="https://img-blog.csdnimg.cn/20191221193415493.jpg" alt="独热编码"><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from sklearn.preprocessing import OneHotEncoder</span><br><span class="line">X = data.iloc[:,1:-1]</span><br><span class="line">enc = OneHotEncoder(categories=&apos;auto&apos;).fit(X)</span><br><span class="line">result = enc.transform(X).toarray()</span><br><span class="line"># OneHotEncoder(categories=&apos;auto&apos;).fit_transform(X).toarray()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/2019122119530984.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="编码与哑变量"></p>
<h5 id="处理连续型特征：二值化与分段"><a href="#处理连续型特征：二值化与分段" class="headerlink" title="处理连续型特征：二值化与分段"></a><font size="3px" color="red">处理连续型特征：二值化与分段</font></h5><ul>
<li><font size="5px" color="greey">sklearn.preprocessing.Binarizer</font><br>根据阈值将数据二值化（将特征值设置为0或1），用于处理连续型变量。大于阈值的值映射为1，而小于或等于阈值的值映射为0。默认阈值为0时，特征中所有的正值都映射到1。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from sklearn.preprocessing import Binarizer</span><br><span class="line">X = data.iloc[:,0].values.reshape(-1,1) #类为特征专用，所以不能使用一维数组</span><br><span class="line">transformer = Binarizer(threshold=30).fit_transform(X)  #  大于30的全部变成1</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ML</tag>
        <tag>特征工程</tag>
        <tag>数据预处理</tag>
      </tags>
  </entry>
  <entry>
    <title>随机森林与调参</title>
    <url>/2019/12/20/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E4%B8%8E%E8%B0%83%E5%8F%82/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center size="8px"><a href="https://www.bilibili.com/video/av35973040/" target="_blank" rel="noopener">学习地址</a></center></h3><p><a href="https://sfz-lyq.cn/2019/09/26/%E5%86%B3%E7%AD%96%E6%A0%91/">决策树</a><br><a href="http://file.sh.peixun.net/file/201811/14/201811142251i2i649620m.pdf" target="_blank" rel="noopener">随机森林与调参课件</a><br><a href="http://www.peixun.net/view/1281.html" target="_blank" rel="noopener">菜菜的机器学习Sklearn课堂</a></p>
<hr>
<h4 id="集成算法"><a href="#集成算法" class="headerlink" title="集成算法"></a><font size="5px" color="red">集成算法</font></h4><p><strong>集成学习</strong>(ensemble learning)是时下非常流行的机器学习算法，它本身不是一个单独的机器学习算法，而是通过在数据上构建多个模型，集成所有模型的建模结果。在各种算法竞赛中，随机森林，梯度提升树(GBDT)，Xgboost等集成算法的身影随处可见，效果之好，应用之广。  </p>
<p><strong>集成学习的目标</strong>考虑多个评估器的建模结果，汇总之后得到一个综合结果，以此来获取一个比单个模型更好的回归或者分类表现。  </p>
<p>多个模型集成成为的模型叫做<strong>集成评估器</strong>(ensemble estimator)，组成集成评估器的每个模型都叫做<strong>基评估器</strong>(base estimator)。</p>
<p>通常来说，有三类集成算法：<strong>装袋法</strong>(Bagging)， <strong>提升法</strong>(Boosting)以及<strong>Stacking</strong><br><img src="https://img-blog.csdnimg.cn/20191219220257909.jpg" alt="集成算法"></p>
<h4 id="sklearn中的集成算法"><a href="#sklearn中的集成算法" class="headerlink" title="sklearn中的集成算法"></a><font size="5px" color="red">sklearn中的集成算法</font></h4><ul>
<li>sklearn中的集成算法模块<code>ensemble</code><br><img src="https://img-blog.csdnimg.cn/20191219220502882.jpg" alt="各种集成算法"></li>
</ul>
<h4 id="随机森林建模"><a href="#随机森林建模" class="headerlink" title="随机森林建模"></a><font size="5px" color="red">随机森林建模</font></h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from sklearn.tree import RandomForestClassifier #导入需要的模块</span><br><span class="line"></span><br><span class="line">rfc = RandomForestClassifier()     #实例化</span><br><span class="line">rfc = rfc.fit(X_train,y_train) #用训练集数据训练模型</span><br><span class="line">result = rfc.score(X_test,y_test) #导入测试集，从接口中调用需要的信息</span><br></pre></td></tr></table></figure>
<h4 id="随机森林分类器"><a href="#随机森林分类器" class="headerlink" title="随机森林分类器"></a><font size="5px" color="red">随机森林分类器</font></h4><p>随机森林是非常具有代表性的Bagging集成算法，它的所有基评估器都是决策树，分类树组成的森林就叫做随机森林分类器，回归树所集成的森林就叫做随机森林回归器。</p>
<h5 id="重要参数"><a href="#重要参数" class="headerlink" title="重要参数"></a><font size="3px" color="red">重要参数</font></h5><ol>
<li><code>n_estimators=</code>: 森林中树的数量，即基评估器的数量，数量越大模型效果往往越好，默认为10，但是在即将更新的0.22版本中，这个默认值会被修正为100</li>
<li>其他基本和决策树一样</li>
</ol>
<p>单个决策树的准确率越高，随机森林的准确率也会越高，因为装袋法是依赖于平均值或者少数服从多数原则来决定集成的结果的。</p>
<h5 id="实例"><a href="#实例" class="headerlink" title="实例"></a><font size="3px" color="red">实例</font></h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeClassifier # 导入分类树</span><br><span class="line">from sklearn.ensemble import RandomForestClassifier # 导入集成随机森林</span><br><span class="line">from sklearn.datasets import load_wine  # 数据集</span><br><span class="line">from sklearn.model_selection import train_test_split # 区分训练集和测试集</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">wine=load_wine()</span><br><span class="line"></span><br><span class="line">X_train,X_test,Y_train,Y_test=train_test_split(wine.data,wine.target,test_size=0.3) # 30%作为测试集</span><br><span class="line"></span><br><span class="line">clf=DecisionTreeClassifier(random_state=0) # 确定随机性</span><br><span class="line">rfc=RandomForestClassifier(random_state=0) # 固定随机森林</span><br><span class="line"></span><br><span class="line">clf=clf.fit(X_train,Y_train)</span><br><span class="line">rfc=rfc.fit(X_train,Y_train)</span><br><span class="line"></span><br><span class="line">score_c=clf.score(X_test,Y_test)</span><br><span class="line">score_r=rfc.score(X_test,Y_test)</span><br><span class="line">print(score_c,score_r)  # 0.8518518518518519 0.9814814814814815</span><br></pre></td></tr></table></figure>
<h5 id="随机森林与决策树对比"><a href="#随机森林与决策树对比" class="headerlink" title="随机森林与决策树对比"></a><font size="3px" color="red">随机森林与决策树对比</font></h5><ul>
<li>交叉验证：是数据集划分为n分，依次取每一份做测试集，每n-1份做训练集，多次训练模型以观测模型稳定性的方法，这样就会有n个结果，取均值即可<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeClassifier # 导入分类树</span><br><span class="line">from sklearn.ensemble import RandomForestClassifier</span><br><span class="line">from sklearn.datasets import load_wine  # 数据集</span><br><span class="line">from sklearn.model_selection import train_test_split # 区分训练集和测试集</span><br><span class="line">from sklearn.model_selection import cross_val_score </span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">wine=load_wine()</span><br><span class="line"></span><br><span class="line">X_train,X_test,Y_train,Y_test=train_test_split(wine.data,wine.target,test_size=0.3) # 30%作为测试集</span><br><span class="line"></span><br><span class="line">score_r=[]</span><br><span class="line">score_c=[]</span><br><span class="line">for i in range(10):</span><br><span class="line">    clf=DecisionTreeClassifier()</span><br><span class="line">    clf_s=cross_val_score(clf,wine.data,wine.target,cv=10).mean() # Evaluate a score by cross-validation</span><br><span class="line">    score_c.append(clf_s)</span><br><span class="line">    rfc=RandomForestClassifier(n_estimators=25)</span><br><span class="line">    rfc_s=cross_val_score(rfc,wine.data,wine.target,cv=10).mean()</span><br><span class="line">    score_r.append(rfc_s)</span><br><span class="line"></span><br><span class="line">plt.plot(range(1,11),score_r,label = &quot;Random Forest&quot;)</span><br><span class="line">plt.plot(range(1,11),score_c,label = &quot;Decision Tree&quot;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20191219224321482.png" alt="交叉验证"></p>
<h5 id="重要属性和接口"><a href="#重要属性和接口" class="headerlink" title="重要属性和接口"></a><font size="3px" color="red">重要属性和接口</font></h5><p>随机森林中有三个非常重要的属性：<code>.estimators_</code>，<code>.oob_score_</code>以及<code>.feature_importances_</code></p>
<ol>
<li><code>.estimators_</code>是用来查看随机森林中所有树的列表的</li>
<li><code>.oob_score_</code>指的是袋外得分。随机森林为了确保林中的每棵树都不尽相同，所以采用了对训练集进行有放回抽样的方式来不断组成信的训练集，在这个过程中，会有一些数据从来没有被随机挑选到，他们就被叫做“袋外数据”。这些袋外数据，没有被模型用来进行训练，sklearn可以帮助我们用他们来测试模型，测试的结果就由这个属性oob_score_来导出，本质还是模型的精确度</li>
<li><code>.feature_importances_</code>和决策树中的<code>.feature_importances_</code>用法和含义都一致，是返回特征的重要性</li>
</ol>
<p>随机森林的接口与决策树完全一致，因此依然有四个常用接口：<code>apply</code>, <code>fit</code>, <code>predict</code>和<code>score</code>。除此之外，还需要注意随机森林的predict_proba接口，这个接口返回每个测试样本对应的被分到每一类标签的概率，标签有几个分类就返回几个概率。</p>
<h4 id="机器学习中调参的基本思想"><a href="#机器学习中调参的基本思想" class="headerlink" title="机器学习中调参的基本思想"></a><font size="5px" color="red">机器学习中调参的基本思想</font></h4><p>通过画学习曲线，或者网格搜索，我们能够探索到调参边缘（代价可能是训练一次模型要跑三天三夜）<br>但是在现实中，高手调参恐怕还是多依赖于经验，而这些经验，来源于：<br>    1）非常正确的调参思路和方法<br>    2）对模型评估指标的理解<br>    3）对数据的感觉和经验<br>    4）用洪荒之力去不断地尝试</p>
<h5 id="泛化误差"><a href="#泛化误差" class="headerlink" title="泛化误差"></a><font size="3px" color="red">泛化误差</font></h5><p>用来<strong>衡量模型在未知数据上的准确率</strong>的指标<br>当模型在未知数据上（训练集或袋外数据）表现糟糕时，我们说模型的泛化程度不够，泛化误差大，模型效果不好。泛化误差与模型复杂度关系：<br><img src="https://img-blog.csdnimg.cn/20191220142559817.jpg" alt="泛化误差"><br>对于本文的树模型来说，树越茂盛，深度越深，模型也就越复杂，所以树模型是天生趋于复杂的模型，随机森林以树模型为基础。所以，随机森林的参数都是向着一个目标去的：减少模型的复杂度，防止过拟合。但是，调参没有绝对的，再调参之前需要判断模型处于过拟合还是欠拟合。<br>泛化误差实际上是<strong>偏差-方差困境</strong>。</p>
<h5 id="参数对模型的影响程度"><a href="#参数对模型的影响程度" class="headerlink" title="参数对模型的影响程度"></a><font size="3px" color="red">参数对模型的影响程度</font></h5><table>
<thead>
<tr>
<th>参数</th>
<th>对模型在未知数据上的评估性能的影响</th>
<th>影响程度</th>
</tr>
</thead>
<tbody>
<tr>
<td>n_estimators</td>
<td>提升至平稳，n_estimators↑，不影响单个模型的复杂度</td>
<td>⭐⭐⭐⭐</td>
</tr>
<tr>
<td>max_depth</td>
<td>有增有减，默认最大深度，即最高复杂度，向复杂度降低的方向调参max_depth↓，模型更简单，且向图像的左边移动</td>
<td>⭐⭐⭐</td>
</tr>
<tr>
<td>min_samples_leaf</td>
<td>有增有减，默认最小限制1，即最高复杂度，向复杂度降低的方向调参min_samples_leaf↑，模型更简单，且向图像的左边移动</td>
<td>⭐⭐</td>
</tr>
<tr>
<td>min_samples_split</td>
<td>有增有减，默认最小限制2，即最高复杂度，向复杂度降低的方向调参min_samples_split↑，模型更简单，且向图像的左边移动</td>
<td>⭐⭐</td>
</tr>
<tr>
<td>max_features</td>
<td>有增有减，默认<strong>auto</strong>，是特征总数的开平方，位于中间复杂度，既可以向复杂度升高的方向，也可以向复杂度降低的方向调参max_features↓，模型更简单，图像左移max_features↑，模型更复杂，图像右移max_features是唯一的，能够让模型更简单，也能够让模型更复杂的参数，所以在调整这个参数的时候，需要考虑我们调参的方向</td>
<td>⭐</td>
</tr>
<tr>
<td>criterion</td>
<td>有增有减，一般使用gini</td>
<td>看具体情况</td>
</tr>
</tbody>
</table>
<p><a href="https://img-blog.csdnimg.cn/20191220144638454.jpg" target="_blank" rel="noopener"></a></p>
<h4 id="关于偏差与方差"><a href="#关于偏差与方差" class="headerlink" title="关于偏差与方差"></a><font size="5px" color="red">关于偏差与方差</font></h4><p>一个集成模型(f)在未知数据集(D)上的泛化误差E(f;D)，由方差(var)，偏差(bais)和噪声(ε)共同决定：<br><img src="https://img-blog.csdnimg.cn/20191220145346250.jpg" alt="泛化误差"></p>
<p><font size="4px" color="greey">偏差</font>：模型的预测值与真实值之间的差异。在集成算法中，每个基评估器都会有自己的偏差，集成评估器的偏差是所有基评估器偏差的均值。模型越精确，偏差越低。   </p>
<p><font size="4px" color="greey">方差</font>：反映的是模型每一次输出结果与模型预测值的平均水平之间的误差，衡量模型的稳定性。模型越稳定，方差越低。   </p>
<p>随着模型越来越复杂，偏差会越来越小，而方差可能就越来越大了。调参的目的就是为了找到偏差曲线与方差曲线的<strong>交叉点</strong>。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ML</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title>大规模机器学习</title>
    <url>/2019/12/17/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1052569043&amp;courseId=1004570029" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="学习数据集"><a href="#学习数据集" class="headerlink" title="学习数据集"></a>学习数据集</h4><p>机器学习问题往往大量数据集训练效果比算法更有优势，但大量数据集带来的问题就是计算问题，如何快速计算梯度、误差等？  </p>
<h4 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h4><p>之前的线性回归模型、逻辑回归、神经网络等模型都是提出一个优化目标/函数，然后使用梯度下降寻找最优点。但普通梯度法并不适用大数据集（计算梯度需要对数据集求和），这种每次都要考虑所有数据集或者说一批数据集叫做批量梯度下降。而随机梯度下降则适用于大数据集。<br>例，计算一个梯度就要遍历整个数据集<br><img src="https://img-blog.csdnimg.cn/2019121710560753.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="批量梯度下降"></p>
<p>随机梯度下降：</p>
<ul>
<li>不需要考虑所有样本，只需对单个样本进行考虑，单个样本的代价函数</li>
<li>这样，在梯度下降时不需要遍历整个数据集更新参数&theta; ，而是一点一点更新</li>
<li>最重要的一点是随机打乱数据集<br><img src="https://img-blog.csdnimg.cn/20191217110911440.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="随机梯度下降"><br><img src="https://img-blog.csdnimg.cn/20191217112558550.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="随机梯度下降"></li>
</ul>
<h4 id="Mini-Batch-梯度下降"><a href="#Mini-Batch-梯度下降" class="headerlink" title="Mini-Batch 梯度下降"></a>Mini-Batch 梯度下降</h4><ul>
<li>批量梯度下降每次迭代使用所有m个样本  </li>
<li>随机梯度下降每次迭代使用 1个样本</li>
<li>Mini-batch梯度下降每次迭代使用b个样本，b表示Mini-batch size</li>
<li>Mini-batch 是介于批量梯度下降和随机梯度下降之间的一种方法，有时候比随机梯度下降还要快</li>
</ul>
<p>假设 b=10<br><img src="https://img-blog.csdnimg.cn/2019121714224736.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="Mini-batch梯度下降"><br>完整计算方法：<br><img src="https://img-blog.csdnimg.cn/20191217142710255.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="Mini-batch梯度下降"></p>
<h4 id="随机梯度下降收敛"><a href="#随机梯度下降收敛" class="headerlink" title="随机梯度下降收敛"></a>随机梯度下降收敛</h4><p>本节介绍关于随机梯度下降过程中，学习率&alpha;的选取？  </p>
<p>检查随机梯度下降是否收敛？   </p>
<ol>
<li>计算代价函数</li>
<li>在学习过程中，在更新参数&theta;之前计算代价函数</li>
<li>画图<img src="https://img-blog.csdnimg.cn/20191217150504494.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="检查收敛性"></li>
<li>通过其它常数慢慢减小学习率<br><img src="https://img-blog.csdnimg.cn/20191217151331112.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="学习率计算"></li>
</ol>
<h4 id="减少映射与数据并行"><a href="#减少映射与数据并行" class="headerlink" title="减少映射与数据并行"></a>减少映射与数据并行</h4><p>根据代价函数计算性质，将大数据集划分成若干小的数据集分给其他计算机并行计算，中心处理器再进行收集计算。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>浅谈互信息与熵</title>
    <url>/2019/12/05/%E6%B5%85%E8%B0%88%E4%BA%92%E4%BF%A1%E6%81%AF%E4%B8%8E%E7%86%B5/</url>
    <content><![CDATA[<h4 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h4><p>最近在做有关曲线相关/似性的方法实现，找到了基于点、形状、分段的方法，考虑到实际研究问题我们选用了基于点的三种方法：DTW（参考<a href="https://blog.csdn.net/qcyfred/article/details/53824507" target="_blank" rel="noopener">1</a>、<a href="https://www.cnblogs.com/luxiaoxun/archive/2013/05/09/3069036.html" target="_blank" rel="noopener">2</a>、<a href="https://blog.csdn.net/raym0ndkwan/article/details/45614813" target="_blank" rel="noopener">3</a>）；LCSS（参考<a href="https://www.cnblogs.com/hugechuanqi/p/10642684.html" target="_blank" rel="noopener">1</a>、<a href="https://blog.csdn.net/NYIST_TC_LYQ/article/details/50826231" target="_blank" rel="noopener">2</a>、<a href="https://www.jianshu.com/nb/33818152" target="_blank" rel="noopener">3</a>）、EDR（参考<a href="https://blog.csdn.net/baodream/article/details/80417695" target="_blank" rel="noopener">1</a>、<a href="https://www.jianshu.com/p/f8f08621fcb5" target="_blank" rel="noopener">2</a>、<a href="https://dl.acm.org/citation.cfm?id=1066213" target="_blank" rel="noopener">3</a>）。<a href="https://paste.ubuntu.com/p/n5KpCXBbSH/" target="_blank" rel="noopener">有关源码</a>   ，非终极版<br>但上述方法还是存在一定的缺陷(如何证明不是自己一厢情愿)，老板又给了一种方法：<a href="https://blog.csdn.net/u011730199/article/details/82935792" target="_blank" rel="noopener">互信息</a><br>本文基于基于互信息，简单介绍一下自己这几天有关熵的补充和理解。</p>
<h4 id="信息量"><a href="#信息量" class="headerlink" title="信息量"></a>信息量</h4><p>就是描述一个信息所需要的数据量。而信息量的大小和这件事的概率有关。简单来说，概率越小，那么信息量越大，比如国足世界杯夺冠（手动狗头）。信息量也可以说是一个事件的不确定性，如果一个事件或者说一个随机变量所有可能的概率之和为1，我这里将一个事件的某种可能记为一条信息，那么这条信息的信息量用I(x<sub>i</sub>)表示。那么<br><img src="https://img-blog.csdnimg.cn/20191205212606736.jpg" alt="信息量"><br>这便是信息量的计算定义，<code>log</code>函数应用非常巧妙，在实际应用中也非常广泛。<br>有了信息量就可以引出信息熵了</p>
<h4 id="信息熵-information-entropy"><a href="#信息熵-information-entropy" class="headerlink" title="信息熵 (information entropy)"></a>信息熵 (information entropy)</h4><p>熵这个概念本质来说我认为是一个期望值，即随机变量所有可能的信息量的期望。通常用H(X)来表示，那么<br><img src="https://img-blog.csdnimg.cn/20191205213920357.png" alt="信息熵"></p>
<h4 id="联合熵-Joint-entropy"><a href="#联合熵-Joint-entropy" class="headerlink" title="联合熵 (Joint entropy)"></a>联合熵 (Joint entropy)</h4><p>联合熵实际是信息熵的推广，如果将一维随机变量推广到多维，以二维为例，那么<br><img src="https://img-blog.csdnimg.cn/20191205214436127.png" alt="联合熵"><br>其中，P(x<sub>i</sub>, y<sub>i</sub>)为联合概率</p>
<h4 id="条件熵-Conditional-entropy"><a href="#条件熵-Conditional-entropy" class="headerlink" title="条件熵  (Conditional entropy)"></a>条件熵  (Conditional entropy)</h4><p>条件熵和条件概率类似，H(Y|X)就表示随机变量X已知的情况下随机变量Y的不确定性，X是随机的故有多种可能，那么条件熵就是在X取遍所有可能的期望<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZXMyMDE4LmNuYmxvZ3MuY29tL2Jsb2cvMTM2MTA0Mi8yMDE4MDQvMTM2MTA0Mi0yMDE4MDQwMzA4NTg0NzgzOC0xMzAxNTI4ODYucG5n?x-oss-process=image/format,png" alt="条件熵"><br>其中，P(y|x)为条件概率<br>当然，公式还可以化简：<strong>H(Y|X)=H(X,Y)−H(X)</strong>，具体推导详见参考</p>
<h4 id="相对熵-Relative-entropy-这部分理解不是很懂"><a href="#相对熵-Relative-entropy-这部分理解不是很懂" class="headerlink" title="相对熵 (Relative entropy) [这部分理解不是很懂]"></a>相对熵 (Relative entropy) [这部分理解不是很懂]</h4><p>相对熵<strong>也称KL散度 (Kullback–Leibler divergence)</strong>，用来刻画两个分布之间的差异性，设 p(x)、q(x) 是 离散随机变量 X 中取值的两个概率分布，则 p 对 q 的相对熵是：<br><img src="https://img-blog.csdnimg.cn/20191205220707986.png" alt="相对熵"><br>相对熵的值越小，表示q分布和p分布越接近。<br>这个并不像联合熵那样具有对称性。</p>
<h4 id="交叉熵-（不懂，略）"><a href="#交叉熵-（不懂，略）" class="headerlink" title="交叉熵 （不懂，略）"></a>交叉熵 （不懂，略）</h4><h4 id="互信息"><a href="#互信息" class="headerlink" title="互信息"></a>互信息</h4><p>一个随即变量中包含的关于另一个随机变量的信息量，或者说知道X，给Y的信息量带来多少损失（或者反过来），关于这损失也可以说是另一个事件的确定性的增加。可以用来度量两个随机变量的相互依赖性。<br>定义如下：<br><img src="https://img-blog.csdn.net/20180519144113545" alt="互信息"><br>其中，<em>p(x)、p(y)</em>为边缘概率<br>简化推导：<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9nc3MwLmJkc3RhdGljLmNvbS85NG8zZFNhZ194STRraEdrcG9XSzFIRjZoaHkvYmFpa2UvcGljL2l0ZW0vZDc4OGQ0M2Y4Nzk0YTRjMmI1YWJkN2I0MGFmNDFiZDVhYzZlMzlhMC5qcGc?x-oss-process=image/format,png" alt="互信息简化"><br>互信息具有对称性</p>
<h4 id="互信息、条件熵与联合熵联系"><a href="#互信息、条件熵与联合熵联系" class="headerlink" title="互信息、条件熵与联合熵联系"></a>互信息、条件熵与联合熵联系</h4><p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2ltYWdlczIwMTUuY25ibG9ncy5jb20vYmxvZy83ODg3NTMvMjAxNjEwLzc4ODc1My0yMDE2MTAyNzE1MTIxMDg0My03NDUzNDgwMjYucG5n?x-oss-process=image/format,png" alt="联系"></p>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><ol>
<li><a href="https://www.cnblogs.com/kyrieng/p/8694705.html" target="_blank" rel="noopener">详解机器学习中的熵、条件熵、相对熵和交叉熵</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/26486223" target="_blank" rel="noopener">通俗理解信息熵</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/26551798" target="_blank" rel="noopener">通俗理解条件熵</a></li>
<li><a href="https://blog.csdn.net/weixin_41806692/article/details/82463577" target="_blank" rel="noopener">关于熵的知识——信息论基本概念</a></li>
<li><a href="https://www.jianshu.com/p/c0e05f042b01" target="_blank" rel="noopener">最大熵模型</a> ，有关条件熵推导公式有误，缺少一个负号</li>
<li><a href="https://blog.csdn.net/u010041824/article/details/70224597" target="_blank" rel="noopener">条件熵的定义</a></li>
<li><a href="https://www.jianshu.com/p/541aa736195b" target="_blank" rel="noopener">自信息和互信息、信息熵</a></li>
<li><a href="https://blog.csdn.net/ranghanqiao5058/article/details/78458815" target="_blank" rel="noopener">信息熵，条件熵，互信息的通俗理解</a></li>
<li><a href="https://blog.csdn.net/github_36299736/article/details/74278803" target="_blank" rel="noopener">互信息——事件相关性度量</a></li>
<li><a href="https://blog.csdn.net/u011322987/article/details/83751213" target="_blank" rel="noopener">互信息、条件互信息</a></li>
<li><a href="https://blog.csdn.net/u011730199/article/details/82935792" target="_blank" rel="noopener">互信息（Mutual Information）的介绍</a></li>
<li><a href="https://baike.baidu.com/item/%E4%BA%92%E4%BF%A1%E6%81%AF/7423853?fr=aladdin" target="_blank" rel="noopener">百度百科-互信息</a></li>
<li><a href="https://baike.baidu.com/item/%E8%81%94%E5%90%88%E7%86%B5/22709235?fr=aladdin" target="_blank" rel="noopener">百度百科-联合熵</a></li>
<li><a href="https://www.douban.com/note/621588501/" target="_blank" rel="noopener">关于互信息的一些注记</a></li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>信息论</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ML</tag>
        <tag>概率论</tag>
        <tag>信息论</tag>
      </tags>
  </entry>
  <entry>
    <title>条件概率/全概率/贝叶斯公式 个人理解</title>
    <url>/2019/12/03/%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87-%E5%85%A8%E6%A6%82%E7%8E%87-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F-%E4%B8%AA%E4%BA%BA%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<h3 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>一个事件A在另一个事件B已经发生条件下的发生概率，记为P(A|B)  </p>
<h4 id="公式化"><a href="#公式化" class="headerlink" title="公式化"></a>公式化</h4><p><img src="https://img-blog.csdn.net/20170715165503267" alt="条件概率公式"><br>若P(B)=0, P(A|B)无定义</p>
<h4 id="联合概率"><a href="#联合概率" class="headerlink" title="联合概率"></a>联合概率</h4><p>两个事件共同发生的概率，记为P(AB)、P(A, B) 或 P(A∩B)</p>
<h4 id="统计独立性"><a href="#统计独立性" class="headerlink" title="统计独立性"></a>统计独立性</h4><p>若两个随机事件A、B相互独立，则 P(AB) = P(A) × P(B)<br>反之，若P(AB) = P(A) × P(B) ，则两个随机事件A、B相互独立  </p>
<h5 id="注"><a href="#注" class="headerlink" title="注"></a>注</h5><p>独立即随机事件发生的情况都有可能，而互斥则表示不可能同时发生</p>
<h3 id="全概率"><a href="#全概率" class="headerlink" title="全概率"></a>全概率</h3><h4 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h4><p>事件L<sub>1</sub>，L<sub>2</sub>，… ，L<sub>n</sub>，是一个完备事件组（全集），对任意一个事件C，若有<br><img src="https://img-blog.csdn.net/20170718154223896" alt="全概率公式"><br>成立，则称此公式为全概率公式<br>将条件概率引入，即 P( C) = P(C|L<sub>1</sub>)+ … + P(C|L<sub>n</sub>)</p>
<h4 id="直观理解"><a href="#直观理解" class="headerlink" title="直观理解"></a>直观理解</h4><p>到达某个目的（C）有多种方式，对应不同的概率，求到达目的的概率P( C)  </p>
<h3 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h3><p>贝叶斯公式也叫贝叶斯定理，由条件概率和全概率导出。关于应用还不太清楚，这里只简单介绍这几个概率公式，为后续互信息熵做基础。  </p>
<h4 id="公式化-1"><a href="#公式化-1" class="headerlink" title="公式化"></a>公式化</h4><p>由条件概率可导出： P(A∩B) = P(A|B) × P(B) = P(B|A) × P(A)<br>即：P(A|B)=P(B|A)×P(A) / P(B)<br>将全概率公式意义代入：即 B &lt;- C ， A &lt;- L<br>则有：<br><img src="https://img-blog.csdn.net/20170718163128878" alt="贝叶斯公式"><br>即<br><img src="https://img-blog.csdn.net/20170718163135811" alt="贝叶斯公式"></p>
<h4 id="直观理解-1"><a href="#直观理解-1" class="headerlink" title="直观理解"></a>直观理解</h4><p>联合全概率会更容易些。<br>全概率是求到达目的的多种方式概率之和，而贝叶斯概率公式执果索因，已知结果C的情况下，求导致这个结果的某个原因L<sub>k</sub>的概率</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol>
<li><a href="https://baike.baidu.com/item/%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87/4475278?fr=aladdin" target="_blank" rel="noopener">百度百科-条件概率</a></li>
<li><a href="https://blog.csdn.net/u010164190/article/details/81043856" target="_blank" rel="noopener">浅谈全概率公式和贝叶斯公式</a></li>
<li><a href="https://baike.baidu.com/item/%E5%85%A8%E6%A6%82%E7%8E%87%E5%85%AC%E5%BC%8F/9980676?fr=aladdin" target="_blank" rel="noopener">百度百科-全概率公式</a></li>
<li><a href="https://baike.baidu.com/item/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F/9683982?fr=aladdin" target="_blank" rel="noopener">百度百科-贝叶斯公式</a></li>
</ol>
]]></content>
      <categories>
        <category>概率论</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title>吴恩达机器学习第八次编程作业-推荐系统</title>
    <url>/2019/12/01/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E5%85%AB%E6%AC%A1%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<h3 id="Programming-Exercise-8-Recommender-Systems"><a href="#Programming-Exercise-8-Recommender-Systems" class="headerlink" title="Programming Exercise 8: Recommender Systems"></a><center><a href="https://github.com/CaptainLYN/Andrew-NG-Meachine-Learning" target="_blank" rel="noopener">Programming Exercise 8: Recommender Systems</a></center></h3><p>目标：实现协同过滤算法并应用于电影评分系统</p>
<hr>
<p>工具：Pycharn，Python3.6</p>
<hr>
<p>参考资料：</p>
<ol>
<li>ex8_cofi.m</li>
<li><a href="https://blog.csdn.net/tymatlab/article/details/79009618" target="_blank" rel="noopener">numpy中的ravel()、flatten()、squeeze()的用法与区别</a></li>
<li><a href="https://blog.csdn.net/weixin_30867015/article/details/98410874" target="_blank" rel="noopener">python中numpy.r_和numpy.c_</a></li>
<li><a href="https://blog.csdn.net/speargod/article/details/80233619" target="_blank" rel="noopener">正则化</a></li>
<li><a href="https://www.cnblogs.com/pinard/p/6351319.html" target="_blank" rel="noopener">矩阵分解在协同过滤推荐算法中的应用</a></li>
<li><a href="https://sfz-lyq.cn/2019/11/18/1/#more">推荐系统</a></li>
<li><a href="https://blog.csdn.net/jiang425776024/article/details/87885969" target="_blank" rel="noopener">scipy.optimize优化器的各种使用</a></li>
</ol>
<hr>
<p>完整代码</p>
<ol>
<li><a href="https://paste.ubuntu.com/p/8dmTjVT2SK/" target="_blank" rel="noopener">Recommender Systems</a></li>
</ol>
<hr>
<h4 id="Movie-ratings-dataset"><a href="#Movie-ratings-dataset" class="headerlink" title="Movie ratings dataset"></a>Movie ratings dataset</h4><p>主要是介绍数据格式(特征维度为100)。略  </p>
<h4 id="Collaborative-filtering-learning-algorithm"><a href="#Collaborative-filtering-learning-algorithm" class="headerlink" title="Collaborative filtering learning algorithm"></a>Collaborative filtering learning algorithm</h4><p>特征矩阵X ：( n<sub>m</sub> , 100)<br>参数矩阵&theta; : （  n<sub>u</sub> ，100 ）<br>评分矩阵Y ：X&theta;<sup>T</sup> ,  y<sup>(i,j)</sup>=(&theta;<sup>(j)</sup>)<sup>T</sup>x<sup>(i)</sup>(具体运算以实际数据为准)<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data1=Get_Data(&apos;./ex8/ex8_movies.mat&apos;)</span><br><span class="line">Y,R=data1[&apos;Y&apos;],data1[&apos;R&apos;]</span><br><span class="line"></span><br><span class="line">data2=Get_Data(&apos;./ex8/ex8_movieParams.mat&apos;)</span><br><span class="line">X,theta=data2[&apos;X&apos;],data2[&apos;Theta&apos;]</span><br><span class="line"></span><br><span class="line">num_u,num_m,num_f=map(int,[data2[&apos;num_users&apos;],data2[&apos;num_movies&apos;],data2[&apos;num_features&apos;]])</span><br><span class="line">print(X.shape,theta.shape)  # (1682, 10) (943, 10)</span><br><span class="line">print(num_u,num_m,num_f)  # 943 1682 10</span><br><span class="line"></span><br><span class="line"># Reduce the data set size so that this runs faster</span><br><span class="line">num_u,num_m,num_f=4,5,3</span><br><span class="line">X,theta=X[:num_m,:num_f],theta[:num_u,:num_f]</span><br><span class="line">Y,R=Y[:num_m,:num_u],R[:num_m,:num_u]</span><br><span class="line"># print(X.shape,theta.shape) # (5, 3) (4, 3)</span><br><span class="line">print(cofiCostFunc(Merge(X,theta),Y,R,num_u,num_m,num_f,0)) # 22.224603725685675</span><br></pre></td></tr></table></figure></p>
<p>为了方便简洁，下述图片与实现均合并了正则化</p>
<h5 id="Collaborative-filtering-cost-function"><a href="#Collaborative-filtering-cost-function" class="headerlink" title="Collaborative filtering cost function"></a>Collaborative filtering cost function</h5><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xMTAyMzI2Mi05M2U3ZmU0MDUxODNjYTI1LnBuZw" alt><br>为了使用高级优化算法，首先把参数合并成一个向量，在函数中再进行分解<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Merge(X,theta):</span><br><span class="line">    return np.r_[X.flatten(),theta.flatten()] # 按行合并</span><br></pre></td></tr></table></figure></p>
<p>协同过滤的参数不需要加偏置项了，故可直接正则化<br>关于正则化：回去又查了一下为什么不惩罚&theta;<sub>0</sub>，倒是找到一篇资料说是约定不惩罚第0项，吴恩达机器学习视频课时57也只是粗略的提了一下‘区别对待’<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def cofiCostFunc(params,Y,R,num_u,num_m,num_f,lamda):</span><br><span class="line">    X,theta=params[:num_m*num_f].reshape(num_m,num_f),params[num_m*num_f:].reshape(num_u,num_f) # 分解</span><br><span class="line">    # print(X.shape,theta.shape,Y.shape,R.shape)</span><br><span class="line">    error=0.5*np.square((X.dot(theta.T)-Y)*R).sum()</span><br><span class="line">    reg1=0.5*lamda*np.square(theta).sum()</span><br><span class="line">    reg2=0.5*lamda*np.square(X).sum()</span><br><span class="line">    return error + reg1 + reg2</span><br></pre></td></tr></table></figure></p>
<p>注意计算error要乘以R，区别评分与未评分</p>
<h5 id="Collaborative-filtering-gradient"><a href="#Collaborative-filtering-gradient" class="headerlink" title="Collaborative filtering gradient"></a>Collaborative filtering gradient</h5><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xMTAyMzI2Mi0zMWNlODIwZmMyYTU1ZmM2LnBuZw" alt><br>为了使用高级优化算法，这里返回的也是两个梯度矩阵的合并形式<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def cofiFradient(params,Y,R,num_u,num_m,num_f,lamda):</span><br><span class="line">    X, theta = params[:num_m * num_f].reshape(num_m, num_f), params[num_m * num_f:].reshape(num_u, num_f)</span><br><span class="line">    X_grad=((X.dot(theta.T)-Y)*R)@theta+lamda*X</span><br><span class="line">    theta_grad=((X.dot(theta.T)-Y)*R).T@X+lamda*theta</span><br><span class="line">    return Merge(X_grad,theta_grad)</span><br></pre></td></tr></table></figure></p>
<h4 id="Learning-movie-recommendations"><a href="#Learning-movie-recommendations" class="headerlink" title="Learning movie recommendations"></a>Learning movie recommendations</h4><ol>
<li><p>导入电影列表，然后手动添加一些评分</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">movies=[]</span><br><span class="line">with open(&apos;./ex8/movie_ids.txt&apos;,&apos;r&apos;,encoding=&apos;utf-16&apos;) as f:</span><br><span class="line">    for line in f:</span><br><span class="line">        movies.append(&apos; &apos;.join(line.split()[1:]))</span><br><span class="line"></span><br><span class="line">my_ratings = np.zeros(len(movies)) # 1682</span><br><span class="line"></span><br><span class="line">my_ratings[0]   = 4</span><br><span class="line">my_ratings[97]  = 2</span><br><span class="line">my_ratings[6]   = 3</span><br><span class="line">my_ratings[11]  = 5</span><br><span class="line">my_ratings[53]  = 4</span><br><span class="line">my_ratings[63]  = 5</span><br><span class="line">my_ratings[65]  = 3</span><br><span class="line">my_ratings[68]  = 5</span><br><span class="line">my_ratings[182] = 4</span><br><span class="line">my_ratings[225] = 5</span><br><span class="line">my_ratings[354] = 5</span><br></pre></td></tr></table></figure>
</li>
<li><p>将自己的评分向量添加到评分矩阵中，注意评分标记矩阵也要作相应修改，然后随机初始化特征矩阵X与参数矩阵&theta;，用高级优化算法运行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Y = np.c_[Y, my_ratings]  # (1682, 944)</span><br><span class="line">R = np.c_[R, my_ratings!=0]  # (1682, 944)</span><br><span class="line">num_m,num_u=Y.shape</span><br><span class="line">num_f=10</span><br><span class="line"></span><br><span class="line">Y_mean,Y_norm=Normalize_rating(Y,R)</span><br><span class="line"></span><br><span class="line">lambd = 10</span><br><span class="line"></span><br><span class="line"># Set Initial Parameters (Theta, X)</span><br><span class="line">X=np.random.random((num_m,num_f))</span><br><span class="line">theta=np.random.random((num_u,num_f))</span><br><span class="line">res=opt.minimize(fun=cofiCostFunc,</span><br><span class="line">                 x0=Merge(X,theta),</span><br><span class="line">                 args=(Y,R,num_u,num_m,num_f,lambd),</span><br><span class="line">                 method=&apos;TNC&apos;,</span><br><span class="line">                 jac=cofiGradient,</span><br><span class="line">                 options=&#123;&apos;maxiter&apos;:100&#125;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>根据返回的参数，预测评分，进行推荐</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ret=res.x</span><br><span class="line">fit_x,fit_theta=ret[:num_m*num_f].reshape(num_m,num_f),ret[num_m*num_f:].reshape(num_u,num_f)</span><br><span class="line"></span><br><span class="line">p=fit_x@fit_theta.T # 预测的评分矩阵</span><br><span class="line"></span><br><span class="line">my_predict=p[:,-1]+Y_mean.flatten() # 在预测基础上加上行均值（之前做过均值归一化）</span><br><span class="line"></span><br><span class="line">idx=np.argsort(my_predict)[::-1] # Returns the indices that would sort an array</span><br><span class="line"></span><br><span class="line">print(&quot;Top recommendations for you:&quot;)</span><br><span class="line">for i in range(10):</span><br><span class="line">    print(&apos;Predicting rating %.1f for movie %s.&apos; \</span><br><span class="line">          %(my_predict[idx[i]],movies[idx[i]]))</span><br><span class="line"></span><br><span class="line">print(&quot;\nOriginal ratings provided:&quot;)</span><br><span class="line">for i in range(len(my_ratings)):</span><br><span class="line">    if my_ratings[i] &gt; 0:</span><br><span class="line">        print(&apos;Rated %d for movie %s.&apos;% (my_ratings[i],movies[i]))</span><br></pre></td></tr></table></figure>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Top recommendations for you:</span><br><span class="line">Predicting rating 8.3 for movie Shawshank Redemption, The (1994).</span><br><span class="line">Predicting rating 8.3 for movie Titanic (1997).</span><br><span class="line">Predicting rating 8.3 for movie Star Wars (1977).</span><br><span class="line">Predicting rating 8.2 for movie Schindler&apos;s List (1993).</span><br><span class="line">Predicting rating 8.2 for movie Close Shave, A (1995).</span><br><span class="line">Predicting rating 8.2 for movie Wrong Trousers, The (1993).</span><br><span class="line">Predicting rating 8.2 for movie Raiders of the Lost Ark (1981).</span><br><span class="line">Predicting rating 8.1 for movie Casablanca (1942).</span><br><span class="line">Predicting rating 8.1 for movie Usual Suspects, The (1995).</span><br><span class="line">Predicting rating 8.1 for movie Good Will Hunting (1997).</span><br><span class="line"></span><br><span class="line">Original ratings provided:</span><br><span class="line">Rated 4 for movie Toy Story (1995).</span><br><span class="line">Rated 3 for movie Twelve Monkeys (1995).</span><br><span class="line">Rated 5 for movie Usual Suspects, The (1995).</span><br><span class="line">Rated 4 for movie Outbreak (1995).</span><br><span class="line">Rated 5 for movie Shawshank Redemption, The (1994).</span><br><span class="line">Rated 3 for movie While You Were Sleeping (1995).</span><br><span class="line">Rated 5 for movie Forrest Gump (1994).</span><br><span class="line">Rated 2 for movie Silence of the Lambs, The (1991).</span><br><span class="line">Rated 4 for movie Alien (1979).</span><br><span class="line">Rated 5 for movie Die Hard 2 (1990).</span><br><span class="line">Rated 5 for movie Sphere (1998).</span><br></pre></td></tr></table></figure>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>大体就是以下几个步骤：</p>
<ol>
<li>正确实现代价函数</li>
<li>正确实现梯度计算</li>
<li>进行细节化处理：如均值归一化</li>
<li>调用优化器求出最优参数</li>
<li>进行预测，注意如果进行过均值归一化还需要加上均值</li>
<li>根据预测评分进行推荐</li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统</title>
    <url>/2019/11/18/1/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1052508118&amp;courseId=1004570029" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="问题规划"><a href="#问题规划" class="headerlink" title="问题规划"></a>问题规划</h4><p>推荐系统recommend system</p>
<ul>
<li>机器学习的一个应用</li>
</ul>
<p>特征</p>
<ul>
<li>机器学习中很重要的一个内容，比起手动设计特征更需要能够自动学习得到的特征</li>
<li>通过特征学习能体会到一点点特征学习的思想</li>
</ul>
<p>例子：预测电影评分  </p>
<p><img src="https://www.privacypic.com/images/2019/11/09/0CH5PJCEFNB9HJY8OB19f435035f760dd2a.png" alt></p>
<p>如果想设计一个推荐系统，我们得先通过学习算法预测这些空缺的值，然后根据数据推荐合适的电影给用户</p>
<h4 id="基于内容的推荐算法"><a href="#基于内容的推荐算法" class="headerlink" title="基于内容的推荐算法"></a>基于内容的推荐算法</h4><p>还是以前面预测电影评分问题为例<br>每个电影 i 都有一个特征向量X<sup>(i)</sup>，分别表示归为爱情片或者动作片的程度，那么对于某个用户 j 来说，需要学习得出一组参数向量&theta;<sup>(j)</sup>，那么预测的评分就是(&theta;<sup>(j)</sup>)<sup>T</sup>X<sup>(i)</sup>，看起来像个线性回归问题</p>
<p><img src="https://www.privacypic.com/images/2019/11/09/MCK8JWVHKDPH5S882502ec1316954c.png" alt></p>
<p>公式化表述：</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMDkvSmVHOGRjREhQanFnMjliLnBuZw?x-oss-process=image/format,png" alt="4S8CX4_R_~3_7X_KQHQ_A`E.png"><br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMDkvVGhGQ0VCcTF2dHpkS2FlLnBuZw?x-oss-process=image/format,png" alt="3`YY954RYFXEMO4OE_C8MKK.png"><br><img src="https://i.loli.net/2019/11/09/FwXxvIVmq4dE5DH.png" alt></p>
<p>以上和线性归回极为相似，本质就是线性回归，但为什么这个算法被称为基于内容的推荐算法？<br>这是因为我们在这里所使用到的特征量是已知的，但很多时候我们是不知道这些特征量的</p>
<h4 id="协同过滤"><a href="#协同过滤" class="headerlink" title="协同过滤"></a>协同过滤</h4><p>协同过滤就是在前面所讲的特征学习算法用来构建推荐系统的，帮助我们得到想要的特征量，简化工作<br>现在，我们不知道特征向量中每一维的含义，但我们可以通过调查用户得知用户对某类的电影的喜好程度即&theta;<sup>(j)</sup>，并且我们已经有了y<sup>(i,j)</sup>，而(&theta;<sup>(j)</sup>)<sup>T</sup>X<sup>(i)</sup>=y<sup>(i,j)</sup>，从而可反推出特征向量的含义  </p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMDkvM094NUxIYVBFckExWFNxLnBuZw?x-oss-process=image/format,png" alt="Q_O1N_S0Z4I_C_SMUR_`E3O.png"></p>
<p>公式化：</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMDkvd1ZlcFlNNkd6QmxFVFV1LnBuZw?x-oss-process=image/format,png" alt="0@__BL5GKQ_J4`@X__H_V76.png"></p>
<p>协同过滤算法：<br>通过已知数据推出未知数据，再推出已知数据，不断重读迭代，最终可以得到想要的模型<br>但这些都是建立在有足够的数据的基础上，即每个用户都给电影进行过评分<br>理解协同过滤率，用户进行评分行为，根据评分数据帮助学习更合适的特征，又可以更好地预测其他用户评分，给用户推荐合适的信息，依次交替得到准确完美的模型，每个用户都在帮助算法学习更合适的特征</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMDkvNGw1dXFLR2lWbldmMWdJLnBuZw?x-oss-process=image/format,png" alt="SQND`69AQXV___A_D6B_0M8.png"></p>
<h4 id="协同过滤算法"><a href="#协同过滤算法" class="headerlink" title="协同过滤算法"></a>协同过滤算法</h4><p>将由X求得参数&theta;和由参数&theta;求得特征X的代价函数结合起来，同时得到特征和参数<br>这里，特征就不用添加偏置值了</p>
<p><img src="https://i.loli.net/2019/11/17/73ntpCVgiJDwfqQ.png" alt="F9NJ7M5_9FBBDBPZW_X_O_A.png"><br><img src="https://i.loli.net/2019/11/17/ZiLJwa3O67vXc1H.png" alt="DUFA558_KR6OS37VY__BT`P.png"></p>
<h4 id="矢量化：低秩矩阵分解"><a href="#矢量化：低秩矩阵分解" class="headerlink" title="矢量化：低秩矩阵分解"></a>矢量化：低秩矩阵分解</h4><p>实际上就是将参数和特征分别用矩阵表示，用矩阵相乘的形式，在前面的线性回归中就有这种用法了<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMTcvUHNWeUY2cThVWHhjaWE5LnBuZw?x-oss-process=image/format,png" alt="QQOKJ_9HAW_Z03O_Q__8M`Q.png"><br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMTcvVzI4RWNLN1BlU01YQnByLnBuZw?x-oss-process=image/format,png" alt="XU_WDSVW4_AE8U5Y_04BBOC.png"></p>
<h4 id="实施细节：均值规范化"><a href="#实施细节：均值规范化" class="headerlink" title="实施细节：均值规范化"></a>实施细节：均值规范化</h4><p>假设存在一个用户没有哦给任何电影评分，那么为了最小化误差所学习出来的参数很可能就是0向量，这样实际上市不符合的<br>还有一种情况就是存在一个电影任何用户都没有对它进行评分，那么实际上不应该把它推荐给别人，关注其他的电影来的更重要些</p>
<p>方法是，采用对评分矩阵进行均值化，即以行或列为单位减去平均值，这样使得行或列均值为0得到新的评分矩阵，并学习出特征和参数。那么实际的评分是X<sup>T</sup>&theta;再加上均值，这样就会使得预测更有一些意义<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMTcvN1NVUER0YTh4Um1ibDEyLnBuZw?x-oss-process=image/format,png" alt="M5KM6P2Z__VIWO`51Z7@YX4.png"></p>
]]></content>
      <categories>
        <category>RL</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>槛菊引发思考</title>
    <url>/2019/11/07/%E6%A7%9B%E8%8F%8A%E5%BC%95%E5%8F%91%E6%80%9D%E8%80%83/</url>
    <content><![CDATA[<h3 id="原文：看剧引发思考"><a href="#原文：看剧引发思考" class="headerlink" title="原文：看剧引发思考"></a><center>原文：看剧引发思考</center></h3><h4 id="为什么想写下这篇文章？"><a href="#为什么想写下这篇文章？" class="headerlink" title="为什么想写下这篇文章？"></a>为什么想写下这篇文章？</h4><p>最近追了<strong>三部日剧</strong>（后面有介绍），不写点什么留下些什么总感觉浪费了宝贵的时间一样<br>最近， ……. , 很长一段时间没有写下与生活相关的东西了，越来越堕落的自己，再这样下去总会出点什么毛病<br>为什么要说堕落呢？明明<a href="https://sfz-lyq.cn/archives/">Archives</a>几乎每天都有新的内容。但，自己每天2+12的堕落日子真的能骗过自己吗<br>为什么又要说2+12，2代表每天至少晚起了2个小时，12则是自己起床后浪费的时间，几乎等于整个白天了<br>每天更新新的内容是让自己的良心好受些，其实也就是骗自己了，实际上过两天还是忘<br>啊～，真实虚度人生呢？<br>哈<br>哈<br>!</p>
<h4 id="关于看剧"><a href="#关于看剧" class="headerlink" title="关于看剧"></a>关于看剧</h4><p>不想用<del>追</del>这个词，自己确实不会沉迷或者说对什么狂热吧<br>而自己看的这些剧几乎都是在连续的时间内看完，哈哈，总喜欢把一件事做好做完，追剧也不例外<br>可是，为什么追日剧呢，emm，貌似国产剧也没啥看的吧，我好像有点奇葩，最近那个什么《陈情令》很火，哈哈哈哈哈，我连肖战是谁都不知道。要说为什么追日剧，可能是突然想了解一哈日本文化？突然有点闲？突然看为了某个日本女星去的？可能也都有?<br>细数看过哪些日剧，emmmmm  </p>
<ul>
<li>《哥哥太爱我了怎么办》- <a href="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1573145034056&amp;di=ee3f80a135df42fdb4e19ecb0fe81f29&amp;imgtype=0&amp;src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201802%2F06%2F20180206185158_itFcj.thumb.700_0.jpeg" target="_blank" rel="noopener">片寄凉太</a>确实帅，剧情emmmmm</li>
<li>《朝五晚九》 - 为了十元或者山下？emmm，我可能实在无聊，剧情emmm</li>
<li>《电影少女2018》- 好想要个七濑一样的老婆，剧情有点甜有点虐？</li>
<li>《轮到你了》- 为了七濑去的？导演简直丧心病狂，剧情有点恐怖，不过导演有点年轻，我都能感觉到导演费尽心思想误导观众</li>
<li>《unnatural》- 十元确实不错，我也明白了为什么这么多人安利</li>
</ul>
<p>自己不会轻易开始一部剧的，有段时间接触到3D动画后就很排斥真人剧了<br>我一直在想，为什么一部剧会让人喜欢上它，明明是一个路人的角色，到看完变成狂热粉丝。。<br>我可能想明白了，如果你对一件事没有感觉右怎么可能喜欢上它呢，从哪里开始呢？我想，应该就是从拨动你的情绪开始，慢慢把你带入进去。如何引发你的喜欢呢？应该就是引起你的共鸣，或者说让你看到你期望的剧情，如果出现你不期望的剧情，这是否代表你的情绪已经或多或少被带入了呢。更进一步，引发你的思考，最后让你明白这部剧为什么好。<br>导演的表现收法拍摄手法，剧情和演员的演绎都是很重要的</p>
<h4 id="介绍一下这三部日剧吧"><a href="#介绍一下这三部日剧吧" class="headerlink" title="介绍一下这三部日剧吧"></a>介绍一下这三部日剧吧</h4><h5 id="電影少女2018"><a href="#電影少女2018" class="headerlink" title="電影少女2018"></a>電影少女2018</h5><p><img src="https://gss2.bdstatic.com/9fo3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike116%2C5%2C5%2C116%2C38/sign=9bb4b27a62061d95694b3f6a1a9d61b4/4e4a20a4462309f7a2e766507e0e0cf3d6cad6ec.jpg" alt="电影少女"><br>后面怎么还有一个《电影少女2019》，也是乃木板成员演的，不过心早已被七濑带走<br>不知道有没有续集，<a href="https://gss0.bdstatic.com/-4o3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D220/sign=1278dab5047b020808c938e352d8f25f/d8f9d72a6059252da854ef143a9b033b5bb5b95b.jpg" target="_blank" rel="noopener">西野七濑</a>在剧中饰演天野爱，个人觉得她的短发造型确实比长发造型好看啊<br>剧情就是介绍从电视中爬出来的电影少女帮助心灵受伤的纯洁少年的故事，从设定为不能爱上人类到爱上男主，是有点小甜<br>不多说了，重点在后面</p>
<h5 id="轮到你了"><a href="#轮到你了" class="headerlink" title="轮到你了"></a>轮到你了</h5><p><img src="https://gss3.bdstatic.com/-Po3dSag_xI4khGkpoWK1HF6hhy/baike/w%3D268%3Bg%3D0/sign=448422898b35e5dd902ca2d94efdc0d2/377adab44aed2e73809244f78901a18b86d6faf6.jpg" alt="轮到你了"><br>总共两季，花了一天时间看完，就是讲述一群心理扭曲变态加上文盲法盲的人从玩了一个shabi游戏到演变成恐怖杀人游戏的故事<br>开始几集着实让我难以接受，可能就是为了完成一个任务把这部剧看完了，不过后面确实丧心病狂啊，各种杀人手法拍出来，一度给自己留下阴影<br>最后，导演为什么要让我可爱的七濑酱成为杀人狂啊啊，剧情到最后都还想误导观众，有些地方确实它的表现手法我不是很能接受呢，日本人的脑洞难以理解，从某个莫名其妙的原因展开，然后详述，比如《青春野狼》的青春综合症设定，这部剧的开始设定也太难以让人接受了吧    </p>
<h5 id="非自然死亡"><a href="#非自然死亡" class="headerlink" title="非自然死亡"></a>非自然死亡</h5><p><img src="https://gss2.bdstatic.com/-fo3dSag_xI4khGkpoWK1HF6hhy/baike/w%3D268%3Bg%3D0/sign=970ba48da5014c08193b2fa332406535/574e9258d109b3dee2adcd4cc0bf6c81800a4c87.jpg" alt="アンナチュラル"><br>难道是天时地利人和，让我最后感叹这部剧确实有它的优点所在。看过十元的一些剧了，十元和可爱系的女生风格就不太一样了，但在剧中女主设定确实给人正能量啊<br>看前面的时候自己在想为什么不改名叫《UDI解剖室》，哈哈哈哈，剧中并没有出现什么不适的场景，可以放心食用<br>这部剧每一集几乎都讲述了一个故事，而且在50分钟左右的时长里总会以正向情感结尾，加上恰到好处的音乐《lemon》，触人心弦，让人微微一笑。mad，现在脑子里也是回旋这首音乐，原来出自这里<br>女主设定心情不好喜欢吃东西，哈哈哈哈哈，和我有点类似，况且镜头下的食用场景也是很有促动作用的，还有那可是十元啊<br>死亡与真相，坚持真理，剧情时不时传达出正能量<br>第7集，讲述了校园暴力导致自杀的情节，故事、情节、背景、它所想传递什么，我不想多说了，在日剧中，可能会出现一些国产剧中不会出现的场景，但这句话并没有任何抨击国产剧的意思，单纯只是那个少年背部三处伤口，以直播杀人的形式，导演这样的表现手法，给个赞吧</p>
<p>全剧有条主线就是大神中堂为了追查恋人死亡真相而努力，性格有点怪异，喜欢骂人，到后面演绎到大家也不经意说些粗话，甚至结尾被吓跑的助理回来这样的剧情，这个研究所很温暖，一个团队配合紧密，很多时候让我想他们所学所做真实能帮助人类，而我之前的一个想法：在天灾人祸面前我所学所作能改变什么？<br>也可能，刚刚才看完这部剧，想法有点多，所以比较唠？上一部引发思考的还是《我不是药神》<br>不过，我觉得这部剧中有个缺点就是似乎有点弱化警察了，而真相总是被研究所的大神们从蛛丝马迹和思维碰撞得出  </p>
<h4 id="哎呀，有点晚了"><a href="#哎呀，有点晚了" class="headerlink" title="哎呀，有点晚了"></a>哎呀，有点晚了</h4><p>要回去洗衣服了。。。<br>没有任何安利的意思，每个人都有自己的事要做，没法改变什么，没法强求什么<br>似乎，言语总能暴露些什么，说的越多，暴露地越多<br>做一个真实地人，每天努力上进好好生活就行了吧</p>
]]></content>
      <categories>
        <category>乱七八糟</category>
      </categories>
      <tags>
        <tag>日剧</tag>
        <tag>娱乐</tag>
      </tags>
  </entry>
  <entry>
    <title>吴恩达机器学习第八次编程作业-异常检测</title>
    <url>/2019/11/06/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E5%85%AB%E6%AC%A1%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A-%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/</url>
    <content><![CDATA[<h3 id="Programming-Exercise-8-Anomaly-Detection"><a href="#Programming-Exercise-8-Anomaly-Detection" class="headerlink" title="Programming Exercise 8: Anomaly Detection"></a><center><a href="https://github.com/CaptainLYN/Andrew-NG-Meachine-Learning" target="_blank" rel="noopener">Programming Exercise 8: Anomaly Detection</a></center></h3><p>目标： 检测服务器的异常行为</p>
<hr>
<p>工具： Pycharm，Python3.6</p>
<hr>
<p>参考资料</p>
<ol>
<li><a href="https://blog.csdn.net/Cowry5/article/details/80541524" target="_blank" rel="noopener">吴恩达机器学习作业Python实现(八)：异常检测和推荐系统</a></li>
<li><a href="https://github.com/CaptainLYN/Andrew-NG-Meachine-Learning" target="_blank" rel="noopener">Andrew-NG-Meachine-Learning</a></li>
<li><a href="http://www.ai-start.com/ml2014/" target="_blank" rel="noopener">斯坦福大学2014机器学习教程中文笔记目录</a></li>
</ol>
<hr>
<p>完整代码：</p>
<ol>
<li><a href="https://paste.ubuntu.com/p/2NS7WTpS9N/" target="_blank" rel="noopener">Anomaly Detection</a></li>
</ol>
<h4 id="绘制数据"><a href="#绘制数据" class="headerlink" title="绘制数据"></a>绘制数据</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import scipy.io as sio</span><br><span class="line"></span><br><span class="line">def Get_data(path):</span><br><span class="line">    data=sio.loadmat(path)</span><br><span class="line">    # for key in data:</span><br><span class="line">    #     print(key)</span><br><span class="line">    return data</span><br><span class="line"></span><br><span class="line">def Plot_Data(X):</span><br><span class="line">    plt.plot(X.T[0],X.T[1],&apos;bx&apos;)</span><br><span class="line">    plt.xlabel(&apos;Latency (ms)&apos;,fontsize=10)</span><br><span class="line">    plt.ylabel(&apos;Throughput (mb/s)&apos;,fontsize=10)</span><br><span class="line">    plt.xlim(0,30)</span><br><span class="line">    plt.ylim(0,30)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">data=Get_data(&apos;./ex8/ex8data1.mat&apos;)</span><br><span class="line">X,Xval,yval=data[&apos;X&apos;],data[&apos;Xval&apos;],data[&apos;yval&apos;]</span><br><span class="line">#print(X.shape,Xval.shape,yval.shape) #  (307, 2) (307, 2) (307, 1)</span><br><span class="line">Plot_Data(X)</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMDYvamFvdFZuWE1ZWmJlSEFnLnBuZw?x-oss-process=image/format,png" alt="Figure_1.png"></p>
<h4 id="Gaussian-distribution"><a href="#Gaussian-distribution" class="headerlink" title="Gaussian distribution"></a>Gaussian distribution</h4><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMDYvektGeXBTQjh1dGk1R21KLnBuZw?x-oss-process=image/format,png" alt="_TP`3~L_C~WEWMK0JUP_F@2.png"></p>
<h4 id="Estimating-parameters-for-a-Gaussian"><a href="#Estimating-parameters-for-a-Gaussian" class="headerlink" title="Estimating parameters for a Gaussian"></a>Estimating parameters for a Gaussian</h4><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMDYvWHlLNndqM0h6TTh0UmRULnBuZw?x-oss-process=image/format,png" alt="_PVWV_T`~_TMWO_A_AG81IS.png"></p>
<p>这个地方花了很多时间，没有搞明白&sum;和&sigma;之间的关系，文档所给的矩阵运算貌似有问题，参考别人的作业才能实现。<br>在课程中原始高斯概率模型和多元高斯概率模型区别听的不是很懂，而所给的文档中使用的Octave实现的代码用的是多元高斯模型  </p>
<h5 id="求-mu-和-sigma"><a href="#求-mu-和-sigma" class="headerlink" title="求&mu;和&sigma;"></a>求&mu;和&sigma;</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Estimate_Gaussian(X):</span><br><span class="line">    mu=X.mean(axis=0)</span><br><span class="line">    sigma2=X.var(axis=0,ddof=0)</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    # 当使用多元高斯概率模型时</span><br><span class="line">    sigma2 = ((X-mu).T @ (X-mu)) /X.shape[0]</span><br><span class="line">    # 这个地方也不明白</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    return mu,sigma2</span><br></pre></td></tr></table></figure>
<h5 id="多元高斯概率分布"><a href="#多元高斯概率分布" class="headerlink" title="多元高斯概率分布"></a>多元高斯概率分布</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Multivariate_Gaussian(X,mu,sigma2):</span><br><span class="line">    # print(X.shape,mu.shape,sigma2.shape)</span><br><span class="line">    if np.ndim(sigma2)==1:</span><br><span class="line">        sigma2=np.diag(sigma2) # Extract a diagonal or construct a diagonal array</span><br><span class="line">    norm=1/(np.power(2*np.pi,X.shape[1]/2)*np.power(np.linalg.det(sigma2),0.5))</span><br><span class="line">    p=np.exp(-1/2*(X-mu).dot(np.linalg.inv(sigma2)).dot((X-mu).T)) # 这里和视频中所给的公式不太一样，可能默认的矩阵形状不同，所给资料矩阵是(307，2）</span><br><span class="line">    return norm*np.diag(p) # 为什么取对角线</span><br></pre></td></tr></table></figure>
<h5 id="绘制高斯概率分布图"><a href="#绘制高斯概率分布图" class="headerlink" title="绘制高斯概率分布图"></a>绘制高斯概率分布图</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Plot_Contour(X,mu,sigma2):</span><br><span class="line">    plt=Plot_Data(X)</span><br><span class="line">    x=np.arange(0,30,0.5)</span><br><span class="line">    y=np.arange(0,30,0.5)</span><br><span class="line">    xx,yy=np.meshgrid(x,y)</span><br><span class="line">    points=np.c_[xx.ravel(),yy.ravel()] # 按列拼接</span><br><span class="line">    z=Multivariate_Gaussian(points,mu,sigma2)</span><br><span class="line">    z=z.reshape(xx.shape)</span><br><span class="line">    cont_levels = [10 ** h for h in range(-20, 0, 3)]</span><br><span class="line">    plt.contour(xx,yy,z,cont_levels)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMDYvUWxMaTlkVk4zdTFDVXhPLnBuZw?x-oss-process=image/format,png" alt="Figure_2.png"></p>
<h4 id="Selecting-the-threshold-epsilon"><a href="#Selecting-the-threshold-epsilon" class="headerlink" title="Selecting the threshold &epsilon;"></a>Selecting the threshold &epsilon;</h4><p>选择不同的 &epsilon;，根据F<sub>1</sub>的值来判定阈值是否合适<br><img src="https://i.loli.net/2019/11/06/voqybHQidLNT4MY.png" alt="9`UJ7F`K1~0SCP_MNE_X_LU.png"><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Select_Threshold(yval,pval):</span><br><span class="line">    best_epsilon=0</span><br><span class="line">    best_F1=0</span><br><span class="line">    F1=0</span><br><span class="line">    epsilons=np.linspace(min(pval),max(pval),1000)</span><br><span class="line">    for e in epsilons:</span><br><span class="line">        pval_tmp=pval&lt;e # True or False vector</span><br><span class="line">        Tp=np.sum(pval_tmp&amp;yval) # correctly classified</span><br><span class="line">        Fp=np.sum(pval_tmp&amp;(yval^1)) # incorrectly classified</span><br><span class="line">        Fn=np.sum((pval_tmp^1)&amp;yval) # incorrectly classified</span><br><span class="line">        prec=Tp/(Tp+Fp) if Tp+Fp else 0</span><br><span class="line">        rec=Tp/(Tp+Fn) if Tp+Fn else 0</span><br><span class="line">        F1=2*prec*rec/(prec+rec) if prec+rec else 0</span><br><span class="line">        if F1&gt;best_F1:</span><br><span class="line">            best_F1=F1</span><br><span class="line">            best_epsilon=e</span><br><span class="line">    return best_epsilon,best_F1</span><br><span class="line">pval=Multivariate_Gaussian(Xval,mu,sigma2)</span><br><span class="line">epsilon,F1 = Select_Threshold(yval.ravel(), pval) # 注意维度</span><br><span class="line">print(epsilon,F1)  # 8.999852631901397e-05 0.8750000000000001</span><br></pre></td></tr></table></figure></p>
<h4 id="High-dimensional-dataset"><a href="#High-dimensional-dataset" class="headerlink" title="High dimensional dataset"></a>High dimensional dataset</h4><p>这里主要就是使用之前的代码跑高维度特征<br>核心是<code>Multivariate_Gaussian</code>函数，根据估计的参数 &mu; 和 &sigma; ，传入数据 X ，计算每个样本的高斯概率<br>使用验证集计算高斯概率 pval，然后和yval对比选择最佳 &epsilon;<br>最后计算训练样本的异常数量<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def High_Dimensional_Dataset(path):</span><br><span class="line">    data=Get_data(path)</span><br><span class="line">    # print(data.keys()) # dict_keys([&apos;__header__&apos;, &apos;__version__&apos;, &apos;__globals__&apos;, &apos;X&apos;, &apos;Xval&apos;, &apos;yval&apos;])</span><br><span class="line">    X,Xval,yval=data[&apos;X&apos;],data[&apos;Xval&apos;],data[&apos;yval&apos;]</span><br><span class="line">    # print(X.shape,Xval.shape,yval.shape) # (1000, 11) (100, 11) (100, 1)</span><br><span class="line">    mu,sigma2=Estimate_Gaussian(X)</span><br><span class="line">    pval=Multivariate_Gaussian(Xval,mu,sigma2)</span><br><span class="line">    epsilon,F1=Select_Threshold(yval.ravel(),pval)</span><br><span class="line">    print(&apos;Best epsilon found using cross-validation: %e&apos; %epsilon)</span><br><span class="line">    print(&apos;Best F1 on Cross Validation Set:  %f&apos; %F1)</span><br><span class="line">    pval=Multivariate_Gaussian(X,mu,sigma2)</span><br><span class="line">    print(&apos;Outliers found:  %d&apos; %(sum(pval&lt;epsilon)))</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    Best epsilon found using cross-validation: 1.378607e-18</span><br><span class="line">    Best F1 on Cross Validation Set:  0.615385</span><br><span class="line">    Outliers found:  117</span><br><span class="line">    &apos;&apos;&apos;</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>异常检测</title>
    <url>/2019/11/06/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1052407578&amp;courseId=1004570029" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="异常检测定义"><a href="#异常检测定义" class="headerlink" title="异常检测定义"></a>异常检测定义</h4><p>以飞机引擎检测为例，从飞机引擎的各项指标中提取特征，同样一台新的引擎也可以提取同类特征，目的就是检测X<sub><em>test</em></sub>是否存在异常。</p>
<p>比如，给出的训练集全是正常的数据或者异常的数据，对无标签数据建模p(X)，表示分布概率，如果测试样本的概率小于某个阈值&epsilon;，那么就检测为<strong>异常</strong><br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMDUvd0hrRUo5N0E2ZWZLZENoLnBuZw?x-oss-process=image/format,png" alt="LT1_8_SH49Y6Q8XX__0O_V1.png"></p>
<h5 id="异常检测例子"><a href="#异常检测例子" class="headerlink" title="异常检测例子"></a>异常检测例子</h5><ol>
<li>识别用户行为是否异常：提取用户行为特征，对数据分布概率进行建模，    识别异常用户</li>
<li>工厂生产</li>
<li>数据中心电脑检测器<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMDUvTFVWUVhneXVJcGExdGVILnBuZw?x-oss-process=image/format,png" alt="DFWWGF9X86_QF_9@9__KBAW.png"></li>
</ol>
<h4 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h4><p>也叫<strong>正态分布</strong></p>
<ul>
<li>如果一个随机变量x服从一个数学期望为&mu;，方差为&sigma;<sup>2</sup>的分布模型，则记为x~N(&mu;,&sigma;<sup>2</sup>)。 其中，期望值&mu;决定了概率密度函数分布的中心位置，标准差&sigma;决定了分布幅度。</li>
<li>N(0,1)称为标准正态分布</li>
<li>其概率密度函数为f(x)=e<sup>-(x-&mu;)<sup>2</sup>/(2&sigma;<sup>2</sup>)</sup> / (&sigma;&radic;2&pi;)</li>
<li>下图中p(x;&mu;,&sigma;<sup>2</sup>)的写法和p(x)的含义是一样的，只是表示x的概率密度由后面两个变量共同决定<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMDUvM0ZmaGVsQ0c4bTVCWjdQLnBuZw?x-oss-process=image/format,png" alt="_Q_B`C1_7_H@DLHZN~FNS9D.png"></li>
</ul>
<h5 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h5><p>如果知道样本数据，如何选取&mu;和&sigma;的高斯分布？<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMDUvdmlwQXU0VXlKMU9GYmR0LnBuZw?x-oss-process=image/format,png" alt="TEJJ_902S___HH__9F`_I59.png"></p>
<h4 id="异常检测算法"><a href="#异常检测算法" class="headerlink" title="异常检测算法"></a>异常检测算法</h4><p>利用高斯概率模型建立一个异常检测算法<br>每一个独立特征都服从一个不同的高斯分布<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMDUvRVRhSDUySUpVRnRHbzNlLnBuZw?x-oss-process=image/format,png" alt="TYBI4@_08LU9R0AX`UC@N43.png"></p>
<p>算法：<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMDUvQXRpSG02T3pYQ3k3d2MyLnBuZw?x-oss-process=image/format,png" alt="N6M7_URYQC_5__5WG8X_PQM.png"><br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMDUvNmxWSDU3RXNHYmNhZHB3LnBuZw?x-oss-process=image/format,png" alt="_`QEMP3_V_ASU`K@MPII_JH.png"></p>
<h4 id="开发和评估异常检测"><a href="#开发和评估异常检测" class="headerlink" title="开发和评估异常检测"></a>开发和评估异常检测</h4><p>在实际应用中，为了评估所设计的算法，我们可能希望有一个<strong>数值</strong>来告诉我们这个算法评估系统的好坏</p>
<p>设计方法如下：<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMDYvNzUzYk1jeUFqYVlPNFZyLnBuZw?x-oss-process=image/format,png" alt="ZOT4L_JNMUY_FQ0ME__0UD3.png"><br>如何设计数据集呢：<br>假如有10000个正常飞机引擎，20个异常，那么通常用60000个正常引擎作为训练集，20000+10个作为交叉验证集，剩下20000+10个作为测试集（不建议使用相同的交叉验证集和测试集）</p>
<p>之后算法需要返回的就是下面的这些评估<strong>指标</strong>：</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMDYvVnJ6ZURsSUxrUDZCc2pZLnBuZw?x-oss-process=image/format,png" alt="_V7_UT_G4_~2E1FO_WCYH_A.png"><br>关于阈值&epsilon;，应该多尝试尝试不同的数值。</p>
<h4 id="异常检测-V-S-监督学习"><a href="#异常检测-V-S-监督学习" class="headerlink" title="异常检测 V.S. 监督学习"></a>异常检测 V.S. 监督学习</h4><p>在上一小节使用的方法和数据是带标签的，但是如果是带标签的数据为什么不使用监督学习的方法来分类呢？<br>在异常检测中，正负样本的数量往往差距很大，很难从数量少的样本中学习到足够的知识，当出现未见过的异常时，使用高斯概率分布可以解决这个问题。<br>在监督学习中，未来会出现的数据很可能和训练的数据也是相同的。<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMDYvSWxGR3hqem5ES2k4cmFjLnBuZw?x-oss-process=image/format,png" alt="S7TCQT__@_ULLN4I`VIO9MU.png"></p>
<h4 id="设计异常检测算法"><a href="#设计异常检测算法" class="headerlink" title="设计异常检测算法"></a>设计异常检测算法</h4><h5 id="非高斯特征"><a href="#非高斯特征" class="headerlink" title="非高斯特征"></a>非高斯特征</h5><p>对数据进行概率图化，发现所选取的特征并不符合高斯分布？  </p>
<ul>
<li><p>对数据进行变换（对数变换或者指数变换），使得变换后的数据更符合高斯概率模型<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMDYvUlRXZ3ZDc0dIOWZxUDN5LnBuZw?x-oss-process=image/format,png" alt="HQV_K__THG_NVZE7ESI_N_I.png"></p>
<h5 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h5><p>这里其实就是一个设计特征的问题，我们希望异常数据的高斯概率看起来也是<strong>异常的</strong>，但如果所有数据的高斯概率都一样的分布，怎么办？</p>
<ul>
<li>使用异常数据创造新的特征，这样正常数据的分布都集中在某些地方，越集中概率越大，而异常数据所产生的新特征就很明显了，这样 p(x) = p(x1; ) p(x2; ) … p(xn; ) 就很容易区分出来了  </li>
<li>使用既不是很大也不是很小的数据</li>
<li>如果怀疑某些特征有问题，可以对不同特征进行运算创造新的特征这样就更容易检测出问题<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMDYvd1VOc2JtRUFNQ1prWTlSLnBuZw?x-oss-process=image/format,png" alt="Y_Q18__~CKNC9_HP_W__PSL.png"></li>
</ul>
</li>
</ul>
<h4 id="多变量高斯分布"><a href="#多变量高斯分布" class="headerlink" title="多变量高斯分布"></a>多变量高斯分布</h4><p>不给每一维特征建立高斯概率模型，直接给整个数据集建立高斯概率模型<br>&sum;是 n*n 的协方差矩阵<br>&mu;是一个向量</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMDYvVkZrbWExOWpnZHRiUHFULnBuZw?x-oss-process=image/format,png" alt="N2ATW26BA_K_DN5ADMB40_6.png"></p>
<h4 id="基于多变量高斯分布的异常检测"><a href="#基于多变量高斯分布的异常检测" class="headerlink" title="基于多变量高斯分布的异常检测"></a>基于多变量高斯分布的异常检测</h4><p>知识回顾</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMDYvUlRsYUhvOUtCeXZZVTM3LnBuZw?x-oss-process=image/format,png" alt="BWVS_USAQ3S_ND__4`_A163.png"><br>异常检测算法<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMDYvekdTSE1xbThmb2puVTQ2LnBuZw?x-oss-process=image/format,png" alt="XY_Y0BXSV~_8_3N6_6XYP6O.png"></p>
<p>多元高斯分布模型与原始模型<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMDYvSk40REJscDNZQTl3VXZGLnBuZw?x-oss-process=image/format,png" alt="8VNON6@0X@1JCPK_@_JH79K.png"><br>两种模型的选择<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMTEvMDYvUzhheUxHeGlSWU4zOXZXLnBuZw?x-oss-process=image/format,png" alt="NQ_O5BZ__GXQQ0@47X6L__2.png"></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Imitation Learning</title>
    <url>/2019/11/05/Imitation-Learning%E3%80%81/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://www.bilibili.com/video/av58458003/?p=9" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p><img src="https://i.loli.net/2019/11/05/TXklH3QfSjOU4ds.png" alt="2019-11-05 14-41-24 的屏幕截图.png"></p>
<h4 id="Behavior-Cloning"><a href="#Behavior-Cloning" class="headerlink" title="Behavior Cloning"></a>Behavior Cloning</h4><p>观察专业行为或者说标准行为，然后进行决策，但这样收集的数据可能比较单一，没有各种情况下的数据。<br><img src="https://i.loli.net/2019/11/05/IijAu7xGD8FXlsh.png" alt="2019-11-05 14-43-44 的屏幕截图.png"><br><img src="https://i.loli.net/2019/11/05/WOLhZc2MmFjGVda.png" alt="2019-11-05 14-46-25 的屏幕截图.png"><br><img src="https://i.loli.net/2019/11/05/SWK1HExnhvbaQUG.png" alt="2019-11-05 14-48-40 的屏幕截图.png"><br><img src="https://www.privacypic.com/images/2019/11/05/2019-11-05-14-51-38-fdc76c7f35e9ecfc.png" alt="2019-11-05-14-51-38-fdc76c7f35e9ecfc.png"><br>上图是以学说话为例的，在说话时可能会用手势促进理解，但是学习者应该学会的是语言，而非附带的手势。</p>
<h4 id="Inverse-Reinforcement-Learning-IRL"><a href="#Inverse-Reinforcement-Learning-IRL" class="headerlink" title="Inverse Reinforcement Learning(IRL)"></a>Inverse Reinforcement Learning(IRL)</h4><p>传统学习模型<br><img src="https://i.loli.net/2019/11/05/ySYmfCHhKB4g1pT.png" alt="2019-11-05 15-02-40 的屏幕截图.png"><br>现在在观察专业行为的基础上反推出Function，再应用到学习行为中<br><img src="https://i.loli.net/2019/11/05/3NO9wQTHE7kylpo.png" alt="2019-11-05 15-04-13 的屏幕截图.png"></p>
<p>假设专业行为是最好的，那么，</p>
<p><img src="https://i.loli.net/2019/11/05/iPzYlH8Nu6Egqj9.png" alt="2019-11-05 15-10-08 的屏幕截图.png"></p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>Sparse Reward</title>
    <url>/2019/11/05/Spase-Reward/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://www.bilibili.com/video/av58458003/?p=8" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="Reward-Shaping"><a href="#Reward-Shaping" class="headerlink" title="Reward Shaping"></a>Reward Shaping</h4><p>即，自己设计一些Reward，并不是实际上的Reward，有利于全局。<br><img src="https://i.loli.net/2019/11/05/IRb5zTfAxNu3iDW.png" alt="2019-11-05 11-07-21 的屏幕截图.png"></p>
<p>下面描述了一个游戏的各种行为的Reward，可以根据自己的需求去调整这个Reward值，比如下面或者为什么要扣分，可能就是为了是使得机器人更<strong>好战</strong>一些。<br>下面的机器手臂，如何让它把蓝色板穿过柱子，要让机械手臂学会这个事，可以设定板子离柱子越近Reward越大。<br><img src="https://i.loli.net/2019/11/05/SCJmyZvpFDNXGt8.png" alt="2019-11-05 11-12-19 的屏幕截图.png"></p>
<h4 id="可能的各种设定"><a href="#可能的各种设定" class="headerlink" title="可能的各种设定"></a>可能的各种设定</h4><h5 id="Curiosity"><a href="#Curiosity" class="headerlink" title="Curiosity"></a>Curiosity</h5><p><img src="https://i.loli.net/2019/11/05/OGjynVS45U67YNC.png" alt="2019-11-05 11-18-23 的屏幕截图.png"><br><img src="https://i.loli.net/2019/11/05/5OBZp3TdxG8flMw.png" alt="2019-11-05 11-23-53 的屏幕截图.png"></p>
<h5 id="Curriculum-Learning"><a href="#Curriculum-Learning" class="headerlink" title="Curriculum Learning"></a>Curriculum Learning</h5><p>规划机器学习，从易到难。</p>
<p><img src="https://i.loli.net/2019/11/05/IuMnR8pD3f4XmoB.png" alt="2019-11-05 12-09-32 的屏幕截图.png"></p>
<p><img src="https://i.loli.net/2019/11/05/BSsg4m38KC5IbTd.png" alt="2019-11-05 12-11-47 的屏幕截图.png"></p>
<p><img src="https://i.loli.net/2019/11/05/WvXSd5a48cqmEMO.png" alt="2019-11-05 12-12-22 的屏幕截图.png"></p>
<h5 id="分层学习"><a href="#分层学习" class="headerlink" title="分层学习"></a>分层学习</h5><p><img src="https://i.loli.net/2019/11/05/R7kYm8lOgVquXSP.png" alt="2019-11-05 12-17-30 的屏幕截图.png"></p>
<p><img src="https://www.privacypic.com/images/2019/11/05/2019-11-05-12-19-59-b9ca344c2fbc8505.png" alt="2019-11-05-12-19-59-b9ca344c2fbc8505.png"></p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>Actor-Ctitic</title>
    <url>/2019/11/03/Actor-Ctitic/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://www.bilibili.com/video/av58458003/?p=7" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="Asynchronous-Advantage-Actor-Critic-A3C"><a href="#Asynchronous-Advantage-Actor-Critic-A3C" class="headerlink" title="Asynchronous Advantage Actor-Critic(A3C)"></a>Asynchronous Advantage Actor-Critic(A3C)</h4><p>Asynchronous 表示异步，如果去掉这个词，就是AN。</p>
<p><img src="https://i.loli.net/2019/11/03/aHjLDOTv3KAZukt.png" alt="2019-11-03 15-40-42 的屏幕截图.png"></p>
<p><img src="https://i.loli.net/2019/11/03/S9HTgvRJneIAtK1.png" alt="2019-11-03 15-45-51 的屏幕截图.png"></p>
<h4 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h4><p><img src="https://i.loli.net/2019/11/03/Xoqhtm52jlfGIMu.png" alt="2019-11-03 15-48-48 的屏幕截图.png"></p>
<p><img src="https://i.loli.net/2019/11/03/7Ri1dEVbYWmILkc.png" alt="2019-11-03 15-53-15 的屏幕截图.png"></p>
<p><img src="https://i.loli.net/2019/11/03/2VLempMIkA49Ech.png" alt="2019-11-03 15-54-42 的屏幕截图.png"><br><img src="https://i.loli.net/2019/11/03/DOdMqo9ivX8NzPG.png" alt="2019-11-03 15-58-36 的屏幕截图.png"></p>
<h4 id="A3C"><a href="#A3C" class="headerlink" title="A3C"></a>A3C</h4><p><img src="https://i.loli.net/2019/11/03/2oQxmIbFKYdijpZ.png" alt="2019-11-03 16-02-15 的屏幕截图.png"></p>
<h4 id="Pathwise-Derivative-Plicy-Gradient"><a href="#Pathwise-Derivative-Plicy-Gradient" class="headerlink" title="Pathwise Derivative Plicy Gradient"></a>Pathwise Derivative Plicy Gradient</h4><p><img src="https://i.loli.net/2019/11/03/AelI6xrvLudUJiC.png" alt="2019-11-03 16-07-38 的屏幕截图.png"></p>
<p><img src="https://i.loli.net/2019/11/03/au6bhsClRzTPp2K.png" alt="2019-11-03 16-14-48 的屏幕截图.png"></p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>Q-Learning-3</title>
    <url>/2019/11/02/Q-Learning-3/</url>
    <content><![CDATA[<h4 id="Q-Learning-for-Continuous-Actions"><a href="#Q-Learning-for-Continuous-Actions" class="headerlink" title="Q-Learning for Continuous Actions"></a>Q-Learning for Continuous Actions</h4><p>很多时候是连续的，而非上下左右这种离散的<br><img src="https://i.loli.net/2019/11/02/64tj7KAvCP2VGHz.png" alt="2019-11-02 17-25-41 的屏幕截图.png"><br><img src="https://i.loli.net/2019/11/02/iFtBMRs3zUekdTj.png" alt="2019-11-02 17-28-46 的屏幕截图.png"></p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>Q-Learning-2</title>
    <url>/2019/11/02/Q-Learning-2/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://www.bilibili.com/video/av58458003/?p=5" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="Double-DQN"><a href="#Double-DQN" class="headerlink" title="Double DQN"></a>Double DQN</h4><p>Q value 通常会 <code>over-estimated</code><br><img src="https://i.loli.net/2019/11/02/cb71GvRJodgIw6f.png" alt="2019-11-02 16-52-49 的屏幕截图.png"></p>
<h4 id="Dueling-DQN"><a href="#Dueling-DQN" class="headerlink" title="Dueling DQN"></a>Dueling DQN</h4><p><img src="https://i.loli.net/2019/11/02/cu1fU8J4jPgC3Rv.png" alt="2019-11-02 16-57-28 的屏幕截图.png"></p>
<h4 id="Prioritized-Reply"><a href="#Prioritized-Reply" class="headerlink" title="Prioritized Reply"></a>Prioritized Reply</h4><p><img src="https://i.loli.net/2019/11/02/v7QBLCmuRas8XKO.png" alt="2019-11-02 17-04-03 的屏幕截图.png"></p>
<h4 id="Multi-step"><a href="#Multi-step" class="headerlink" title="Multi-step"></a>Multi-step</h4><p><img src="https://i.loli.net/2019/11/02/TcmL7nbghM5PFsv.png" alt="2019-11-02 17-06-16 的屏幕截图.png"></p>
<h4 id="Noisy-Net"><a href="#Noisy-Net" class="headerlink" title="Noisy Net"></a>Noisy Net</h4><p><img src="https://i.loli.net/2019/11/02/c3l8fTnVAzteBCF.png" alt="2019-11-02 17-07-49 的屏幕截图.png"></p>
<p><img src="https://i.loli.net/2019/11/02/yBmhzO5XfQrtCFa.png" alt="2019-11-02 17-11-21 的屏幕截图.png"></p>
<h4 id="Distributional-Q-function"><a href="#Distributional-Q-function" class="headerlink" title="Distributional Q-function"></a>Distributional Q-function</h4><p><img src="https://i.loli.net/2019/11/02/WdwN5zjC27Y9xby.png" alt="2019-11-02 17-18-54 的屏幕截图.png"></p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>Q-Learning-1</title>
    <url>/2019/11/01/Q-Learning/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://www.bilibili.com/video/av58458003/?p=4" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="Critic"><a href="#Critic" class="headerlink" title="Critic"></a>Critic</h4><p><code>Q-Learning</code>是value-based的强化学习，它学习的是一个<code>Critic</code>，评价所给出的actor &pi;有多好</p>
<p>例如： State value function V<sup>&pi;</sup>(s)</p>
<ul>
<li>当使用actor &pi;，V<sup>&pi;</sup>(s)所代表就是在观察到状态s后累积的期望收益值<br><img src="https://i.loli.net/2019/11/01/i4tYBJ2KdleNo9E.png" alt="2019-11-01 15-55-14 的屏幕截图.png"></li>
</ul>
<h3 id="如何衡量V-pi-s"><a href="#如何衡量V-pi-s" class="headerlink" title="如何衡量V&pi;(s)"></a>如何衡量V<sup>&pi;</sup>(s)</h3><h4 id="基于蒙特卡罗"><a href="#基于蒙特卡罗" class="headerlink" title="基于蒙特卡罗"></a>基于蒙特卡罗</h4><p><code>Critic</code>观察 &pi; 玩游戏，让Agent与环境互动<br>当看到状态s<sub>a</sub>时，到游戏结束所得到的累积收益越接近G<sub>a</sub>越好，但需要等到游戏结束才能得到结果<br><img src="https://i.loli.net/2019/11/01/VGoFzMyOu1CQ96U.png" alt="2019-11-01 16-00-35 的屏幕截图.png"></p>
<h4 id="基于时间差分-Temporal-difference"><a href="#基于时间差分-Temporal-difference" class="headerlink" title="基于时间差分(Temporal-difference)"></a>基于时间差分(Temporal-difference)</h4><p>不需要等一整个episode结束，只需等到一个状态(s<sub>t</sub> ，a<sub>t</sub>，r<sub>t</sub>)到下一个状态s<sub>t+1</sub><br>V<sup>&pi;</sup>(s<sub>t</sub>)=V<sup>&pi;</sup>(s<sub>t+1</sub>) + r<sub>t</sub><br><img src="https://i.loli.net/2019/11/01/4RWOC312ciGVxZJ.png" alt="2019-11-01 16-10-36 的屏幕截图.png"></p>
<p><img src="https://i.loli.net/2019/11/01/9uAbaZvyBxhU648.png" alt="2019-11-01 16-14-08 的屏幕截图.png"></p>
<h3 id="State-action-value-function-Q-pi-s-a"><a href="#State-action-value-function-Q-pi-s-a" class="headerlink" title="State-action value function Q&pi;(s,a)"></a>State-action value function Q<sup>&pi;</sup>(s,a)</h3><p>假设在状态s采取的行为是a，在策略&pi;下所累积的收益期望<br><img src="https://i.loli.net/2019/11/01/Ni8olFM1qpVfcWE.png" alt="2019-11-01 16-23-23 的屏幕截图.png"></p>
<p><img src="https://i.loli.net/2019/11/01/rpYwaVzjtBcWZSA.png" alt="2019-11-01 16-30-49 的屏幕截图.png"><br><img src="https://i.loli.net/2019/11/01/HE9Qd8AMxXc4yPR.png" alt="2019-11-01 16-52-12 的屏幕截图.png"></p>
<h3 id="Target-Network"><a href="#Target-Network" class="headerlink" title="Target Network"></a>Target Network</h3><p>训练时固定一个Q<sup>&pi;</sup>，作为目标网络，更新其他的Q<sup>&pi;</sup>，然后用更新好几次的Q代替所固定的Q<br><img src="https://i.loli.net/2019/11/01/aQlE4YnH1BkzfvU.png" alt="2019-11-01 16-57-07 的屏幕截图.png"></p>
<h3 id="Exploration"><a href="#Exploration" class="headerlink" title="Exploration"></a>Exploration</h3><p><img src="https://i.loli.net/2019/11/01/4TK6MwrfdHEcmVZ.png" alt="2019-11-01 17-04-18 的屏幕截图.png"></p>
<h3 id="Replay-Buffer"><a href="#Replay-Buffer" class="headerlink" title="Replay Buffer"></a>Replay Buffer</h3><p><img src="https://i.loli.net/2019/11/01/Gh2W4mM6QUn5pXZ.png" alt="2019-11-01 17-07-05 的屏幕截图.png"></p>
<h3 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h3><p><img src="https://i.loli.net/2019/11/01/2yUXV1f8xMpLCwb.png" alt="2019-11-01 17-11-18 的屏幕截图.png"></p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>From on-policy to off-policy</title>
    <url>/2019/10/28/From-on-policy-to-off-policy/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://www.bilibili.com/video/av58458003/?p=3" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="On-policy-v-s-Off-policy"><a href="#On-policy-v-s-Off-policy" class="headerlink" title="On-policy v.s. Off-policy"></a>On-policy v.s. Off-policy</h4><ul>
<li>On-policy : 进行学习的智能体与环境发生交互的智能体是同一个</li>
<li>Off-policy：进行学习的智能体与环境发生交互的智能体不是同一个，通过观察别人进行学习</li>
</ul>
<p><img src="https://i.loli.net/2019/10/28/Ab1efFzKrJsv3h2.png" alt="2019-10-28 20-38-54 的屏幕截图.png"><br><img src="https://i.loli.net/2019/11/01/qjKPNEGTVmslYo9.png" alt="2019-11-01 11-07-23 的屏幕截图.png"><br><img src="https://i.loli.net/2019/11/01/fxHF9lBMZLCbp1G.png" alt="2019-11-01 11-14-47 的屏幕截图.png"><br><img src="https://i.loli.net/2019/11/01/sqIVuW2we59HJDM.png" alt="2019-11-01 11-24-26 的屏幕截图.png"></p>
<h4 id="Proximal-Policy-Optimization"><a href="#Proximal-Policy-Optimization" class="headerlink" title="Proximal Policy Optimization"></a>Proximal Policy Optimization</h4><p><img src="https://i.loli.net/2019/10/28/27Zd4qXS5Ws1Igw.png" alt="2019-10-28 20-59-33 的屏幕截图.png"></p>
<p><img src="https://i.loli.net/2019/10/28/P3Gzo2U7t8hlZWJ.png" alt="2019-10-28 21-05-04 的屏幕截图.png"><br><img src="https://i.loli.net/2019/11/01/UnhrWLN7f9d3EKG.png" alt="2019-11-01 14-55-41 的屏幕截图.png"></p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>Proximal Policy Optimazation</title>
    <url>/2019/10/27/Proximal-Policy-Optimazation/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://www.bilibili.com/video/av58458003/?p=2" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="策略梯度"><a href="#策略梯度" class="headerlink" title="策略梯度"></a>策略梯度</h4><h5 id="Policy-of-Actor"><a href="#Policy-of-Actor" class="headerlink" title="Policy of Actor"></a>Policy of Actor</h5><ul>
<li>Policy &pi; is  a network with parameter &theta;<ul>
<li>Input: the observation of machine represented as a vector or a matrix</li>
<li>Output: each action corresponds to a neuron in output layer</li>
</ul>
</li>
</ul>
<p><img src="https://i.loli.net/2019/10/27/cqaKmdgNfuRtOZh.png" alt="2019-10-27 11-34-27 的屏幕截图.png"><br><img src="https://i.loli.net/2019/10/27/oZsjftg3PvuLWm6.png" alt="2019-10-27 11-42-51 的屏幕截图.png"><br><img src="https://i.loli.net/2019/10/27/7H8JIXMGSsRz3E4.png" alt="2019-10-27 11-44-49 的屏幕截图.png"><br><img src="https://i.loli.net/2019/10/27/7H8JIXMGSsRz3E4.png" alt="2019-10-27 11-44-49 的屏幕截图.png"><br><img src="https://i.loli.net/2019/11/01/TYMG4SrCf19oijt.png" alt="2019-11-01 09-47-31 的屏幕截图.png"><br><img src="https://i.loli.net/2019/10/27/irj5AW8Xy4twCJT.png" alt="2019-10-27 14-47-16 的屏幕截图.png"></p>
<p>Tips</p>
<ol>
<li>关于上述图片的一些解释，结合前面章节</li>
<li>技巧1表示有的数据可能没有被采样，但它对结果可能是正反馈，我们给所有数据减去一个基准值，使得所有数据有正有负，这样采样平均一些。</li>
<li>技巧2表示应该考虑各种情况，故 t’ 不从0开始，折扣因子表示当前对以后的影响，这个影响应该是越来越小的。</li>
</ol>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>强化学习导论</title>
    <url>/2019/10/25/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/</url>
    <content><![CDATA[<h3 id="Deep-Reinforcement-Learning"><a href="#Deep-Reinforcement-Learning" class="headerlink" title="Deep Reinforcement Learning"></a><center><a href="https://www.bilibili.com/video/av58458003/?p=1" target="_blank" rel="noopener">Deep Reinforcement Learning</a></center></h3><h4 id="强化学习情景"><a href="#强化学习情景" class="headerlink" title="强化学习情景"></a>强化学习情景</h4><ol>
<li><code>Agent</code> ：与<strong>环境</strong>发生<strong>交互</strong>的智能体、智能代理</li>
<li><code>State</code>： 环境状态，智能体能<strong>观察</strong>到的东西</li>
<li><code>Action</code>：智能体与环境发生的<strong>交互</strong>，对环境会产生影响</li>
<li><code>Reward</code>：环境反馈给智能体的一个信息，好亦或坏</li>
</ol>
<p><strong>监督学习</strong>中，对于一种状态，智能体可能会有应对的措施，以alphago为例，不同的棋句有不同的策略，这样可以通过学习学到对应策略，即<code>Learning from teacher</code>，需要大量的<strong>训练</strong>，这样学习到的策略<strong>不一定是最佳策略</strong><br>在<code>Reinforcement Learning</code>中，往往不知道可能的状态，<code>Learning from experience</code>，不断进行试探，根据<strong>规则</strong>得出结果好或者坏，这就是<code>experience</code>。Alphago采取监督学习+强化学习，两台alphago对弈很多局</p>
<h4 id="强化学习难点"><a href="#强化学习难点" class="headerlink" title="强化学习难点"></a>强化学习难点</h4><ol>
<li><p>Reward delay</p>
<ul>
<li>有的<code>action</code>可能并不能立刻得到正反馈，甚至可能出现负反馈，但是小的牺牲能够换来更多的正反馈</li>
</ul>
</li>
<li><p>智能体的行为会影响之后看到的东西，所以智能体需要学会去探索环境，探索未知世界，探索没有做过的事</p>
</li>
</ol>
<h4 id="强化学习类别"><a href="#强化学习类别" class="headerlink" title="强化学习类别"></a>强化学习类别</h4><p><img src="https://i.loli.net/2019/10/30/ParkAg9WVCqHUd8.png" alt="2019-10-30 15-02-04 的屏幕截图.png"></p>
<h5 id="Policy-based"><a href="#Policy-based" class="headerlink" title="Policy-based"></a>Policy-based</h5><p>实质： <code>Learning an Actor</code></p>
<p>在机器学习中，其实就是学习一个函数，基于规则的强化学习做了类似的事  </p>
<p><code>Actor</code>就是一个<code>Function</code>，通常写成<code>Policy</code>，这个<code>Function</code>的输入就是智能体所看到的环境状态，输出就是智能体的行为决策，根据<code>Reward</code>找出<code>best function</code><br><strong> 以上公式化为： Action=&pi;(Observation) </strong><br><img src="https://i.loli.net/2019/10/30/ywde1TmbP6iaXnG.png" alt="2019-10-30 15-18-22 的屏幕截图.png"></p>
<h6 id="寻找Function的三个步骤"><a href="#寻找Function的三个步骤" class="headerlink" title="寻找Function的三个步骤"></a>寻找Function的三个步骤</h6><ol>
<li>定义Function，比如用NN神网作为Actor</li>
<li>决定Functino的好坏，Goodness of Actor<ul>
<li>在连续的决策过程结束后，总的Reward可以写成各个reward的叠加R<sub>&theta;</sub>；但即使是同一个actor，R<sub>&theta;</sub>在每一次连续决策过程结束后都可能不同，所以定义R<sub>&theta;</sub>加上画线为一个<strong>期望值</strong>，衡量Actor好坏</li>
<li>如何计算R<sub>&theta;</sub>的期望值<br><img src="https://i.loli.net/2019/10/25/vjXH8MGRTfBZpy7.png" alt="2019-10-25 21-41-34 的屏幕截图.png"></li>
</ul>
</li>
<li>选取最佳function<br><img src="https://i.loli.net/2019/10/25/WMTk6Xgo4BHiDxO.png" alt="2019-10-25 21-50-11 的屏幕截图.png"><br><img src="https://i.loli.net/2019/10/25/GKx4RNTJjDI6YcB.png" alt="2019-10-25 21-55-10 的屏幕截图.png"><br><img src="https://i.loli.net/2019/10/30/DYyt6ISoMw8GXVq.png" alt="2019-10-30 15-58-17 的屏幕截图.png"><br><img src="https://i.loli.net/2019/10/30/NSZKg2CE1hDdBWy.png" alt="2019-10-30 15-59-06 的屏幕截图.png"><br><img src="https://i.loli.net/2019/10/30/srqUJSCbYn6hWxl.png" alt="2019-10-30 15-56-08 的屏幕截图.png"></li>
</ol>
<h5 id="Value-based"><a href="#Value-based" class="headerlink" title="Value-based"></a>Value-based</h5><p>实质： <code>Learning a Critic</code><br><code>Critic</code> 并不做决策，它评估的是actor的好与坏<br><img src="https://i.loli.net/2019/10/25/GSxB57CaQF1WPDJ.png" alt="2019-10-25 22-09-43 的屏幕截图.png"></p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>心累慌乱防猝死</title>
    <url>/2019/10/25/%E7%83%A6%E8%BA%81%E6%85%8C%E4%B9%B1%E9%98%B2%E7%8C%9D%E6%AD%BB/</url>
    <content><![CDATA[<p>好累啊，停下来就不知道干嘛了，看看<del>老婆</del>防猝死吧。</p>
<p><img src="https://i.loli.net/2019/10/25/LwVq7iYPA85CD4r.jpg" alt="5.jpg"><br><img src="https://i.loli.net/2019/10/25/AKulBwCoHkSTrp3.jpg" alt="4.jpg"><br><img src="https://i.loli.net/2019/10/25/JLPrVCKpDnj1MIU.jpg" alt="3.jpg"><br><img src="https://i.loli.net/2019/10/25/4CrPgcMoFeYlpz5.jpg" alt="2.jpg"><br><img src="https://i.loli.net/2019/10/25/VObYjKGvk3ifly8.jpg" alt="1.jpg"><br><img src="https://www.privacypic.com/images/2019/11/03/0aec0ea51fa98071d.jpg" alt="0aec0ea51fa98071d.jpg"><br><img src="https://i.loli.net/2019/10/25/aSNni9xy2CK5WYg.jpg" alt="6.jpg"></p>
]]></content>
      <categories>
        <category>乱七八糟</category>
      </categories>
      <tags>
        <tag>美图</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux vim 编辑器</title>
    <url>/2019/10/24/Linux-vim-%E7%BC%96%E8%BE%91%E5%99%A8/</url>
    <content><![CDATA[<h4 id="vi-和-vim"><a href="#vi-和-vim" class="headerlink" title="vi 和 vim"></a>vi 和 vim</h4><ul>
<li>vi(visusl interface) 的简称，是Linux 中最经典的文本编辑器   </li>
<li>设计思想： 让程序员的手指始终保持在键盘的核心区域，就能完成所有的编辑操作  </li>
<li>特点: 没有图形界面，但功能强大只能编辑文本内容，不能对字体、段落进行排版，不支持鼠标操作，没有菜单和命令。</li>
<li>vim 是vi 的发行版，在代码补全，快速定位，错误跳转等十分方便</li>
</ul>
<h4 id="打开和新建文件"><a href="#打开和新建文件" class="headerlink" title="打开和新建文件"></a>打开和新建文件</h4><ul>
<li>在终端输入 vi 文件名 即可</li>
<li>如果文件已经存在，会直接打开该文件，如果不存在则新建一个文件</li>
</ul>
<h5 id="打开文件并且定位行"><a href="#打开文件并且定位行" class="headerlink" title="打开文件并且定位行"></a>打开文件并且定位行</h5><p><code>vi 文件名 +行数</code> 如果只带上 + 而不指定行，则直接定位到文件末尾</p>
<h5 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h5><ul>
<li>如果vi 一场异常退出，在磁盘上可能保存有交换文件，下次再使用vi编辑该文件时，会看到注意信息，按下字母 d 可以删除交换文件即可。</li>
</ul>
<h4 id="工作模式"><a href="#工作模式" class="headerlink" title="工作模式"></a>工作模式</h4><h5 id="命令模式"><a href="#命令模式" class="headerlink" title="命令模式"></a>命令模式</h5><ul>
<li>打开文件首先进入命令模式，是使用vi的入口</li>
<li>通过命令对文件进行常规的编辑操作，例如：定位、翻页、复制、粘帖、删除等等<ul>
<li><code>:</code>进入末行模式</li>
<li><code>i</code> 进入编辑模式(insert) </li>
</ul>
</li>
</ul>
<h5 id="末行模式-–-执行保存、退出等操作"><a href="#末行模式-–-执行保存、退出等操作" class="headerlink" title="末行模式 – 执行保存、退出等操作"></a>末行模式 – 执行保存、退出等操作</h5><ul>
<li>要退出vi 返回到控制台，需要在末行模式下输入命令</li>
<li>末行模式是vi的出口</li>
</ul>
<h5 id="编辑模式-–-正常的编辑文字"><a href="#编辑模式-–-正常的编辑文字" class="headerlink" title="编辑模式 – 正常的编辑文字"></a>编辑模式 – 正常的编辑文字</h5><ul>
<li>末行模式和编辑模式之间的切换需要通过命令模式进行： ESC 返回命令模式</li>
</ul>
<h4 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h4><h5 id="重复次数"><a href="#重复次数" class="headerlink" title="重复次数"></a>重复次数</h5><ul>
<li>在命令模式下，先输入一个数字，再跟上一个命令，可以让该命令重复执行指定次数</li>
</ul>
<h5 id="移动和选择"><a href="#移动和选择" class="headerlink" title="移动和选择"></a>移动和选择</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">     vi 之所以快，关键在于能够快速定位到要编辑的代码行</span><br><span class="line"> 移动命令和编辑操作能够组合使用</span><br><span class="line"> h、j、k、l 分别代表 左、下、上、右</span><br><span class="line">4.2.1 行内移动</span><br><span class="line"> w 向后移动一个单词</span><br><span class="line"> b  向前移动一个单词</span><br><span class="line"> 0 跳到行首</span><br><span class="line"> ^ 跳到行首第一个非空白字符的位置</span><br><span class="line"> $ 跳到行尾</span><br><span class="line">4.2.2 行数移动</span><br><span class="line"> gg 文件顶部</span><br><span class="line"> G  文件末尾</span><br><span class="line"> 数字gg 移动到对应行</span><br><span class="line"> 数字G 移动到数字对应行数</span><br><span class="line">：数字 移动到数字对应行数</span><br><span class="line">4.2.3 屏幕移动</span><br><span class="line"> Ctrl + b 向上翻页</span><br><span class="line"> Ctrl + f 向下翻页</span><br><span class="line"> H  光标跳到屏幕顶部</span><br><span class="line"> M  光标跳到屏幕中间</span><br><span class="line"> L  光标跳到屏幕底部</span><br><span class="line">4.2.4 段落移动</span><br><span class="line"> vi 中以空行来区分段落</span><br><span class="line"> &#123;  跳到上一段段首</span><br><span class="line"> &#125;  跳到下一段段首</span><br><span class="line"> % 在成对的括号间快速切换</span><br><span class="line">4.2.5 标记</span><br><span class="line"> 用 mx 在此行添加标记，可以快速跳转回来，x为标记名称，可以是a~z或A~Z之间的任意一个字母</span><br><span class="line"> 添加了标记的行如果被删除，则标记同时被删除</span><br><span class="line"> 如果在同一行添加了相同的标记，之前添加的标记会被替换</span><br><span class="line"> ‘x  跳转回标记x所在的位置</span><br><span class="line">4.2.6 选中文本</span><br><span class="line"> 在复制文本时首先需要选中文本，在vi 中要选择文本，需要先使用Visual 命令切换到 可视模式</span><br><span class="line"> vi 中提供了三种可视模式，可以方便选择选中文本的方式</span><br><span class="line"> 按 ESC 可以放弃选中，返回到命令模式</span><br><span class="line"> v 可视模式，从光标位置开始按照正常模式选择文本</span><br><span class="line"> V 可视行模式，选中光标经过的完整行</span><br><span class="line"> Ctrl + v 可视块模式，垂直方向选中文本</span><br><span class="line">     在可视块模式下，需要键入 I 进入编辑模式</span><br></pre></td></tr></table></figure>
<h5 id="编辑操作"><a href="#编辑操作" class="headerlink" title="编辑操作"></a>编辑操作</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">     删除、复制、粘贴、替换、缩排</span><br><span class="line">4.3.1 撤销和恢复撤销</span><br><span class="line"> u 撤销上次操作(undo)</span><br><span class="line"> CTRL + r 恢复撤销的命令(redo)</span><br><span class="line">4.3.2 删除文本</span><br><span class="line"> x 删除光标所在字符，或者选中文字</span><br><span class="line"> d(移动命令) 删除移动命令所对应的内容</span><br><span class="line"> dd 删除光标所在的行，可以ndd删除多行</span><br><span class="line"> D 删除至行尾</span><br><span class="line"> 删除命令可以和移动命令连用，常见用法如下：</span><br><span class="line"> dw  从光标位置删除至单词末尾</span><br><span class="line"> d0  从光标位置删除到一行的起始位置</span><br><span class="line"> d&#125;  从光标位置删除至本段结尾</span><br><span class="line"> ndd 从光标位置向下连续删除n行</span><br><span class="line"> dnG 将光标所在行至指定行直接所有代码全部删除</span><br><span class="line"> d&apos;a 将光标所在位置到标记之间所有代码全部删除</span><br><span class="line">4.3.3 复制、粘贴</span><br><span class="line"> vi 中提供有一个被复制文本的缓冲区</span><br><span class="line"> 复制命令会选中的文字保存在缓冲区</span><br><span class="line"> 删除命令删除的文字将保存在缓冲区</span><br><span class="line"> 在需要的位置可以使用粘帖命令将缓冲区的文字插入到光标所在的位置</span><br><span class="line"> y(移动命令) 复制(copy)</span><br><span class="line"> yy 复制一行，可以nyy复制多行</span><br><span class="line"> p  粘贴(paste)</span><br><span class="line"> 可以在编辑模式下使用鼠标右键粘贴</span><br><span class="line">4.3.4 替换</span><br><span class="line"> r 在命令模式下替换当前字符(replace)</span><br><span class="line"> R 替换模式，替换当前行光标后的字符</span><br><span class="line"> 对文件进行轻量级的修改，不用进入编辑模式，ESC 可以返回到命令模式</span><br><span class="line">4.3.5 缩排和重复执行</span><br><span class="line"> &gt;&gt; 向右增加缩进</span><br><span class="line"> &lt;&lt; 向左减少缩进</span><br><span class="line"> . 重复上次命令</span><br><span class="line"> 一次性在选中的代码行前增加4个空格，就叫增加缩进</span><br><span class="line"> 一次性在选中的代码行前删除4个空格，就叫减少缩进</span><br><span class="line"> 在可视模式下，缩排命令只需要使用一个 &gt; 或 &lt; </span><br><span class="line">     在程序中，缩进通常用来表示代码的归属关系，前面的空格越少，代码的级别越高</span><br></pre></td></tr></table></figure>
<h5 id="查找和替换"><a href="#查找和替换" class="headerlink" title="查找和替换"></a>查找和替换</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">    4.4.1 查找</span><br><span class="line">  /str  查找str</span><br><span class="line">  n  自动跳到下一个匹配项(next)</span><br><span class="line">  N  自动跳到上一个匹配项</span><br><span class="line">  *  向后查找 当前光标 所在的单词</span><br><span class="line">  #  向前查找 当前光标 所在的单词</span><br><span class="line">  匹配的所有内容会高亮显示，如果不想看到高亮，直接查找一个文本中不存在的东西即可</span><br><span class="line">4.4.2 查找并替换</span><br><span class="line">  在vi中，查找和替换命令需要在 末行模式 下执行</span><br><span class="line">  固定格式   :%s///g                #  :进入末行模式</span><br><span class="line">  全局替换   :%s/旧文本/新文本/g     #  一次性替换所有旧文本</span><br><span class="line">  可视区域范围替换   :s/旧文本/新文本/g</span><br><span class="line">  确认替换  把末尾的 g 改成 gc 在替换的时候，会有!</span><br><span class="line">    y  确认替换(yes)</span><br><span class="line">    n  不替换(no)</span><br><span class="line">    a  替换所有(all)</span><br><span class="line">    q  退出替换(quit)</span><br><span class="line">    l  替换最后一个，并把光标移动到行首(last)</span><br><span class="line">    ^g 向下滚屏</span><br><span class="line">    ^Y 向上滚屏</span><br><span class="line"> 4.4.3 插入命令</span><br><span class="line">   i  在当前字符前插入文本</span><br><span class="line">   I  在行首插入文本</span><br><span class="line">   a  在当前字符后添加文本</span><br><span class="line">   A  在行末添加文本</span><br><span class="line">   o  在当前行后面插入空行</span><br><span class="line">       O  在当前行上面插入空行</span><br></pre></td></tr></table></figure>
<h5 id="末行模式扩展"><a href="#末行模式扩展" class="headerlink" title="末行模式扩展"></a>末行模式扩展</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">      e .   打开内置的文件浏览器，浏览当前目录下的文件(切换前要求当前文件先保存),e 后面也可直接跟文件名</span><br><span class="line">n 文件名  新建文件</span><br><span class="line">      w 文件名  另存为，但是仍然编辑当前文件，并不会切换文件。在项目实战中方便阶段性备份代码</span><br></pre></td></tr></table></figure>
<h5 id="分屏命令"><a href="#分屏命令" class="headerlink" title="分屏命令"></a>分屏命令</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">       使用分屏命令可是同时查看和编辑多个文件</span><br><span class="line">sp 文件名     横向增加分屏(split)</span><br><span class="line">vsp 文件名    纵向增加分屏</span><br><span class="line">分屏窗口都是基于CTRL + w 这个快捷键的,相当于中间过渡，w对应的是window</span><br><span class="line">w  切换到下一个窗口</span><br><span class="line">r  互换窗口</span><br><span class="line">c  关闭当前窗口，但是不能关闭最后一个窗口(close)</span><br><span class="line">q  退出当前窗口，如果是最后一个窗口则关闭vi</span><br><span class="line">o  关闭其他的窗口(other)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux 系统信息相关命令</title>
    <url>/2019/10/24/Linux-%E7%B3%BB%E7%BB%9F%E4%BF%A1%E6%81%AF%E7%9B%B8%E5%85%B3%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<h4 id="时间和日期"><a href="#时间和日期" class="headerlink" title="时间和日期"></a>时间和日期</h4><p><code>date</code>  查看系统时间<br><code>cal [-y]</code>  显示月历、[-y 表示查看年历]</p>
<h4 id="磁盘信息"><a href="#磁盘信息" class="headerlink" title="磁盘信息"></a>磁盘信息</h4><p><code>df [-h]</code> ： disk free 显示磁盘剩余空间<br><code>du [-h] [目录名]</code> ： disk usage 显示目录下文件大小<br><code>-h</code> 表示以人性化的方式显示文件大小</p>
<h4 id="进程信息"><a href="#进程信息" class="headerlink" title="进程信息"></a>进程信息</h4><p><code>ps aux</code> : process status 查看进程的详细情况<br><code>top</code> : 动态显示运行中的进程并且排序； 按q退出动态显示<br><code>kill [-9] 进程代号</code>： 终止指定代号，-9表示强行终止 </p>
<pre><code>*  ps 默认只会显示当前用户通过终端启动的应用程序 
</code></pre><p><code>a</code> : 显示终端上的所有进程，包括其他用户的进程<br><code>u</code> : 显示进程的详细状态<br><code>x</code> : 显示没有控制终端的进程 </p>
<h4 id="查找文件"><a href="#查找文件" class="headerlink" title="查找文件"></a>查找文件</h4><p><code>find</code> 用来在特定的目录下搜索符合条件的文件<br><code>find [路径] -name &quot;.py&quot;</code>: 查找指定路径下扩展名是.py的文件，包括子目录 </p>
<ul>
<li>如果省略路径表示在当前文件夹下查找</li>
<li>通配符 * 可以和find 联用</li>
</ul>
<h4 id="软链接与硬链接"><a href="#软链接与硬链接" class="headerlink" title="软链接与硬链接"></a>软链接与硬链接</h4><p><code>In [-s]</code> 被链接的源文件 链接文件 ： 建立文件的软链接，链接文件相当于一个快捷方式  </p>
<ul>
<li>省去-s选项建立的是一个硬链接文件，硬链接以指针的形式链接到源文件，软链接是符号链接</li>
<li>源文件要使用绝对路径，不能使用相对路径，这样可以方便移动链接文件后，仍然可以正常使用</li>
<li>软链接如果源文件删除或者移动后就失效了，而硬链接并无影响</li>
</ul>
<p>关于硬链接：  </p>
<ul>
<li>在Linux中，文件名和文件数据是分开存储的，如果要想删除文件数据，必须将文件名和所有的硬链接删除后才能删除。</li>
</ul>
<h4 id="索引节点inode"><a href="#索引节点inode" class="headerlink" title="索引节点inode"></a>索引节点inode</h4><p>[<a href="http://www.ruanyifeng.com/blog/2011/12/inode.html]" target="_blank" rel="noopener">http://www.ruanyifeng.com/blog/2011/12/inode.html]</a></p>
<ul>
<li>若干个扇区组成块，是文件存取的最小单位，最常见的是4KB，即连续八个 sector组成一个 block。 </li>
<li>文件数据都储存在”块”中，那么很显然，我们还必须找到一个地方储存文件的元信息，比如文件的创建者UID、GID、权限、创建日期、大小等等。</li>
<li>这种储存文件元信息的区域就叫做inode，中文译名为”索引节点”。</li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux 用户权限</title>
    <url>/2019/10/24/Linux-%E7%94%A8%E6%88%B7%E6%9D%83%E9%99%90/</url>
    <content><![CDATA[<h4 id="用户和权限"><a href="#用户和权限" class="headerlink" title="用户和权限"></a>用户和权限</h4><p>用户管理包括用户与组管理<br>对文件/目录的权限包括：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">读    r  4  </span><br><span class="line">写    w  2  </span><br><span class="line">执行  x  1</span><br></pre></td></tr></table></figure></p>
<p>用ls -l 可以以列表的形式查看文件的详细信息<br>第一列是 <code>-</code> 表明是文件， <code>d</code> 则表示目录<br>后面的依次是 拥有者权限，组权限，其他用户权限<br>再后面是硬链接数 拥有者，组，大小，时间，名称   </p>
<p><strong>硬链接数含义</strong>： 通俗的讲就是有多少种方式可以访问到当前目录/文件<br>注：要想在终端对文件进行操作，必须有可执行权限</p>
<h4 id="组"><a href="#组" class="headerlink" title="组"></a>组</h4><p>为了方便用户管理，提出组的概念，在实际应用中可以预先对组设置好权限，然后将不同的用户添加到对应的组中，从而不用依次为每个用户设置权限</p>
<h4 id="chmod-修改文件-拥有者和组-权限"><a href="#chmod-修改文件-拥有者和组-权限" class="headerlink" title="chmod 修改文件 拥有者和组 权限"></a>chmod 修改文件 拥有者和组 权限</h4><p>chmod可以修改用户/相对文件/目录的权限<br><code>chmod +/-rwx 文件名|目录名</code><br><code>chmod [options] mode file...</code>  </p>
<p><code>-R</code>： 改变目录及其所有子目录的文件的权限<br><code>-v</code> ：详细说明权限的变化</p>
<p>关于超级用户  </p>
<ul>
<li>Linux 系统中的root 帐号通常用于系统的维护和管理，对操作系统的所有资源具有访问权限</li>
<li>Linux系统在安装过程中，系统会自动创建一个用户帐号，而这个默认的用户就成为“标准用户”</li>
<li>su 是substitute user缩写，表示使用另一个用户的身份</li>
<li>sudo命令用来以其他身份执行命令，预设的身份为root</li>
<li>用户使用sudo时，必须先输入密码，之后会有5分钟的有效期，超过期限必须重新输入密码</li>
</ul>
<h4 id="组管理终端命令"><a href="#组管理终端命令" class="headerlink" title="组管理终端命令"></a>组管理终端命令</h4><p>Tips：创建组/删除组的终端命令都需要通过 sudo 执行,组信息保存在<code>/etc/group</code>文件中<br> 添加组： <code>groupadd  group_name</code><br>删除组： <code>groupdel  group_name</code><br>确认组信息： <code>cat/etc/group</code><br>递归修改文件/目录的所属组： <code>chgrp -R group_name 文件/目录名</code></p>
<h4 id="创建用户-设置密码-删除用户"><a href="#创建用户-设置密码-删除用户" class="headerlink" title="创建用户/设置密码/删除用户"></a>创建用户/设置密码/删除用户</h4><p>添加新用户： <code>useradd -m -g 组名 用户名</code>  </p>
<ul>
<li><code>-m</code> 自动建立用户家目录，如果忘了添加-m从而无法在home目录下看到新用户，可以删除用户重新创建</li>
<li><code>-g</code> 指定用户所在组，否则创建一个同名的组</li>
</ul>
<p>设置用户密码： <code>passwd 用户名</code></p>
<ul>
<li>如果是普通用户，直接用passwd可以修改自己的账户密码</li>
</ul>
<p>删除用户： <code>userdel -r 用户名</code></p>
<ul>
<li><code>-r</code> 会自动删除用户家目录</li>
</ul>
<h4 id="查看用户信息"><a href="#查看用户信息" class="headerlink" title="查看用户信息"></a>查看用户信息</h4><p><code>id 用户名</code> ： 查看用户UID 和 GID信息<br><code>who</code>： 查看当前所有登录的用户列表<br><code>whoami</code>： 查看当前登录用户的账号名  </p>
<h4 id="passwd"><a href="#passwd" class="headerlink" title="passwd"></a>passwd</h4><p><code>/etc/passwd</code> 文件存放的是用户的信息，由6个分号组成的7个信息，分别是：</p>
<ul>
<li>用户名;密码（x,表示加密的密码）;UID;GID;用户全名或本地账号;家目录;登录使用的Shell，就是登录之后，使用的终端命令<br>例：<code>liuyuqiang0:x:1000:1000:liuyuqiang0:/home/liuyuqiang0:/bin/bash</code>  <ul>
<li>passwd 可以修改当前用户的登录密码，更新用户的身份认证令牌.</li>
<li>在超级管理员权限下可以修改任何用户的密码，命令格式为passwd +要修改的用户的名称<ul>
<li><code>-d</code>：删除密码，仅有系统管理者才能使用；</li>
<li><code>-f</code>：强制执行；</li>
<li><code>-k</code>：设置只有在密码过期失效后，方能更新；</li>
<li><code>-l</code>：锁住密码</li>
<li><code>-s</code>：列出密码的相关信息，仅有系统管理者才能使用；</li>
<li><code>-u</code>：解开已上锁的帐号</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="usermod"><a href="#usermod" class="headerlink" title="usermod"></a>usermod</h4><p><code>usermod</code>可以用来设置用户的主组/附加组和登录Shell<br><code>usermod -g 组 用户名</code>   修改用户的主组</p>
<ul>
<li>主组：通常在新建用户时指定，在etc/passwd 的第四列GID对应的组</li>
</ul>
<p><code>usermod -G 组 用户名   //修改用户的附加组</code></p>
<ul>
<li>附加组：在etc/group中最后一列表示该组的用户列表，用于指定用户的附加权限，需重新登录有效</li>
</ul>
<p><code>usermod -s /bin/bash 用户名</code></p>
<ul>
<li>修改用户登录Shell</li>
</ul>
<h4 id="which"><a href="#which" class="headerlink" title="which"></a>which</h4><p><code>/etc/passwd</code> 是用于保存用户信息的文件<br><code>/usr/bin/passwd</code> 是用于修改用户密码的程序<br><code>which</code>命令可以查看执行命令所在的位置</p>
<p>关于bin和sbin  </p>
<ul>
<li>在Linux 中，绝大多数可执行文件都是保存在<code>/bin、/sbin、/usr/bin、/usr/sbin</code></li>
<li><code>/bin(binary)</code>是二进制执行文件目录，主要用于具体应用</li>
<li><code>/sbin(system binary)</code>是系统管理员专用的二进制代码存放目录，主要用户系统管理</li>
<li><code>/usr/bin(user commands for applications)</code>后期安装的一些软件</li>
<li><code>/usr/sbin(super user commands for applications)</code>超级用户的一些管理程序</li>
</ul>
<h4 id="切换用户"><a href="#切换用户" class="headerlink" title="切换用户"></a>切换用户</h4><p><code>su-用户名</code>： 切换用户，并且切换到用户家目录(不加减号只切换用户)<br><code>exit</code>： 退出当前登录账户，返回上层登录用户</p>
<h4 id="修改文件权限"><a href="#修改文件权限" class="headerlink" title="修改文件权限"></a>修改文件权限</h4><p><code>chgrp -R 组名 文件名|目录名</code></p>
<ul>
<li>递归修改文件|目录的组</li>
</ul>
<p>chown 用户名 文件名|目录名`</p>
<ul>
<li>修改文件|目录的拥有者</li>
</ul>
<p><code>chmod -R 755 文件名|目录名</code></p>
<ul>
<li>递归修改文件权限</li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux 远程登录和复制文件</title>
    <url>/2019/10/24/Linux-%E8%BF%9C%E7%A8%8B%E7%99%BB%E5%BD%95%E5%92%8C%E5%A4%8D%E5%88%B6%E6%96%87%E4%BB%B6/</url>
    <content><![CDATA[<h4 id="SSH-基础"><a href="#SSH-基础" class="headerlink" title="SSH 基础"></a>SSH 基础</h4><p> 关机/重新启动：<code>ssh 用户名@ip</code><br> 远程复制文件 ：<code>scp 用户名@ip：文件名或路径</code>  </p>
<p>在Linux中SSH是非常常用的工具，通过SSH客户端我们可以连接到运行了SSH服务器的远程机器上<br>数据传输是加密的，可以防止信息泄露；数据传输是压缩的，可以提高传输速度<br>SSH客户端使用Secure Shell协议连接到远程计算机的软件程序，能够防止DNS欺骗和IP欺骗<br>SSH服务器的默认端口号是22，默认端口号在连接的时候可以省略。<br>Web服务器：80<br>HTTPS：443<br>FTP服务器：21  </p>
<h4 id="SSH客户端的简单使用"><a href="#SSH客户端的简单使用" class="headerlink" title="SSH客户端的简单使用"></a>SSH客户端的简单使用</h4><p><code>ssh [-p port] user@remote</code></p>
<p>user是在远程机器上的用户名，如果不指定则默认为当前用户<br>remote是远程机器的地质，可以是ip/域名<br>port是SSH Server监听的端口，如果不指定则默认为22<br>tips：使用exit退出当前用户的登录  </p>
<h4 id="SCP"><a href="#SCP" class="headerlink" title="SCP"></a>SCP</h4><p>scp就是Secure copy，是一个在linux下用来远程拷贝文件的命令<br>它的地址格式与SSH基本相同，在指定端口时用的是大写的-P，而不是小写的。<br>例1：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp -P port 01.py user@remote:Desktop/01.py  </span><br><span class="line"># 把本地当前目录下的01.py 文件复制到远程家目录下的Desktop.py</span><br><span class="line"># 如果：后面的路径不是绝对路径，则以用户的家目录作为参照路径</span><br></pre></td></tr></table></figure></p>
<p>例2：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 加上-r选项可以传送文件夹</span><br><span class="line"># 把当前目录下的demo 文件夹复制到远程家目录下的Desktop</span><br><span class="line">scp -r demo user@remote:Desktop</span><br></pre></td></tr></table></figure></p>
<p>ssh和scp这个两终端命令只能在Linux或者Unix系统下使用<br>如果是windows系统中，可以安装PuTTy 或者 XShell 客户端软件即可</p>
<h4 id="SSH免密码登录与配置别名-通常只需配置一次"><a href="#SSH免密码登录与配置别名-通常只需配置一次" class="headerlink" title="SSH免密码登录与配置别名 (通常只需配置一次)"></a>SSH免密码登录与配置别名 (通常只需配置一次)</h4><p>tips：有关SSH配置信息都 保存在用户家目录下的.ssh目录下  </p>
<h5 id="免密码登录"><a href="#免密码登录" class="headerlink" title="免密码登录"></a>免密码登录</h5><p>配置公钥：ssh-keygen 即可生成SSH钥匙，一路回车即可。<br>上传公钥到服务器： 执行<code>ssh-copy-id -p port user@remote</code>可以让远程服务器记住我们的公钥  </p>
<p>本地使用私钥对数据进行加密/解密<br>服务器使用公钥对数据进行加密/解密</p>
<h4 id="配置别名"><a href="#配置别名" class="headerlink" title="配置别名"></a>配置别名</h4><p>用别名替换 user@remote和port等很方便。<br>在<code>~/.ssh/config</code> 里面追加：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Host name</span><br><span class="line">  HostName ip地址</span><br><span class="line">  User 服务器名称</span><br><span class="line">  Port 22</span><br></pre></td></tr></table></figure></p>
<h4 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h4><p>背景： 想把服务器上的题目数据导入到NYOJ,在Windows下操作太过麻烦。  </p>
<p>不用登录服务器，直接在本地终端即可完成，在建立传输时需要输入服务器登录密码。</p>
<h5 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h5><ul>
<li>先把服务器上的数据下载到本地。</li>
<li>目录：  /home/judge/data</li>
<li>在本地/home/liuyuqiang0下新建一个目录data： <code>mkdir/home/liuyuqiang0/data</code></li>
<li>在终端下使用scp命令远程复制文件：<code>[liuyuqiang0@localhost ~]$ scp -r ubuntu@ip:/home/judge/data /home/liuyuqiang0/data</code></li>
<li>查看对应文件，等待传输完成。</li>
<li>对应文件保存位置： <code>/home/liuyuqiang0/data/data</code> #  早知道不建立data目录了，直接在~目录下下载。。</li>
</ul>
<h5 id="坑"><a href="#坑" class="headerlink" title="坑"></a>坑</h5><ul>
<li>目前只使用过上述格式的命令，<ul>
<li>对于： <code>scp -r /home/judge/data ubuntu@ip /home/liuyuqiang0/data</code> 格式还没有使用过</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux文件和目录常用命令</title>
    <url>/2019/10/24/Linux%E6%96%87%E4%BB%B6%E5%92%8C%E7%9B%AE%E5%BD%95%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<h4 id="Linux文件系统和windos文件系统区别"><a href="#Linux文件系统和windos文件系统区别" class="headerlink" title="Linux文件系统和windos文件系统区别"></a>Linux文件系统和windos文件系统区别</h4><ol>
<li>linux区分大小写，windows在dos界面命令下不区分大小写</li>
<li>linux所有内容均以文件形式保存包括硬件、用户，而windows文件和硬件没什么关系，两个之间没有关联；</li>
<li>windows用扩展名区分文件如.exe代表执行文件，.txt代表文本文件，而linux无扩展名的概念，<br>当然为了管理员区分方便会有部分扩展，例如.gz ，.bz2 ，.tar.bz2代表压缩包，.html ，,php代表网页文件，这些纯粹是给管理员看的便于区分，<br>但是linux本身是没有扩展名的，linux是以权限区分文件的，文件权限总共有十位（可用ls -l查看每个文件的权限）</li>
<li>windows属于多根目录文件系统，linux属于根目录文件系统，所有文件都在根目录()下，</li>
<li>Linux有四种基本文件系统类型：普通文件、目录文件、连接文件和特殊文件，可用file命令来识别。<br>详见：<a href="https://www.cnblogs.com/cyjaysun/p/4462325.html" target="_blank" rel="noopener">https://www.cnblogs.com/cyjaysun/p/4462325.html</a></li>
<li>Linux文件或者目录名称最长可以有256个字符<br>以.开头的文件为隐藏文件，需要使用-a参数才能显示，比如在桌面上touch .123.txt ,这个文件是看不到的<br>.代表当前目录<br>..代表上级目录</li>
</ol>
<h4 id="常见命令"><a href="#常见命令" class="headerlink" title="常见命令"></a>常见命令</h4><h5 id="曾经使用过的命令"><a href="#曾经使用过的命令" class="headerlink" title="曾经使用过的命令"></a>曾经使用过的命令</h5><p>按上下光标键可以在曾经使用过的命令间来回切换<br>如果想退出选择并且不想执行当前选中的命令，可以按 ctrl+c</p>
<h5 id="ls命令常用选项"><a href="#ls命令常用选项" class="headerlink" title="ls命令常用选项"></a>ls命令常用选项</h5><p><code>-a</code>  显示指定目录下所有子目录与文件，包括隐藏文件<br><code>-l</code>  以列表方式显示文件的详细信息(权限，大小，修改时间等),白色字体为文件，蓝色字体为目录首字母为d<br><code>-h</code>  配合-l以人性化的方式显示文件大小,单独使用无效果。<br><code>-lS</code> 列出文件列表的同时按size从大到小排序<br><code>-r</code>  按行逆序输出<br><code>-R</code>  递归列出遇到的子目录,当内容太多可以用ctrl+c停止终端输出<br><code>-t</code>  按时间顺序排序<br>以上选项常常参杂配合使用，十分灵活，下面有几个常见用法：<br><code>ls *.txt</code> : 列出以.txt结尾的文件（*为通配符）<br><code>ls -al &gt; mydirectorylist</code>: 将目录信息写入文件(&gt;为输出重定向，若文件存在则覆盖，否则创建并写入)<br><code>ls -ltr</code> : 以修改时间倒序列出</p>
<h5 id="ls通配符的使用"><a href="#ls通配符的使用" class="headerlink" title="ls通配符的使用"></a>ls通配符的使用</h5><p><code>*</code> 代表任意多个字符<br><code>?</code>  代表任意一个字符<br><code>[]</code> 代表可以匹配字符数组中的任意一个<br><code>[abc]</code> 匹配a\b\c中的任意一个<br><code>[a-f]</code> 匹配从a-f范围内的任意一个字符</p>
<h5 id="touch"><a href="#touch" class="headerlink" title="touch"></a>touch</h5><p>创建文件或修改文件时间（普通文件），如果文件不存在则创建一个空白文件，否则修改文件的修改时间。<br><code>-a</code> 只更改访问时间<br><code>-c</code> 不创建任何文件<br><code>-d</code> 用指定数字串（4位以内必须合法）表示文件时间<br><code>-m</code> 只更改修改时间，和访问时间不同</p>
<h5 id="mkdir"><a href="#mkdir" class="headerlink" title="mkdir"></a>mkdir</h5><p>在当前目录下创建新的目录，新的目录名称不能与当前目录中已有的目录或文件重名<br><code>mkdir</code>命令不加任何选项默认在当前目录下创建目录，目录的默认权限为<code>777-umask</code>。如果目录已经存在将会提示错误。<br><code>-p</code>  递归创建目录,若要创建的路径已存在，则忽略<br><code>-m</code>  为目录指定权限,如 <code>mkdir -m=777 a</code> （创建目录a并指定权限为：rwxrwxrwx）<br><code>-v</code>  为每个目录打印提示信息，一般和-p联用<br>例： <code>mkdir -pv 123/{1,2/{3,4}}</code>    #注意不要有多余空格<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir: 已创建目录 &quot;123&quot; </span><br><span class="line">mkdir: 已创建目录 &quot;123/1&quot;   </span><br><span class="line">mkdir: 已创建目录 &quot;123/2&quot;  </span><br><span class="line">mkdir: 已创建目录 &quot;123/2/3&quot;  </span><br><span class="line">mkdir: 已创建目录 &quot;123/2/4&quot;</span><br></pre></td></tr></table></figure></p>
<h5 id="rm命令删除文件或目录，直接从磁盘上删除，而非放入回收站，无法恢复，慎用！"><a href="#rm命令删除文件或目录，直接从磁盘上删除，而非放入回收站，无法恢复，慎用！" class="headerlink" title="rm命令删除文件或目录，直接从磁盘上删除，而非放入回收站，无法恢复，慎用！"></a>rm命令删除文件或目录，直接从磁盘上删除，而非放入回收站，无法恢复，慎用！</h5><p><code>-r</code> 递归删除目录下的内容，若删除的是文件夹则必须加此参数<br><code>-f</code> 强制删除，忽略不存在的文件，不显示提示信息<br><code>-i</code> 提示是否移除每个文件。如果回答是否定的（y/n），文件将被跳过<br><code>-v</code> 在移除每个文件之前打印其名称，一般和-r联用<br>例：  <code>mkdir -p 123/1/2</code><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rm -rv 123  </span><br><span class="line">已删除目录：&quot;123/1/2&quot;  </span><br><span class="line">已删除目录：&quot;123/1&quot;  </span><br><span class="line">已删除目录：&quot;123&quot;</span><br></pre></td></tr></table></figure></p>
<h6 id="rm与rmdir"><a href="#rm与rmdir" class="headerlink" title="rm与rmdir"></a>rm与rmdir</h6><p><code>rmdir [options] directory...</code><br><code>rmdir</code> 删除空目录。如果所给出的目录不为空，则报错<br><code>-p</code>如果目录由多个路径名组成，从最后一个路径名开始依次删除，直到所有的路径名都被删完。<br>例如：命令<code>rmdir -p a/b/c</code>按 照<code>rmdir/a/b/c</code>; <code>rmdir a/b</code>; <code>rmdir a</code>的顺序删除目录。</p>
<h5 id="cp与mv，拷贝和移动文件"><a href="#cp与mv，拷贝和移动文件" class="headerlink" title="cp与mv，拷贝和移动文件"></a>cp与mv，拷贝和移动文件</h5><h6 id="copy"><a href="#copy" class="headerlink" title="copy"></a>copy</h6><p><code>cp</code> 源文件 目标文件<br>cp 复制文件(或者目录等),可以使用这个命令复制一个文件到一个指定的目的地,或者复制任意多个文件到一个目的目录.<br>如果目标路径已有重名文件则覆盖，否则直接复制并给其命名。<br><code>-i</code>  提示是否覆盖现有普通目标文件。<br><code>-r</code>  若给出的源文件是目录文件，则递归复制该目录下的所有子目录和文件，目标文件必须为一个目录名,否则提示无法覆盖非目录  </p>
<h6 id="move"><a href="#move" class="headerlink" title="move"></a>move</h6><p><code>mv 源文件 目标文件</code><br>mv改“源文件”名到“目标文件”名, 或移动“源文件”(可以不只一 个)到一个“目录”<br>若操作合法的前提下：当目标文件含有与源文件重名的文件时直接覆盖；当目标文件不存在则创建目标文件再将内容移入，相当于重命名<br><code>-f</code> 直接覆盖不提示<br><code>-i</code> 覆盖前提示<br><code>-v</code> 说明完成了什么  </p>
<h5 id="查看文件内容"><a href="#查看文件内容" class="headerlink" title="查看文件内容"></a>查看文件内容</h5><h6 id="cat-选项列表-文件列表"><a href="#cat-选项列表-文件列表" class="headerlink" title="cat [选项列表] [文件列表]..."></a><code>cat [选项列表] [文件列表]...</code></h6><p>将文件列表中的文件或标准输入连接到标准输出。<br><code>-A</code>  全部显示<br><code>-b</code>  给非空行输出行号<br><code>-n</code>  给所有行输出行号<br><code>-s</code>  将所有的连续的多个空行替换为一个空行</p>
<h6 id="more"><a href="#more" class="headerlink" title="more"></a>more</h6><p><code>more</code>是一个过滤器, 用于分页显示(一次一屏)文本.<br><code>-num</code> 这个选项指定屏幕的行数 (以整数表示).<br><code>+num</code> 从行号num开始显示<br><code>-f</code> 使more计数逻辑行, 而不是屏幕行 (就是说, 长行不会断到 下一行).<br><code>-s</code> 把重复的空行压缩成一个空行.  </p>
<h6 id="grep"><a href="#grep" class="headerlink" title="grep"></a><code>grep</code></h6><p><code>grep [options] PATTERN [FILE...]</code><br>Grep 搜索以 FILE 命名的文件输入(或者是标准输入，如果没有指定文件名，或者给出的文件名是-的话)，寻找含有与给定的模式PATTERN相匹配的内容的行。<br>默认情况下，grep将把含有匹配内容的行打印出来。<br><code>^a</code> 行首，搜寻以a开头的行<br><code>a$</code> 行尾，搜寻以a结束的行<br><code>-H</code> 为每个匹配打印文件名。<br><code>-i</code> 忽略模式PATTERN 和输入文件中的大小写的分别。<br><code>-n</code> 在输出的每行前面加上它所在的文件中它的行号。<br><code>-R/r</code> 递归地读每一目录下的所有文件。这样做和 -d recurse 选项等价<br><code>-v</code> 改变匹配的意义，只选择不匹配的行  </p>
<p>正则表达式是一个描述了一个字符串集合的模式。正则表达式的构造类似于算<br>术表达式，使用各种各样的操作符来将更小的表达式连在一起</p>
<h5 id="echo"><a href="#echo" class="headerlink" title="echo"></a>echo</h5><p><code>echo[OPTION]... [STRING]...</code><br>允许在标准输出上显示STRING(s).<br><code>-n</code> 不输出行尾的换行符.<br><code>-e</code> 允许对下面列出的加反斜线转义的字符进行解释.<br><code>-E</code> 禁止对在STRINGs中的那些序列进行解释.  </p>
<h5 id="重定向"><a href="#重定向" class="headerlink" title="重定向"></a>重定向</h5><p><code>&gt;</code>  将命令执行结果重定向到一个文件,如果文件已存在，直接覆盖，否则创建文件并写入<br><code>&gt;&gt;</code> 表示追加，将内容追加到已有文件的末尾,如果文件不存在则创建文件并写入</p>
<h5 id="管道"><a href="#管道" class="headerlink" title="管道|"></a>管道<code>|</code></h5><p>Linux允许将一个命令的输出通过管道作为另一个命令的输入， | 的左端为写，右端为读（管道至少需要两个命令）<br>常用的管道命令：<br><code>more</code>： 分屏显示内容<br><code>grep</code>： 在命令执行结果的基础上查询指定的文本<br>如： <code>ls -lha | grep -n d</code></p>
<h4 id="实验：在输出信息的同时把信息记录到文件中"><a href="#实验：在输出信息的同时把信息记录到文件中" class="headerlink" title="实验：在输出信息的同时把信息记录到文件中"></a>实验：在输出信息的同时把信息记录到文件中</h4><h5 id="tee-命令"><a href="#tee-命令" class="headerlink" title="tee 命令"></a><code>tee</code> 命令</h5><p>用法：<code>tee [选项]... [文件]...</code><br>将标准输入复制到每个指定文件，并显示到标准输出。<br>-a 内容追加到给定的文件而非覆盖<br>例： <code>ls -l ha | tee -a 1.txt</code></p>
<h5 id="将一个文件中的内容读入另一个文件中：cat"><a href="#将一个文件中的内容读入另一个文件中：cat" class="headerlink" title="将一个文件中的内容读入另一个文件中：cat"></a>将一个文件中的内容读入另一个文件中：cat</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat a &gt;&gt; b</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux 常见终端命令</title>
    <url>/2019/10/24/Linux-%E5%B8%B8%E8%A7%81%E7%BB%88%E7%AB%AF%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<p>终端命令格式: <code>command [-options] [parameter]</code><br><code>command</code>： 命令名，相应功能的英文单词或单词缩写如<code>cd</code>、<code>rm</code><br><code>[-options]</code>：选项，可用来对命令进行控制，也可以省略<br><code>parameter</code>：传给命令的参数，可以是了零个、一个，也可以是多个<br><code>[]</code>表示可选</p>
<ol>
<li><code>ls</code>：  查看当前文件夹下的内容(list)</li>
<li><code>pwd</code>： 查看当前所在的文件夹(print work directory)</li>
<li><code>cd [目录名]</code>： 切换文件夹（change directory）,后面必须接目录或者文件夹才能切换</li>
<li><code>mkdir [目录名]</code>： 在当前文件夹下创建目录（make directory）</li>
<li><code>touch [文件名]</code>：创建文件和修改文件或者目录的时间戳，文件已存在则修改</li>
<li><code>clear</code> : 清屏，类似dos窗口下的cls命令</li>
<li><code>man [命令]</code>： Linux下的帮助指令，通过man指令可以查看Linux中的指令帮助、配置文件帮助和编程帮助等信息</li>
</ol>
<p>Tips：</p>
<ol>
<li><p>终端窗口字体大小调整<br>放大终端窗口字体显示： <code>ctrl</code> + <code>shift</code> + <code>=</code><br>缩小终端窗口字体显示： <code>ctrl</code> + <code>-</code></p>
</li>
<li><p><code>tab</code>键<br>键入首字母后会自动补全，有多个匹配项时需多按几次</p>
</li>
<li><p>查阅命令帮助信息<br><code>command --help</code>： 显示command命令的帮助信息<br><code>man command</code> ：查阅此命令的使用手册</p>
</li>
<li><p>切换目录<br><code>cd ~</code> : 快速回到当前用户的home目录<br><code>cd -</code> ： 可以在最近两次工作目录之间来回切换</p>
</li>
</ol>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Python文件操作与文件编码方式</title>
    <url>/2019/10/24/Python%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://www.bilibili.com/video/av68350017/?p=473" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h4><ol>
<li>文件概念</li>
<li>基本操作</li>
<li>文件/文件夹常用操作</li>
<li>文本文件的编码方式</li>
</ol>
<h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><ol>
<li>计算机的文件，就是存储在某种长期储存设备上的一段数据</li>
<li>文件能将数据长期保存下来，在需要的是时候使用</li>
</ol>
<h5 id="存储方式"><a href="#存储方式" class="headerlink" title="存储方式"></a>存储方式</h5><ol>
<li><p>文本文件</p>
<ul>
<li>可以使用文本编辑软件查看</li>
<li>本质上还是二进制文件</li>
</ul>
</li>
<li><p>二进制文件</p>
<ul>
<li>保存的二进制数据，提供给其他软件使用</li>
<li>不能使用文本编辑软件查看</li>
</ul>
</li>
</ol>
<h4 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h4><ol>
<li>打开文件</li>
<li>读写文件(读入内存，写入文件)</li>
<li>关闭文件</li>
</ol>
<h5 id="操作方法"><a href="#操作方法" class="headerlink" title="操作方法"></a>操作方法</h5><p><code>Python</code>中操作文件一个函数三个方法</p>
<ol>
<li><code>open</code>： 打开文件，并且返回文件操作对象</li>
<li><code>read</code>： 将文件内容读取到内存</li>
<li><code>write</code>： 将指定内容写入文件</li>
<li><code>close</code>：关闭文件</li>
</ol>
<p><code>open</code>函数负责打开文件，并且返回文件对象；<code>read/write/close</code>三个方法都需要通过文件对象来调用</p>
<h5 id="read方法"><a href="#read方法" class="headerlink" title="read方法"></a><code>read</code>方法</h5><p><code>open</code>函数的第一个参数是要打开的文件名(区分大小写)<br>如果文件存在则返回文件操作对象(读写指针)，否则抛出异常</p>
<p><code>read</code>方法可以一次性读入并返回文件的所有内容<br><code>close</code>方法负责关闭文件，如果忘记关闭文件，会造成系统资源消耗，而且会影响到后续对文件的访问</p>
<p><strong>注意</strong>： 方法执行后，会把文件指针移动到文件末尾<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 打开文件</span><br><span class="line">fid=open(&apos;file_path&apos;)</span><br><span class="line"></span><br><span class="line"># 读取</span><br><span class="line">text=fid.read()</span><br><span class="line">print(text)</span><br><span class="line"></span><br><span class="line"># 关闭</span><br><span class="line">fid.close()</span><br></pre></td></tr></table></figure></p>
<p>在开发中，通常会先编写<strong>打开和关闭</strong>的代码，再编写中间针对文件的编写</p>
<h5 id="文件指针"><a href="#文件指针" class="headerlink" title="文件指针"></a>文件指针</h5><p>文件指针标记从哪个位置开始读取数据<br>第一次打开文件时，通常文件指针会指向文件的开始位置<br>当执行了<code>read</code>方法后，文件指针会移动到读取内容的末尾</p>
<ul>
<li>默认情况下会移动到文件末尾</li>
</ul>
<p>如果执行了一次<code>read</code>后，再次掉用无法读取到任何内容了</p>
<h5 id="打开文件的方式"><a href="#打开文件的方式" class="headerlink" title="打开文件的方式"></a>打开文件的方式</h5><p><code>open</code>函数默认以<code>只读方式</code>打开文件，并且返回文件对象<br>语法：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fid=open(&quot;文件路径&quot;,&quot;访问方式&quot;)</span><br></pre></td></tr></table></figure></p>
<p>关于访问方式：</p>
<ul>
<li><code>r</code>  只读，文件不存在则抛出异常</li>
<li><code>w</code>  只写，如果文件不存在则创建新文件</li>
<li><code>a</code>  追加，文件指针指向文件末尾，如果文件不存在则创建新文件</li>
<li><code>r+</code> 读写，如果指针指向文件开头，文件不存在则抛出异常</li>
<li><code>w+</code> 读写，如果文件存在会被覆盖，如果文件不存在创建新文件</li>
<li><code>a+</code> 读写，文件指针指向文件末尾，文件不存在则创建新文件进行写入</li>
</ul>
<p>Tips：频繁地移动文件指针会影响文件地读写效率，开发中更多以只读、只写方式来操作文件</p>
<h6 id="readline按行读取"><a href="#readline按行读取" class="headerlink" title="readline按行读取"></a><code>readline</code>按行读取</h6><ul>
<li><code>read</code>方法默认会把文件地所有内容一次性读取到内存</li>
<li>如果文件太大，对内存地占用非常严重</li>
<li><code>readline</code>方法一次读取一行内容</li>
<li>方法执行后，文件指针移动到下一行，准备再次读取</li>
</ul>
<p>读取大文件地正确姿势：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fid=open(&apos;file_path&apos;)</span><br><span class="line">while True:</span><br><span class="line">    text=fid.readline()</span><br><span class="line">    if not text:break</span><br><span class="line">    print(text,end=&quot;&quot;)</span><br><span class="line">fid.close()</span><br></pre></td></tr></table></figure></p>
<h5 id="文件读写案例"><a href="#文件读写案例" class="headerlink" title="文件读写案例"></a>文件读写案例</h5><p>目标： 实现文件复制<br>如果是小文件复制，直接<code>read</code>完整内容就可以；如果是大文件，则按行读取按行写入<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 先创建文件手动添加内容</span><br><span class="line"># fid=open(&apos;haha1&apos;,&apos;w&apos;)</span><br><span class="line"># fid.write(&apos;123&apos;)</span><br><span class="line"># fid.write(&apos;\n&apos;)</span><br><span class="line"># fid.write(&apos;456&apos;)</span><br><span class="line"># fid.close()</span><br><span class="line"></span><br><span class="line"># 小文件复制</span><br><span class="line">fid=open(&apos;haha1&apos;,&apos;r&apos;)</span><br><span class="line">fid2=open(&apos;haha2&apos;,&apos;w&apos;)</span><br><span class="line">text=fid.read()</span><br><span class="line">fid2.write(text)</span><br><span class="line"># print(text)</span><br><span class="line">fid.close()</span><br><span class="line">fid2.close()</span><br><span class="line"></span><br><span class="line"># 大文件复制</span><br><span class="line">fid=open(&apos;haha1&apos;,&apos;r&apos;)</span><br><span class="line">fid2=open(&apos;haha2&apos;,&apos;w&apos;)</span><br><span class="line">while True:</span><br><span class="line">    text=fid.readline()</span><br><span class="line">    if not text:break</span><br><span class="line">    fid2.write(text)</span><br><span class="line"></span><br><span class="line">fid.close()</span><br><span class="line">fid2.close()</span><br></pre></td></tr></table></figure></p>
<h4 id="导入os模块执行文件和目录管理操作"><a href="#导入os模块执行文件和目录管理操作" class="headerlink" title="导入os模块执行文件和目录管理操作"></a>导入<code>os</code>模块执行文件和目录管理操作</h4><p>在终端/文件浏览器中可以执行常规地文件/目录管理操作：创建、重命名、删除、更改路径、查看…<br>在<code>Python</code>中，如果希望通过程序实现上述功能，需要导入<code>os</code>模块</p>
<h5 id="文件操作"><a href="#文件操作" class="headerlink" title="文件操作"></a>文件操作</h5><ul>
<li><code>rename</code> 重命名文件，<code>os.rename(源文件名,目标文件名)</code></li>
<li><code>remove</code> 删除文件, <code>os.remove(文件名)</code></li>
</ul>
<h5 id="目录操作"><a href="#目录操作" class="headerlink" title="目录操作"></a>目录操作</h5><ul>
<li><code>listdir</code> ：目录列表，<code>print(os.listdir(&#39;.&#39;))</code></li>
<li><code>mkdir</code>：创建目录，<code>os.mkdir(目录名)</code></li>
<li><code>rmdir</code>：删除目录，<code>os.rmdir(目录名)</code></li>
<li><code>getcwd</code>：获取当前目录，<code>os.getcwd()</code></li>
<li><code>chdir</code>： 修改工作目录， <code>os.chdir(目标目录)</code></li>
<li><code>path.isdir</code>：判断是否文件，<code>os.path.isdir(文件路径)</code></li>
</ul>
<p>Tips：文件或目录都支持相对路径和绝对路径<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import os</span><br><span class="line">print(os.getcwd())</span><br><span class="line">print(os.listdir(&apos;.&apos;))</span><br><span class="line">print(os.path.isdir(&apos;ML&apos;))</span><br><span class="line"></span><br><span class="line">- - - - - - - - - - - -  -</span><br><span class="line">/home/liuyuqiang0/PycharmProjects/LYQ_YES</span><br><span class="line">[&apos;.idea&apos;, &apos;__pycache__&apos;, &apos;lyq.py&apos;, &apos;craw&apos;, &apos;Creat_pictures&apos;, &apos;ML&apos;, &apos;haha1&apos;, &apos;haha2&apos;]</span><br><span class="line">True</span><br></pre></td></tr></table></figure></p>
<h4 id="文本编码"><a href="#文本编码" class="headerlink" title="文本编码"></a>文本编码</h4><p>文本文件存储的内容是基于字符编码的文件，常见的编码有<code>ASCII</code>编码，<code>UNICODE</code>编码等</p>
<ul>
<li><code>Python2.x</code>默认使用<code>ASCII</code>编码`</li>
<li><code>Python3.x</code>默认使用<code>UTF-8</code>编码</li>
</ul>
<h5 id="ASCII编码与UNICODE编码"><a href="#ASCII编码与UNICODE编码" class="headerlink" title="ASCII编码与UNICODE编码"></a>ASCII编码与UNICODE编码</h5><p>ASCII编码</p>
<ul>
<li>计算机中只有256个<code>ASCII</code>字符</li>
<li>一个<code>ASCII</code>在内存中占用1个字节，即2**8种字符</li>
<li>详见<img src="https://gss3.bdstatic.com/-Po3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike116%2C5%2C5%2C116%2C38/sign=a8288ae7fc1fbe090853cb460a096756/e850352ac65c103880a07b53bc119313b17e8941.jpg" alt="`ASCII`表"></li>
</ul>
<p>UTF-8编码格式</p>
<ul>
<li>计算机种使用1-6个字节来表示一个<code>UTF-8</code>字符，涵盖地球上几乎有所地区的文字</li>
<li>大多数汉字会使用3个字节表示</li>
<li><code>UTF-8</code>是<code>UNICODE</code>编码的一种编码格式</li>
</ul>
<h5 id="Python2-x中使用中文"><a href="#Python2-x中使用中文" class="headerlink" title="Python2.x中使用中文"></a>Python2.x中使用中文</h5><p>在程序第一行增加以下代码，解释器会以<code>utf-8</code>编码来处理<code>Python</code>文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#-*-coding:utf-8-*-</span><br></pre></td></tr></table></figure></p>
<p>或者<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># coding=utf8</span><br></pre></td></tr></table></figure></p>
<p>unicode字符串</p>
<ul>
<li>在<code>Pyhton2.x</code>中，即使指定了文件使用<code>UTF-8</code>的编码格式，但是在遍历字符串时，仍然会以字节为单位遍历字符串</li>
<li>要能够正确的遍历字符串，在定义字符串时，需要在字符串的引号前，增加一个小写字母<code>u</code>，告诉解释器这是一个<code>unicode</code>字符串</li>
<li>9012年，建议使用Python3.X(X&ge;5)</li>
</ul>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python模块与包</title>
    <url>/2019/10/24/Python%E6%A8%A1%E5%9D%97%E4%B8%8E%E5%8C%85/</url>
    <content><![CDATA[<h4 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://www.bilibili.com/video/av68350017/?p=459" target="_blank" rel="noopener">学习地址</a></center></h4><h3 id="模块"><a href="#模块" class="headerlink" title="模块"></a>模块</h3><p>模块是Python程序架构的一个核心概念  </p>
<p>每一个以扩展名<code>py</code>结尾的<code>Python</code>源代码文件都是一个<strong>模块</strong><br>模块名同样也是一个标识符，需要符合标识符的命名规则<br>在模块中定义的全局变量、函数、类都是提供给外界直接使用的<strong>工具</strong><br><strong>模块</strong>就好比是工具包，要想使用这个工具包中的工具，就需要先<strong>导入</strong>这个模块</p>
<h4 id="导入"><a href="#导入" class="headerlink" title="导入"></a>导入</h4><ol>
<li><code>import</code>导入<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import 模块名1, 模块名2</span><br><span class="line">import 模块名3</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>在导入模块时，每个导入应该独占一行<br>通过<code>模块名.</code>就可以使用模块中的工具 – <strong>全局变量、函数、类</strong>。<br>使用<code>as</code>指定模块的别名，作为一种长模块名的缩写方便使用<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import 模块名 as 模块别名</span><br></pre></td></tr></table></figure></p>
<ol start="2">
<li><p><code>from ... import ...</code>导入</p>
<ul>
<li>希望从某一个模块中导入指定工具</li>
<li><code>import 模块名</code>是一次性把模块中所有的工具全部导入，并且通过模块名/别名访问</li>
</ul>
<p>不需要通过<code>模块名.</code>使用工具了，直接使用即可</p>
</li>
<li><p><code>from ... import *</code></p>
<ul>
<li>从模块中导入所有工具，也可以直接使用</li>
<li>实际开发中不推荐使用，因为函数重命没有任何提示，出现问题难以排查</li>
</ul>
</li>
</ol>
<p>注意：</p>
<ul>
<li>如果两个模块，存在同名函数，从不同的模块中导入同名的函数时，后导入模块的函数会覆盖先导入的函数</li>
<li>开发时，<code>import</code> 代码应该统一写在代码顶部，一旦发现冲突可以用<code>as</code>给其中一个函数起别名</li>
</ul>
<h4 id="模块搜索顺序"><a href="#模块搜索顺序" class="headerlink" title="模块搜索顺序"></a>模块搜索顺序</h4><p><code>Python</code>的解释器在导入模块时，会：</p>
<ol>
<li>搜索<strong>当前目录</strong>指定模块名的文件，如果有就直接导入</li>
<li>如果没有，再搜索<strong>系统目录</strong></li>
</ol>
<p>在开发时，给文件起名不要和系统的模块文件重名<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import 模块名</span><br><span class="line">print(模块名.__file__) # 查看模块的路径</span><br></pre></td></tr></table></figure></p>
<h4 id="模块开发原则"><a href="#模块开发原则" class="headerlink" title="模块开发原则"></a>模块开发原则</h4><h5 id="原则"><a href="#原则" class="headerlink" title="原则"></a>原则</h5><ul>
<li>每一个文件都应该是可以被导入的</li>
</ul>
<p>一个独立的<code>Python</code>文件就是一个模块<br>在导入文件时，文件中所有没有任何缩进的代码都会被执行一遍<br>直接执行的代码不是向外界提供的工具(全局变量，函数，类)，这样只要<code>import</code>了这个模块，就会自动执行  </p>
<p>实际开发中：</p>
<ul>
<li>每个模块都是独立开发的，开发人员通常会再模块下方增加一些<strong>测试代码</strong><code>if __name__==__main__</code></li>
</ul>
<h5 id="name-属性"><a href="#name-属性" class="headerlink" title="__name__属性"></a><code>__name__</code>属性</h5><ul>
<li><code>__name__</code>属性可以做到，测试模块的代码只在测试情况下被运行，而在被导入时，不被执行</li>
<li><code>__name__</code>是<code>Python</code>的一个内置属性，记录着一个字符串</li>
<li>如果被<strong>其他文件导入</strong>的，<code>__name__</code>就是所属的模块的模块名</li>
<li>如果是<strong>当前执行的程序</strong>,<code>__name</code>是<code>__main__</code></li>
<li>即<code>if __name__==__main__</code>只会在当前执行的程序中有效，如果这条语句所属的文件被其他文件导入，<code>__name__</code>记录的是模块名，就无法执行了之后的代码了</li>
</ul>
<h3 id="包-package"><a href="#包-package" class="headerlink" title="包(package)"></a>包(package)</h3><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><ol>
<li>包是一个<strong>包含多个模块</strong>的<strong>特殊目录</strong></li>
<li>目录下由一个<strong>特殊的文件</strong><code>__init__.py</code></li>
<li>包的<strong>命名方式</strong>和变量名一致</li>
</ol>
<p>使用<code>import 包名</code>可以一次性导入包中所有的模块</p>
<h4 id="init-py"><a href="#init-py" class="headerlink" title="__init__.py"></a><code>__init__.py</code></h4><p>在当前目录下新建目录<code>Directory</code>或者<code>Python Package</code>，前者需要手动创建<code>__init__.py</code>文件，后者则会自动创建空的<code>__init__.py</code>  </p>
<p>如果在新建的目录也就是我们要用的包下新建了其他的模块，在导入包时想调用模块，<code>table</code>键却无法自动提示所含的模块，那么就需要对<code>__init__.py</code>文件进行修改了</p>
<p>要在外界使用<strong>包</strong>中的模块，需要在<code>__init__.py</code>中指定对外界提供的模块列表<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 从 当前目录 导入 模块列表</span><br><span class="line">from . import 模块1</span><br><span class="line">from . import 模块2</span><br></pre></td></tr></table></figure></p>
<h3 id="发布模块"><a href="#发布模块" class="headerlink" title="发布模块"></a>发布模块</h3><p>如果希望自己开发的模块，分享给他人，可以按照一下步骤</p>
<h4 id="制作模块压缩包"><a href="#制作模块压缩包" class="headerlink" title="制作模块压缩包"></a>制作模块压缩包</h4><h5 id="创建setup-py"><a href="#创建setup-py" class="headerlink" title="创建setup.py"></a>创建<code>setup.py</code></h5><p>路径同所要发布的包在同一目录下<br><code>setup.py</code>文件很固定<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#-*-coding:utf-8-*-</span><br><span class="line">from distutils.core import setup</span><br><span class="line"></span><br><span class="line">setup(name=&apos;craw_picture&apos;, # 包名</span><br><span class="line">      version=&apos;1.0&apos;, # 版本</span><br><span class="line">      description=&apos;&apos;, # 描述信息</span><br><span class="line">      long_description=&apos;&apos;, # 完整的描述信息</span><br><span class="line">      author=&apos;&apos;, # 作者</span><br><span class="line">      author_email=&apos;&apos;, # 作者邮箱</span><br><span class="line">      url=&apos;&apos;, # 个人主页</span><br><span class="line">      py_modules=[&quot;包名.模块1&quot;,&quot;包名.模块2&quot;])</span><br></pre></td></tr></table></figure></p>
<p>最后一个参数很关键，在列表中需要指定包中的模块</p>
<h5 id="构建模块"><a href="#构建模块" class="headerlink" title="构建模块"></a>构建模块</h5><p>在终端中进入<code>setup.py</code>所在目录，然后执行<code>python3 setup.py build</code><br>会在当前目录下生成<code>build</code>目录，其中<code>build</code>目录下的<code>lib</code>文件中有完整的包</p>
<h5 id="生成发布压缩包"><a href="#生成发布压缩包" class="headerlink" title="生成发布压缩包"></a>生成发布压缩包</h5><p>在终端执行<code>python3 setup.py sdist</code><br>当前目录下会生成<code>dist</code>目录，其中有一个压缩包就是我们要分享的包</p>
<h4 id="安装模块"><a href="#安装模块" class="headerlink" title="安装模块"></a>安装模块</h4><p>首先把压缩包解压，进入目标包目录下，然后使用root身份进行安装<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -zxvf 压缩包</span><br><span class="line">sudo python3 setup.py install</span><br></pre></td></tr></table></figure></p>
<h4 id="卸载模块"><a href="#卸载模块" class="headerlink" title="卸载模块"></a>卸载模块</h4><p>直接从安装目录下，把安装模块的<strong>目录</strong>删除就可以<br>先找到安装目录<code>包名.__file__</code><br>然后去对应目录删除相应文件</p>
<h4 id="pip-安装第三方模块"><a href="#pip-安装第三方模块" class="headerlink" title="pip 安装第三方模块"></a><code>pip</code> 安装第三方模块</h4><p>第三方模块通常是指由知名的第三方团队开发的并被程序员广泛使用的<code>python</code>包/模块，例如<code>pygame</code></p>
<p><code>pip</code>是一个现代的通用的<code>Python</code>包管理工具<br>提供了对<code>Python</code>包的查找、下载、安装、卸载等功能</p>
<p>安装卸载命令如下<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 将模块安装到Python2.x环境中</span><br><span class="line">sudo pip install pygame</span><br><span class="line">sudo pip uninstall pygame</span><br><span class="line"># 将模块安装到Python3.x环境</span><br><span class="line">sudo pip3 install pygame</span><br><span class="line">sudo pip3 uninstall pygame</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python单例与异常</title>
    <url>/2019/10/23/Python%E5%8D%95%E4%BE%8B%E4%B8%8E%E5%BC%82%E5%B8%B8/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://www.bilibili.com/video/av68350017/?p=445" target="_blank" rel="noopener">学习地址</a></center></h3><h3 id="单例"><a href="#单例" class="headerlink" title="单例"></a>单例</h3><p>目标</p>
<ol>
<li>单例设计模式</li>
<li><code>__new__</code>方法</li>
<li>Python中的单例</li>
</ol>
<hr>
<h4 id="单例设计模式"><a href="#单例设计模式" class="headerlink" title="单例设计模式"></a>单例设计模式</h4><ol>
<li><p>设计模式  </p>
<ul>
<li>设计模式是<strong>前人工作的总结和提炼</strong>，通常，被人们广泛流传的设计模式都是针对某一特定问题的成熟解决方案</li>
<li>使用设计模式是为了可重用代码、让代码更容易地被他人理解、保证代码地可靠性</li>
</ul>
</li>
<li><p>单例设计模式</p>
<ul>
<li>目的 – 让类创建的对象，在系统中只有<strong>唯一的一个实例</strong></li>
<li>每一次执行<code>类名()</code>返回的对象，<strong>内存地址是相同的</strong></li>
</ul>
</li>
<li><p>单例设计模式的应用场景</p>
<ul>
<li>音乐播放对象</li>
<li>回收站对象</li>
<li>打印机对象</li>
<li>都只有唯一的一个存在</li>
</ul>
</li>
</ol>
<h4 id="new-方法"><a href="#new-方法" class="headerlink" title="__new__方法"></a><code>__new__</code>方法</h4><p> 使用<code>类名()</code>创建对象时，<code>Python</code>解释器会调用<code>__new__</code>方法为对象分配空间 <code>__new__</code>是一个由<code>object</code>基类提供的内置的静态方法，主要作用有两个  </p>
<ul>
<li>在内存中为对象<strong>分配空间</strong></li>
<li>返回对象的引用</li>
</ul>
<p><code>Python</code>的解释器获得对象的引用后，将引用作为第一个参数，传递给<code>__init__</code>方法</p>
<blockquote>
<p>了解<code>__new__</code>方法就可以进行改造了<br>重写<code>__new__</code>方法的代码十分固定！</p>
</blockquote>
<p>重写<code>__new__</code>方法一定要<code>return super().__new__(cls)</code><br>否则，<code>Python</code>解释器得不到分配了空间的对象引用，就不会调用对象的初始化方法<br>注意：<code>__new__</code>是一个静态方法，在调用时需要主动传递参数<code>cls</code><br>示例<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class MusicPlayer(object):</span><br><span class="line">    def __new__(cls, *args, **kwargs):</span><br><span class="line">        # auto call</span><br><span class="line">        print(&apos;Create objects, allocate memory&apos;)</span><br><span class="line">        # 为对象分配空间并返回引用</span><br><span class="line">        # return super().__new__(cls)</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        print(&apos;player init....&apos;)</span><br><span class="line"></span><br><span class="line">player=MusicPlayer()</span><br><span class="line">print(player)</span><br><span class="line"></span><br><span class="line">- - - - - - -</span><br><span class="line">Create objects, allocate memory</span><br><span class="line">None</span><br></pre></td></tr></table></figure></p>
<h4 id="Python中的单例"><a href="#Python中的单例" class="headerlink" title="Python中的单例"></a>Python中的单例</h4><h5 id="单例-1"><a href="#单例-1" class="headerlink" title="单例"></a>单例</h5><p>让类创建的对象，在系统中只有唯一的一个实例，即每次创建对象所返回的引用是一样的  </p>
<ol>
<li>定义一个类属性，初始值是<code>None</code>，用于记录单例对象的引用</li>
<li>重写<code>__new__</code>方法</li>
<li>如果类属性<code>is None</code>，调用父类方法分配空间，记录引用</li>
<li>否则，返回这个类属性代表的引用</li>
</ol>
<h5 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class MusicPlayer(object):</span><br><span class="line">    instance=None</span><br><span class="line">    def __new__(cls, *args, **kwargs):</span><br><span class="line">        # auto call</span><br><span class="line">        # print(&apos;Create objects, allocate memory&apos;)</span><br><span class="line">        if MusicPlayer.instance is None: MusicPlayer.instance=super().__new__(cls)</span><br><span class="line">        return MusicPlayer.instance</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        print(&apos;player init....&apos;)</span><br><span class="line"></span><br><span class="line">player1=MusicPlayer()</span><br><span class="line">print(player1)</span><br><span class="line">player2=MusicPlayer()</span><br><span class="line">print(player2)</span><br><span class="line"></span><br><span class="line">- - - - - - - - - - - -</span><br><span class="line">player init....</span><br><span class="line">&lt;__main__.MusicPlayer object at 0x7f2f98dd69b0&gt;</span><br><span class="line">player init....</span><br><span class="line">&lt;__main__.MusicPlayer object at 0x7f2f98dd69b0&gt;</span><br></pre></td></tr></table></figure>
<p>可以看到两次创建对象的内存地址是一样的</p>
<h5 id="代码升级"><a href="#代码升级" class="headerlink" title="代码升级"></a>代码升级</h5><p>需求</p>
<ul>
<li>使得单例对象在被创建时，只执行一次初始化工作(音乐播放器换歌不需要重启)  </li>
</ul>
<p>解决</p>
<ul>
<li>定义一个类属性<code>init_flag</code>初始值为<code>False</code>标记是否执行过初始化动作</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class MusicPlayer(object):</span><br><span class="line">    instance=None</span><br><span class="line">    init_flag=False</span><br><span class="line">    def __new__(cls, *args, **kwargs):</span><br><span class="line">        # auto call</span><br><span class="line">        # print(&apos;Create objects, allocate memory&apos;)</span><br><span class="line">        if MusicPlayer.instance is None: MusicPlayer.instance=super().__new__(cls)</span><br><span class="line">        return MusicPlayer.instance</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        if MusicPlayer.init_flag == True:return</span><br><span class="line">        print(&apos;player init....&apos;)</span><br><span class="line">        MusicPlayer.init_flag=True</span><br><span class="line"></span><br><span class="line">player1=MusicPlayer()</span><br><span class="line">print(player1)</span><br><span class="line">player2=MusicPlayer()</span><br><span class="line">print(player2)</span><br><span class="line"></span><br><span class="line">- - - - - - - - - - - - </span><br><span class="line">player init....</span><br><span class="line">&lt;__main__.MusicPlayer object at 0x7f5e4abc4a20&gt;</span><br><span class="line">&lt;__main__.MusicPlayer object at 0x7f5e4abc4a20&gt;</span><br></pre></td></tr></table></figure>
<h3 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h3><p>目标</p>
<ol>
<li>概念</li>
<li>捕获异常</li>
<li>异常的传递</li>
<li>自定义异常</li>
</ol>
<hr>
<h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p>程序在运行时，如果<code>Python</code>解释器遇到一个错误，会停止程序的执行，并且提示一些错误信息，这就是异常<br><strong>程序停止执行并且提示错误信息</strong>这个动作，我们通常称之为：<strong>抛出(raise)异常</strong></p>
<h4 id="异常捕获"><a href="#异常捕获" class="headerlink" title="异常捕获"></a>异常捕获</h4><p>程序开发时，很难将所有的特殊情况都处理的面面俱到，通过异常捕获可以针对突发事件做集中的处理，从而保证程序的稳定性和健壮性</p>
<h5 id="简单的捕获异常语法"><a href="#简单的捕获异常语法" class="headerlink" title="简单的捕获异常语法"></a>简单的捕获异常语法</h5><p>在程序开发中，如果对某些代码的执行不能确定是否正确，可以增加<code>try:</code>来捕获异常<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">try:</span><br><span class="line">    # try to do </span><br><span class="line">    pass</span><br><span class="line">except:</span><br><span class="line">    # deal with error</span><br><span class="line">    pass</span><br></pre></td></tr></table></figure></p>
<p>比如程序设计中常见的多组输入<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">while True:</span><br><span class="line">    try:</span><br><span class="line">        a,b=int(input())</span><br><span class="line">        print(a+b)</span><br><span class="line">    except:break</span><br></pre></td></tr></table></figure></p>
<h5 id="根据错误类型捕获异常"><a href="#根据错误类型捕获异常" class="headerlink" title="根据错误类型捕获异常"></a>根据错误类型捕获异常</h5><p>在程序执行时，可能会遇到不同类型的异常，并且需要针对不同类型的异常，做出不同的响应，这个时候，就需要捕获错误类型了<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">try:</span><br><span class="line">    # 尝试执行的代码</span><br><span class="line">	pass</span><br><span class="line">except error_1:</span><br><span class="line">    # 针对错误类型1，对应的代码处理，下面同理</span><br><span class="line">    pass</span><br><span class="line">except (error_2,error_3):</span><br><span class="line">    pass</span><br><span class="line">except Exception as result: # 捕获未知错误</span><br><span class="line">    print(&apos;unknown mistake: %s&apos; %result)</span><br></pre></td></tr></table></figure></p>
<p>当Python解释器抛出异常时，最后一行错误信息的第一个单词就是错误类型，即代码中的<code>error</code>，如<code>ValueError</code>、<code>ZeroDivisionError</code>等等</p>
<h5 id="异常捕获完整语法"><a href="#异常捕获完整语法" class="headerlink" title="异常捕获完整语法"></a>异常捕获完整语法</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">try:</span><br><span class="line">    # 尝试执行</span><br><span class="line">    pass</span><br><span class="line">except error_1:</span><br><span class="line">    # 错误类型1，对应的处理</span><br><span class="line">    pass</span><br><span class="line">except (error_2,error_3):</span><br><span class="line">    pass</span><br><span class="line">except Exception as result:</span><br><span class="line">    print(&apos;unknown mistake: %s&apos; result)</span><br><span class="line">else:</span><br><span class="line">    # 没有异常会执行的代码</span><br><span class="line">    pass</span><br><span class="line">finally:</span><br><span class="line">    # 无论是否有异常，都会执行的代码</span><br><span class="line">    print(&quot;this must be execution&quot;)</span><br></pre></td></tr></table></figure>
<h4 id="异常的传递性"><a href="#异常的传递性" class="headerlink" title="异常的传递性"></a>异常的传递性</h4><p>异常的传递 – 当函数/方法执行出现异常，会将异常传递给函数/方法的调用一方<br>如果传递到主程序，仍然没有异常处理，程序才会被终止</p>
<p>提示：</p>
<ol>
<li>在开发中，可以在主函数中增加异常捕获</li>
<li>主函数中调用的其他函数，出现异常都会传递到主函数的异常捕获中</li>
<li>只需在主函数中进行异常捕获，保证代码整洁性</li>
</ol>
<h4 id="主动抛出异常"><a href="#主动抛出异常" class="headerlink" title="主动抛出异常"></a>主动抛出异常</h4><p>在开发中，除了代码执行出错<code>Python</code>解释器会抛出异常外<br>还可以根据应用程序特有的需求业务主动抛出异常</p>
<h5 id="抛出异常"><a href="#抛出异常" class="headerlink" title="抛出异常"></a>抛出异常</h5><p>Python中提供了一个<code>Exception</code>异常类<br>在开发时，如果满足特定业务需求时，希望抛出异常，可以：</p>
<ol>
<li>创建一个<code>Exception</code>对象</li>
<li>使用<code>raise</code>关键字抛出异常对象</li>
</ol>
<h5 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h5><p>需求</p>
<ol>
<li>定义<code>input_password</code>函数，提示用户输入密码</li>
<li>如果用户输入长度&lt;8，抛出异常</li>
<li>如果用户输入长度&gt;=8，返回输入的密码</li>
</ol>
<p>实现<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def input_password():</span><br><span class="line">    password=input(&apos;please input password: &apos;)</span><br><span class="line">    if len(password)&gt;=8:return password</span><br><span class="line"></span><br><span class="line">    print(&apos;raise an exception&apos;)</span><br><span class="line">    exp=Exception(&apos;the length of password is not enough&apos;)</span><br><span class="line">    raise exp</span><br><span class="line"></span><br><span class="line">try:</span><br><span class="line">    print(input_password())</span><br><span class="line">except Exception as result:</span><br><span class="line">    print(result)</span><br><span class="line"></span><br><span class="line">- - - - - - - - - - - - -</span><br><span class="line">please input password: 12345</span><br><span class="line">raise an exception</span><br><span class="line">the length of password is not enough</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python多态与类属性、类方法</title>
    <url>/2019/10/22/Python%E5%A4%9A%E6%80%81%E4%B8%8E%E7%B1%BB%E5%B1%9E%E6%80%A7/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://www.bilibili.com/video/av68350017/?p=432" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="多态"><a href="#多态" class="headerlink" title="多态"></a>多态</h4><p>不同的子类调用相同的父类方法，产生不同的执行结果，以<strong>继承和重写父类方法为前提</strong>  </p>
<h5 id="案例演练"><a href="#案例演练" class="headerlink" title="案例演练"></a>案例演练</h5><p>需求：</p>
<ol>
<li>在<code>Dog</code>类中封装方法<code>game</code>  <ul>
<li>普通狗只是简单的玩耍</li>
</ul>
</li>
<li>定义<code>XiaoTianDog</code>继承自<code>Dog</code>，并且重写<code>game</code>方法  <ul>
<li>小天犬可以在天上玩耍</li>
</ul>
</li>
<li>定义<code>Person</code>类，并且封装一个和狗玩的方法  <ul>
<li>在方法内部，直接让狗对象调用<code>game</code>方法</li>
</ul>
</li>
</ol>
<h5 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h5> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> class Dog:</span><br><span class="line">    def __init__(self,name):</span><br><span class="line">        self.name=name</span><br><span class="line">    def game(self):</span><br><span class="line">        print(&apos;playing on the ground&apos;)</span><br><span class="line"></span><br><span class="line">class XiaoTianDog(Dog):</span><br><span class="line">    def game(self):</span><br><span class="line">        print(&apos;%s playing in the sky&apos; %self.name)</span><br><span class="line"></span><br><span class="line">class Person:</span><br><span class="line">    def __init__(self,name):</span><br><span class="line">        self.name=name</span><br><span class="line">    def play_with_dog(self,dog):</span><br><span class="line">        print(&apos;%s play with %s&apos; %(self.name,dog.name))</span><br><span class="line">        dog.game()</span><br><span class="line"></span><br><span class="line"># 1. 创建狗对象</span><br><span class="line"># wangcai=Dog(&apos;wangcai&apos;)</span><br><span class="line">wangcai=XiaoTianDog(&apos;wangcai&apos;)</span><br><span class="line"># 2. 创建小明对象</span><br><span class="line">xiaoming=Person(&apos;小明&apos;)</span><br><span class="line"># 3. 让小明调用和狗玩的方法</span><br><span class="line">xiaoming.play_with_dog(wangcai)</span><br></pre></td></tr></table></figure>
<h4 id="类属性与实例属性"><a href="#类属性与实例属性" class="headerlink" title="类属性与实例属性"></a>类属性与实例属性</h4><h5 id="类的结构"><a href="#类的结构" class="headerlink" title="类的结构"></a>类的结构</h5><ol>
<li>实例  <ul>
<li>使用面向对象开发时，第一步时设计类</li>
<li>使用<code>类名()</code>创建对象，创建对象的动作分两步：分配内存； <code>__init__(self)</code>初始化</li>
<li>创建完成，内存中就有了一个对象，实实在在的存在 - 实例</li>
</ul>
</li>
</ol>
<h5 id="类是一个特殊的对象"><a href="#类是一个特殊的对象" class="headerlink" title="类是一个特殊的对象"></a>类是一个特殊的对象</h5><blockquote>
<blockquote>
<p><code>Python</code>中一切皆对象 </p>
<ul>
<li><code>class AAA:</code>定义的类属于类对象</li>
<li><code>object=AAA()</code>属于实例对象</li>
</ul>
</blockquote>
</blockquote>
<p>程序运行时，类对象在内存中只有一份，使用一个类可以创建出很多个对象实例<br>除了封装实例的属性和方法外，类对象还可以拥有自己的属性和方法<br>通过<code>类名.</code>的方式可以访问类的属性调用类的方法</p>
<h5 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h5><p>类属性就是给类对象中定义的属性<br>通常用来记录与这个类相关的特征<br>类属性不会用于记录具体对象的特征  </p>
<p>示例需求：</p>
<ol>
<li>定义一个工具类</li>
<li>每个工具类都有自己的<code>name</code></li>
<li>需求 - - 知道这个类，创建了多少个工具对象<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Tool(object):</span><br><span class="line">    # 使用赋值语句定义类属性</span><br><span class="line">    count=0</span><br><span class="line">    def __init__(self,name):</span><br><span class="line">        self.name=name</span><br><span class="line">        # 使用类名访问类属性</span><br><span class="line">        Tool.count+=1</span><br><span class="line"></span><br><span class="line"># 1. 创建工具对象</span><br><span class="line">too1=Tool(&apos;斧头&apos;)</span><br><span class="line">too2=Tool(&apos;锤子&apos;)</span><br><span class="line"># 2. 输出工具对象的数量</span><br><span class="line">print(Tool.count)</span><br><span class="line"></span><br><span class="line">- - - - - - - </span><br><span class="line">2</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h5 id="属性的向上查找机制"><a href="#属性的向上查找机制" class="headerlink" title="属性的向上查找机制"></a>属性的向上查找机制</h5><p>在<code>Python</code>中属性的获取存在一个<strong>向上查找机制</strong><br>即，访问对象属性时首先在指定的对象内部查找，没有找到则向上寻找类的属性，这就是继承可以实现代码重用。<br>那么 ，访问对象属性有两种方法</p>
<ol>
<li><code>类名.类属性</code></li>
<li><code>对象名.类属性</code>(不推荐)</li>
</ol>
<h5 id="使用对象名-类属性进行赋值"><a href="#使用对象名-类属性进行赋值" class="headerlink" title="使用对象名+类属性进行赋值"></a>使用对象名+类属性进行赋值</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Tool(object):</span><br><span class="line">    # 使用赋值语句定义类属性</span><br><span class="line">    count=0</span><br><span class="line">    def __init__(self,name):</span><br><span class="line">        self.name=name</span><br><span class="line">        # 使用类名访问类属性</span><br><span class="line">        Tool.count+=1</span><br><span class="line"></span><br><span class="line"># 1. 创建工具对象</span><br><span class="line">too1=Tool(&apos;斧头&apos;)</span><br><span class="line">too2=Tool(&apos;锤子&apos;)</span><br><span class="line">too1.count+=1 # 这条赋值语句并不是访问类属性，而是给对象添加属性</span><br><span class="line"># 2. 输出工具对象的数量</span><br><span class="line">print(Tool.count)</span><br><span class="line"></span><br><span class="line">- - - - - - - </span><br><span class="line">2</span><br></pre></td></tr></table></figure>
<p>使用<code>对象.类属性=值</code>赋值语句只会给实例对象添加一个属性，而不会影响到类属性的值。</p>
<h4 id="类方法和静态方法"><a href="#类方法和静态方法" class="headerlink" title="类方法和静态方法"></a>类方法和静态方法</h4><ol>
<li>类属性就是针对类对象定义的属性  <ul>
<li>使用赋值语句在class关键字下方可以定义类属性</li>
<li>类属性用于记录与这个类相关的特征</li>
</ul>
</li>
<li><p>类方法就是针对类对象定义的方法  </p>
<ul>
<li>在类方法内部可以直接调用类属性或者带哦用其他的类方法</li>
</ul>
</li>
<li><p>语法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@classmethod</span><br><span class="line">def function_name(cls):</span><br><span class="line">    pass</span><br></pre></td></tr></table></figure>
<ul>
<li>类方法需要用<strong>修饰器</strong><code>classmethod</code>来标识，告诉解释器这是一个类方法</li>
<li>类方法的第一个参数应该是<code>cls</code>，由哪一个类调用的方法，方法内的<code>cls</code>就是那一个类的引用，和实例方法的第一个参数<code>self</code>类似</li>
<li>通过<code>类名.</code>调用类方法，调用方法时，不需要传递<code>cls</code>参数</li>
<li>在方法内部，可以通过<code>cls.</code>访问类的属性，也可以通过<code>cls.</code>调用其他的类方法</li>
</ul>
</li>
</ol>
<h5 id="示例需求"><a href="#示例需求" class="headerlink" title="示例需求"></a>示例需求</h5><ol>
<li>定义一个工具类</li>
<li>每个工具都有自己的<code>name</code></li>
<li>需求–在类封装一个<code>show_tool_count</code>的类方法，输出使用当前类所创建的实例对象的数量</li>
</ol>
<h5 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Tool(object):</span><br><span class="line">    count=0</span><br><span class="line">    @classmethod</span><br><span class="line">    def show_tool_count(cls):</span><br><span class="line">        print(&apos;the number of tools is %d&apos; %cls.count)</span><br><span class="line">    def __init__(self,name):</span><br><span class="line">        self.name=name</span><br><span class="line">        Tool.count+=1</span><br><span class="line"></span><br><span class="line">too1=Tool(&apos;锤子&apos;)</span><br><span class="line">too2=Tool(&apos;斧头&apos;)</span><br><span class="line">Tool.show_tool_count()</span><br><span class="line"></span><br><span class="line">- - - - - - - - - - - -</span><br><span class="line">the number of tools is 2</span><br></pre></td></tr></table></figure>
<h4 id="静态方法"><a href="#静态方法" class="headerlink" title="静态方法"></a>静态方法</h4><p>在开发时，如果需要在类中封装一个方法，这个方法  </p>
<ul>
<li>即不需要访问实例属性或者调用实例方法</li>
<li>也不需要访问类属性或者调用类方法</li>
</ul>
<p>那么，就可以把这个方法封装成一个静态方法  </p>
<h5 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@staticmethod</span><br><span class="line">def function_name():</span><br><span class="line">    pass</span><br></pre></td></tr></table></figure>
<ul>
<li>静态方法需要使用修饰器<code>staticmethod</code>来标识，告诉解释器这是一个静态方法</li>
<li>通过<code>类名.</code>调用静态方法</li>
</ul>
<h4 id="综合演练"><a href="#综合演练" class="headerlink" title="综合演练"></a>综合演练</h4><h5 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h5><ol>
<li>设计一个<code>Game</code>类</li>
<li><p>属性  </p>
<ul>
<li>定义一个<strong>类属性</strong><code>top_score</code>记录游戏的历史最高分</li>
<li>定义一个<strong>实例属性</strong><code>player_game</code>记录当前游戏的玩家姓名</li>
</ul>
</li>
<li><p>方法  </p>
<ul>
<li><strong>静态方法</strong><code>show_help</code>显示游戏帮助信息</li>
<li><strong>类方法</strong><code>show_top_score</code>显示历史最高分</li>
<li><strong>实例方法</strong><code>start_game</code>开始当前玩家的游戏</li>
</ul>
</li>
<li><p>主程序步骤  </p>
<ul>
<li>查看帮助信息</li>
<li>查看历史最高分</li>
<li>创建游戏对象，开始游戏</li>
</ul>
</li>
</ol>
<h5 id="实现-2"><a href="#实现-2" class="headerlink" title="实现"></a>实现</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Game(object):</span><br><span class="line">    # 类属性</span><br><span class="line">    top_score=0</span><br><span class="line"></span><br><span class="line">    def __init__(self,player_name):</span><br><span class="line">        self.name=player_name # 实例属性</span><br><span class="line"></span><br><span class="line">    # 静态方法，不需要访问其他信息</span><br><span class="line">    @staticmethod</span><br><span class="line">    def show_help():</span><br><span class="line">        print(&apos;---- help ---&apos;)</span><br><span class="line"></span><br><span class="line">    # 类方法，使用cls访问类属性</span><br><span class="line">    @classmethod</span><br><span class="line">    def show_top_score(cls):</span><br><span class="line">        print(&apos;The highest score is %d&apos; %cls.top_score)</span><br><span class="line"></span><br><span class="line">    # 实例属性</span><br><span class="line">    def start_game(self):</span><br><span class="line">        print(&apos;1 2 3 , run&apos;)</span><br><span class="line"></span><br><span class="line">Game.show_help()</span><br><span class="line">Game.show_top_score()</span><br><span class="line">player=Game(&apos;小明&apos;)</span><br><span class="line">player.start_game()</span><br><span class="line"></span><br><span class="line">- - - - - - - - - - - - - - -</span><br><span class="line">---- help ---</span><br><span class="line">The highest score is 0</span><br><span class="line">1 2 3 , run</span><br></pre></td></tr></table></figure>
<h5 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h5><ol>
<li><p>实例方法：方法内部需要访问实例属性  </p>
<ul>
<li>实例方法内部可以使用<code>类名.</code>访问类属性</li>
</ul>
</li>
<li><p>类方法： 方法内部只需要访问类属性，使用<code>cls.</code>访问类属性</p>
</li>
<li>静态方法： 方法内部，不需要访问实例属性和类属性</li>
</ol>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python继承与属性</title>
    <url>/2019/10/21/Python%E5%8D%95%E7%BB%A7%E6%89%BF%E4%B8%8E%E6%96%B9%E6%B3%95%E9%87%8D%E5%86%99/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://www.bilibili.com/video/av68350017/?p=418" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="单继承"><a href="#单继承" class="headerlink" title="单继承"></a>单继承</h4><p>目标：</p>
<ul>
<li>单继承</li>
<li>多继承</li>
</ul>
<h5 id="面向对象三大特性"><a href="#面向对象三大特性" class="headerlink" title="面向对象三大特性"></a>面向对象三大特性</h5><ol>
<li>封装：根据职责将属性和方法封装到一个抽象的类中</li>
<li>多态：不同的对象调用相同的方法，产生不同的执行结果，增加代码的灵活度</li>
<li>继承：实现代码的<strong>重用</strong>，相同的代码不需要重复的编写</li>
</ol>
<h5 id="单继承-1"><a href="#单继承-1" class="headerlink" title="单继承"></a>单继承</h5><ol>
<li>继承的概念：<strong>子类</strong>拥有<strong>父类</strong>所 有方法和属性</li>
<li><p>继承的语法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class class_name(father_name):</span><br><span class="line">  pass</span><br></pre></td></tr></table></figure>
<ul>
<li>子类继承自父类，拥有父类所有的方法和属性</li>
<li>子类中应该根据职责，封装子类特有的属性和方法</li>
<li>子类也称<strong>派生类</strong>，父类也称<strong>基类</strong></li>
</ul>
</li>
<li><p>继承的传递性<br>简单来说，就是<strong>派生类的派生类</strong>可以调用基类的方法和属性。<br>但是，注意这个传递的<del>唯一性</del>，只能单线继承，类似树状结构。</p>
</li>
</ol>
<h4 id="继承示例"><a href="#继承示例" class="headerlink" title="继承示例"></a>继承示例</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class A:</span><br><span class="line">    def __init__(self,name):</span><br><span class="line">        self.name=name</span><br><span class="line"></span><br><span class="line">class B(A):</span><br><span class="line">    def play(self):</span><br><span class="line">        print(&apos;%s is playing&apos; %self.name)</span><br><span class="line"></span><br><span class="line">b=B(&apos;abc&apos;)</span><br><span class="line">b.play()</span><br><span class="line"></span><br><span class="line">- - - - - - - - - - - - </span><br><span class="line">abc is playing</span><br></pre></td></tr></table></figure>
<h4 id="方法的重写"><a href="#方法的重写" class="headerlink" title="方法的重写"></a>方法的重写</h4><p>当父类的方法不能满足子类的需求时，可以对方法进行<strong>重写(override)</strong>。<br>重写父类方法有两种情况：</p>
<ol>
<li><strong>覆盖</strong>父类的方法：开发中如果父类的方法实现和子类的方法实现完全不同的情况，在子类中定义和父类同名的方法并实现</li>
<li>对父类的方法进行<strong>扩展</strong>：父类中封装的方法时子类要封装的方法的一部分，在子类中重写父类的方法，在需要的位置使用<code>super().父类方法</code>调用父类方法执行，方法中其他地方针对子类需求编写</li>
</ol>
<p>关于<code>super</code>：<br>    Python中<code>super</code>是一个特殊的类，<code>super()</code>就是使用<code>super</code>类创建出来的对象，最常使用的场景就是在重写父类方法时，调用在父类中封装的方法实现。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Animal:</span><br><span class="line">    def eating(self):</span><br><span class="line">        print(&apos;eating food&apos;)</span><br><span class="line">    def sleeping(self):</span><br><span class="line">        print(&quot;sleeping&quot;)</span><br><span class="line"></span><br><span class="line">class Dog(Animal):</span><br><span class="line">    def bark(self):</span><br><span class="line">        print(&apos;wangwangwang&apos;)</span><br><span class="line"></span><br><span class="line">class XiapTianQuan(Dog):</span><br><span class="line">    def fly(self):</span><br><span class="line">        print(&apos;flying&apos;)</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    # 覆盖父类的方法，方法名与父类相同</span><br><span class="line">    def bark(self):</span><br><span class="line">        print(&apos;bark like a god&apos;)</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line">    # 扩展父类的方法</span><br><span class="line">    def bark(self):</span><br><span class="line">        print(&apos;bark like a god,&apos;,end=&quot; &quot;)</span><br><span class="line">        super().bark()</span><br><span class="line"></span><br><span class="line">xtq=XiapTianQuan()</span><br><span class="line">xtq.bark()</span><br><span class="line"></span><br><span class="line">- - - - - - - - - - - - </span><br><span class="line">bark like a god, wangwangwang</span><br></pre></td></tr></table></figure>
<h5 id="使用父类名调用父类方法"><a href="#使用父类名调用父类方法" class="headerlink" title="使用父类名调用父类方法"></a>使用父类名调用父类方法</h5><p>早期的Python版本如果需要调用父类的方法还可以使用：<code>父类名.方法(self)</code>，但不推荐，如果使用当前子类名会形成递归调用死循环</p>
<h4 id="私有方法和属性"><a href="#私有方法和属性" class="headerlink" title="私有方法和属性"></a>私有方法和属性</h4><h5 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h5><p>对象所不希望公开的属性和方法，不希望外部访问，就是不对外公开，外界及子类都不能直接访问，通常用于做一些内部的事情  </p>
<ol>
<li>子类不能在自己的方法内部直接访问父类的私有属性或者方法</li>
<li>子类对象可以通过父类的公有方法<strong>间接访问</strong>到私有属性或者私有方法</li>
</ol>
<h5 id="定义方式"><a href="#定义方式" class="headerlink" title="定义方式"></a>定义方式</h5><p>在定义属性或方法时，在属性名或者方法名前增加两个下划线<code>__</code>即可，定义的就是私有属性或方法。<br>这样，外界<strong>无法直接访问</strong>，但可以通过对象内部方法访问。</p>
<h5 id="伪私有"><a href="#伪私有" class="headerlink" title="伪私有"></a>伪私有</h5><p>在Python中，并没有真正意义的私有<br>在给属性、方法命名时，实际时对名称做了一些特殊处理，使得外界无法访问到<br>处理方式：在名称前面加上<code>_类名</code>，即<code>_类名__名称</code><br>这种方法虽然也可以访问到，但在开发中不要使用这种方式，遵守应有的规则即可。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class A:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.num1=10</span><br><span class="line">        self.__num2=20</span><br><span class="line">    def __test(self):</span><br><span class="line">        print(&apos;私有方法： num1 = %d num2 = %d&apos; %(self.num1,self.__num2))</span><br><span class="line">    def test(self):</span><br><span class="line">        print(&apos;this is public function!&apos;)</span><br><span class="line">        self.__test()</span><br><span class="line"></span><br><span class="line">class B(A):</span><br><span class="line">    def demo(self):</span><br><span class="line">        print(&quot;访问父类公开属性： num1 = %d &quot; %self.num1)</span><br><span class="line">        self.test()</span><br><span class="line">        print(&apos;外挂访问私有属性(不推荐)&apos;)</span><br><span class="line">        print(&apos;私有属性： num2 = %d&apos; %self._A__num2)</span><br><span class="line"></span><br><span class="line">b=B()</span><br><span class="line">b.demo()</span><br><span class="line"></span><br><span class="line">- - - - - - - - - - - - -</span><br><span class="line">访问父类公开属性： num1 = 10 </span><br><span class="line">this is public function!</span><br><span class="line">私有方法： num1 = 10 num2 = 20</span><br><span class="line">外挂访问私有属性(不推荐)</span><br><span class="line">私有属性： num2 = 20</span><br></pre></td></tr></table></figure>
<h4 id="多继承"><a href="#多继承" class="headerlink" title="多继承"></a>多继承</h4><h5 id="概念及语法"><a href="#概念及语法" class="headerlink" title="概念及语法"></a>概念及语法</h5><p>子类可以拥有多个父类、并且具有所有父类的属性和方法<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class class_name(father_name1,father_name2,...):</span><br><span class="line">    pass</span><br></pre></td></tr></table></figure></p>
<p>示例：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class A:</span><br><span class="line">    def demo1(self):</span><br><span class="line">        print(&apos;you are in class A&apos;)</span><br><span class="line"></span><br><span class="line">class B:</span><br><span class="line">    def demo2(self):</span><br><span class="line">        print(&apos;you are in class B&apos;)</span><br><span class="line"></span><br><span class="line">class C(A,B):</span><br><span class="line">    def demo3(self):</span><br><span class="line">        self.demo1()</span><br><span class="line">        self.demo2()</span><br><span class="line"></span><br><span class="line">c=C()</span><br><span class="line">c.demo3()</span><br><span class="line"></span><br><span class="line">- - - - - - - - - - - -</span><br><span class="line">you are in class A</span><br><span class="line">you are in class B</span><br></pre></td></tr></table></figure></p>
<h5 id="同名方法与属性"><a href="#同名方法与属性" class="headerlink" title="同名方法与属性"></a>同名方法与属性</h5><p>如果继承的不同父类中存在同名的方法或者属性，子类对象在调用时会按照继承列表的先后顺序执行一次。<br>开发时，应尽量避免这种容易产生混淆的情况，或者放弃使用多继承</p>
<h5 id="解决多继承重名问题的MRO-方法搜索顺序"><a href="#解决多继承重名问题的MRO-方法搜索顺序" class="headerlink" title="解决多继承重名问题的MRO(方法搜索顺序)"></a>解决多继承重名问题的MRO(方法搜索顺序)</h5><ul>
<li>Python中针对类提供了一个内置属性<code>__mro__</code>可以查看方法搜索顺序</li>
<li>MRO即<code>method_resolution_order</code>，主要用于在多继承时判断方法、属性的调用路径</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(C.__mro__)</span><br><span class="line"></span><br><span class="line">- - - - - - </span><br><span class="line">(&lt;class &apos;__main__.C&apos;&gt;, &lt;class &apos;__main__.A&apos;&gt;, &lt;class &apos;__main__.B&apos;&gt;, &lt;class &apos;object&apos;&gt;)</span><br></pre></td></tr></table></figure>
<h5 id="新式类和旧式类"><a href="#新式类和旧式类" class="headerlink" title="新式类和旧式类"></a>新式类和旧式类</h5><p><code>object</code>时Python为所有对象提供的基类，提供有一些内置的属性和方法，可以使用<code>dir</code>函数查看，Python3默认新式类，如果没有指定父类默认<code>object</code></p>
<ul>
<li>新式类：以<code>object</code>为基类的类，推荐使用</li>
<li>旧式类：不以<code>object</code>为基类的类，不推荐使用</li>
</ul>
<p>这个会影响方法搜索顺序，建议新式类。</p>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python面向对象封装案例</title>
    <url>/2019/10/20/Python%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E5%B0%81%E8%A3%85%E6%A1%88%E4%BE%8B/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://www.bilibili.com/video/av68350017/?p=405" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h4><ul>
<li>封装</li>
<li>小明爱跑步</li>
<li>存放家具</li>
</ul>
<hr>
<h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><ul>
<li>小明爱跑步</li>
</ul>
<h5 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h5><ol>
<li>小明<strong>体重</strong>75.0 Kg</li>
<li>每次<strong>跑步</strong>减肥0.5公斤</li>
<li>每次<strong>吃东西</strong>增加1公斤<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; Person  </span><br><span class="line">&gt;  -----------</span><br><span class="line">&gt; name，weight</span><br><span class="line">&gt; ------------</span><br><span class="line">&gt; `__init__`(self,name,weight):  </span><br><span class="line">&gt; `__str__`(self):  </span><br><span class="line">&gt; running(self):  </span><br><span class="line">&gt; eating(self):</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h5 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Person:</span><br><span class="line">    def __init__(self,name,weight):</span><br><span class="line">        # self.属性=形参</span><br><span class="line">        self.name=name</span><br><span class="line">        self.weight=weight</span><br><span class="line">    def __str__(self):</span><br><span class="line">        return &apos;my name is %s&apos; %self.name</span><br><span class="line">    def eating(self):</span><br><span class="line">        self.weight+=1</span><br><span class="line">    def running(self):</span><br><span class="line">        self.weight-=0.5</span><br><span class="line"></span><br><span class="line">xiaoming=Person(&apos;小明&apos;,75.0)</span><br><span class="line">print(xiaoming)</span><br><span class="line"></span><br><span class="line">- - - - - - - - - - - </span><br><span class="line">my name is 小明</span><br></pre></td></tr></table></figure>
<h4 id="案例扩展"><a href="#案例扩展" class="headerlink" title="案例扩展"></a>案例扩展</h4><p>同一个类创建的多个对象之间，属性互不干扰</p>
<h5 id="需求-1"><a href="#需求-1" class="headerlink" title="需求"></a>需求</h5><ol>
<li>小明和小美都爱跑步</li>
<li>小明75.0Kg，小美45.0Kg</li>
<li>running减少0.5Kg</li>
<li>eating增加1Kg</li>
</ol>
<h5 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Person:</span><br><span class="line">    def __init__(self,name,weight):</span><br><span class="line">        # self.属性=形参</span><br><span class="line">        self.name=name</span><br><span class="line">        self.weight=weight</span><br><span class="line">    def __str__(self):</span><br><span class="line">        return &apos;my name is %s,my weight is %.1f&apos; %(self.name,self.weight)</span><br><span class="line">    def eating(self):</span><br><span class="line">        self.weight+=1</span><br><span class="line">    def running(self):</span><br><span class="line">        self.weight-=0.5</span><br><span class="line"></span><br><span class="line">xiaoming=Person(&apos;小明&apos;,75.0)</span><br><span class="line">xiaomei=Person(&apos;小美&apos;,45.0)</span><br><span class="line">xiaoming.eating()</span><br><span class="line">xiaoming.running()</span><br><span class="line">print(xiaoming)</span><br><span class="line">print(xiaomei)</span><br><span class="line"></span><br><span class="line">- - - - - - - - - - - - - -</span><br><span class="line">my name is 小明,my weight is 75.5</span><br><span class="line">my name is 小美,my weight is 45.0</span><br></pre></td></tr></table></figure>
<h4 id="摆放家具"><a href="#摆放家具" class="headerlink" title="摆放家具"></a>摆放家具</h4><h5 id="需求-2"><a href="#需求-2" class="headerlink" title="需求"></a>需求</h5><ol>
<li>房子House有户型，总面积和家具名称列表<ul>
<li>新房子没有任何家具</li>
</ul>
</li>
<li>家具HouseItem有名字和占地面积，其中<ul>
<li>席梦思bed占地4 m<sup>2</sup></li>
<li>衣柜chest占地2 m<sup>2</sup></li>
<li>餐桌table占地1.5 m<sup>2</sup></li>
</ul>
</li>
<li>将以上家具添加到房子中</li>
<li>打印房子时，要求输出：户型，总面积，剩余面积，家具名称列表</li>
</ol>
<p>类：House, HouseItem<br>各自属性：name，area ;   house_type, area, free_area, item_list<br>各自方法：</p>
<ul>
<li><code>__init__(self,name,area):</code>, <code>__str__(self):</code></li>
<li><code>__init__(self,house_type,area):</code>, <code>__str__(self):</code>, <code>add_item(self,item):</code></li>
</ul>
<p>注意更新房子剩余面积<br>在开发时，哪个类被用到就先开发哪个类</p>
<h5 id="实现-2"><a href="#实现-2" class="headerlink" title="实现"></a>实现</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class HouseItem:</span><br><span class="line">    def __init__(self,name,area):</span><br><span class="line">        self.name=name</span><br><span class="line">        self.area=area</span><br><span class="line">    def __str__(self):</span><br><span class="line">        return &apos;This is %s , area is %.1f&apos; %(self.name,self.area)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class House:</span><br><span class="line">    def __init__(self,type,area):</span><br><span class="line">        self.type=type</span><br><span class="line">        self.area=area</span><br><span class="line">        self.free_area=area</span><br><span class="line">        self.item_list=[]</span><br><span class="line">    def __str__(self):</span><br><span class="line">        # Python会自动将一对括号内部的代码连接在一起</span><br><span class="line">        return (&apos;\n户型： %s\n总面积： %.1f(剩余%.1f)\n家具： %s&apos;</span><br><span class="line">                %(self.type,self.area,self.free_area,self.item_list))</span><br><span class="line">    def add_item(self,item):</span><br><span class="line">        print(&apos;\n添加家具%s,占地面积%.1f...&apos; %(item.name,item.area))</span><br><span class="line">        self.free_area-=item.area</span><br><span class="line">        self.item_list.append(item.name)</span><br><span class="line"></span><br><span class="line"># 1. 创建家具</span><br><span class="line">bed=HouseItem(&apos;ximengsi&apos;,4)</span><br><span class="line">chest=HouseItem(&apos;衣柜&apos;,2)</span><br><span class="line">table=HouseItem(&apos;餐桌&apos;,2)</span><br><span class="line">print(chest)</span><br><span class="line"></span><br><span class="line"># 2. 创建房子对象</span><br><span class="line">my_home=House(&apos;两室一厅&apos;,120)</span><br><span class="line">print(my_home)</span><br><span class="line">my_home.add_item(bed)</span><br><span class="line">print(my_home)</span><br><span class="line">my_home.add_item(chest)</span><br><span class="line">my_home.add_item(table)</span><br><span class="line">print(my_home)</span><br><span class="line"></span><br><span class="line">- - - - - - - - - - - - - -</span><br><span class="line">This is 衣柜 , area is 2.0</span><br><span class="line"></span><br><span class="line">户型： 两室一厅</span><br><span class="line">总面积： 120.0(剩余120.0)</span><br><span class="line">家具： []</span><br><span class="line"></span><br><span class="line">添加家具ximengsi,占地面积4.0...</span><br><span class="line"></span><br><span class="line">户型： 两室一厅</span><br><span class="line">总面积： 120.0(剩余116.0)</span><br><span class="line">家具： [&apos;ximengsi&apos;]</span><br><span class="line"></span><br><span class="line">添加家具衣柜,占地面积2.0...</span><br><span class="line"></span><br><span class="line">添加家具餐桌,占地面积2.0...</span><br><span class="line"></span><br><span class="line">户型： 两室一厅</span><br><span class="line">总面积： 120.0(剩余112.0)</span><br><span class="line">家具： [&apos;ximengsi&apos;, &apos;衣柜&apos;, &apos;餐桌&apos;]</span><br></pre></td></tr></table></figure>
<h5 id="进一步改进"><a href="#进一步改进" class="headerlink" title="进一步改进"></a>进一步改进</h5><p>在添加家具时，应该考虑到房子剩余面积不小于家具占地面积<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def add_item(self,item):</span><br><span class="line">    print(&apos;\n添加家具%s,占地面积%.1f...&apos; %(item.name,item.area))</span><br><span class="line">    if self.free_area&lt;item.area:</span><br><span class="line">        print(&apos;The remaining area is not enough to put down this furniture&apos;)</span><br><span class="line">        return </span><br><span class="line">    self.free_area-=item.area</span><br><span class="line">    self.item_list.append(item.name)</span><br></pre></td></tr></table></figure></p>
<h5 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h5><p>主程序只负责创建房子对象和家具对象<br>其余方法都被封装到类内部</p>
<h4 id="面向对象封装案例"><a href="#面向对象封装案例" class="headerlink" title="面向对象封装案例"></a>面向对象封装案例</h4><p>一个对象的属性可以是另一个类所创建的对象</p>
<h5 id="士兵突击"><a href="#士兵突击" class="headerlink" title="士兵突击"></a>士兵突击</h5><ol>
<li>许三多 有一把 AK47</li>
<li>士兵可以开火</li>
<li>枪可以发射子弹</li>
<li>枪能够装填子弹</li>
</ol>
<p>这里，枪对象就是士兵的属性<br>根据上述需求定义枪类和士兵类</p>
<h5 id="关于士兵类"><a href="#关于士兵类" class="headerlink" title="关于士兵类"></a>关于士兵类</h5><p>假设：一个新兵没有枪<br>定义没有初始值的属性<br>在定义属性时，如果不知道设置什么初始值，可以设置维<code>None</code></p>
<ul>
<li><code>None</code>关键字表示空</li>
<li>表示一个空对象，没有方法和属性，是一个特殊的常量</li>
<li>可以将<code>None</code>赋给任意一个变量</li>
</ul>
<h5 id="实现-3"><a href="#实现-3" class="headerlink" title="实现"></a>实现</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Gun:</span><br><span class="line">    def __init__(self,model,bullet):</span><br><span class="line">        self.model=model # 枪的型号</span><br><span class="line">        self.bullet=bullet # 子弹数量</span><br><span class="line">    def add_bullet(self,count):</span><br><span class="line">        self.bullet+=count</span><br><span class="line">    def shoot(self):</span><br><span class="line">        if self.bullet&lt;=0:</span><br><span class="line">            print(&quot;No bullets&quot;)</span><br><span class="line">            return 0</span><br><span class="line">        print(&apos;biubiubiu...&apos;)</span><br><span class="line">        self.bullet-=1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Soldier:</span><br><span class="line">    def __init__(self,name):</span><br><span class="line">        self.name=name</span><br><span class="line">        self.gun=None # 新兵没有枪</span><br><span class="line">    def fire(self):</span><br><span class="line">        # 1. 判断是否有枪</span><br><span class="line"></span><br><span class="line">        if self.gun is None: # 针对None比较时，建议使用is判断,&quot;is&quot;判断引用对象是否一样，“==”判断对象值是否一样</span><br><span class="line">            print(&apos;%s 还没有枪...&apos; %self.name)</span><br><span class="line">            return -1</span><br><span class="line">        self.gun.shoot()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 1. 创建枪对象</span><br><span class="line">ak47=Gun(&apos;AK47&apos;,36)</span><br><span class="line"></span><br><span class="line"># 2. 创建士兵</span><br><span class="line">xusanduo=Soldier(&apos;许三多&apos;)</span><br><span class="line">xusanduo.gun=ak47</span><br><span class="line">xusanduo.fire()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python面向对象</title>
    <url>/2019/10/11/Python%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://www.bilibili.com/video/av68350017/?p=390" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="面向过程"><a href="#面向过程" class="headerlink" title="面向过程"></a>面向过程</h4><h5 id="如何做"><a href="#如何做" class="headerlink" title="如何做"></a>如何做</h5><ol>
<li>把完成某一个需求的所有步骤从头到尾逐步实现</li>
<li>根据开发需求，将某些功能独立的代码封装成一个又一个函数</li>
<li>最后完成的代码，就是顺序地调用不同的函数</li>
</ol>
<h5 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h5><ol>
<li>注重步骤与过程，不注重职责分工</li>
<li>如果需求复杂，代码会变得很复杂</li>
<li>开发复杂项目，没有固定的套路，开发难度大</li>
</ol>
<h4 id="面向对象"><a href="#面向对象" class="headerlink" title="面向对象"></a>面向对象</h4><p>相比较函数，面向对象是更大的封装，根据职责再一个对象中封装多个方法</p>
<h5 id="如何做-1"><a href="#如何做-1" class="headerlink" title="如何做"></a>如何做</h5><ol>
<li>在完成某一个需求前，首先确定职责–要做的事情(方法)</li>
<li>根据职责确定不同的对象，在对象内部封装不同的方法</li>
<li>最后完成的代码，就是顺序地让不同地对象调用不同的方法</li>
</ol>
<h5 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h5><ol>
<li>注重对象和职责，不同的对象承担不同的职责，</li>
<li>更加适合应对复杂的需求变化，是专门应对复杂项目开发，提供的规定套路</li>
<li>需要在面向过程的基础上，再学习一些面向对象的语法</li>
</ol>
<h4 id="类和对象"><a href="#类和对象" class="headerlink" title="类和对象"></a>类和对象</h4><ol>
<li><p>类<br>类是对一群具有<strong>相同特征</strong>或者<strong>行为</strong>的事物的一个统称，不能直接使用<br>特征 被称为 属性<br>行为 被称为 方法</p>
</li>
<li><p>对象<br>对象是由类别创建出来的一个具体的存在，可以直接使用<br>由哪一个类创建出来的对象，就拥有那一个类中定义的方法与属性</p>
</li>
<li><p>关系<br>类只有一个，对象可以有很多个<br>不同对象之间属性可能会各不相同     </p>
</li>
</ol>
<h4 id="类的设计"><a href="#类的设计" class="headerlink" title="类的设计"></a>类的设计</h4><p>使用面向对象开发前，应该首先分析需求，确定一下程序中需要包含哪些类！<br>对于每个类，紧扣属性与方法<br>类名： 比如动物，人类，大驼峰命名法（单词首字母大写，不用下划线）<br>属性： 这类事物的特征，比如人有年龄身高<br>方法：事物具有的行为，比如人会吃饭睡觉</p>
<h5 id="类名确定"><a href="#类名确定" class="headerlink" title="类名确定"></a>类名确定</h5><p>名词提炼法：分析整个业务流程，出现的名词，通常就是找到的类</p>
<h5 id="属性和方法的确定"><a href="#属性和方法的确定" class="headerlink" title="属性和方法的确定"></a>属性和方法的确定</h5><p>对 <strong>对象的特征描述</strong>，通常定义成属性<br><strong>对象具有的行为（动词）</strong>通常可以定义成方法</p>
<h4 id="面向对象基础语法"><a href="#面向对象基础语法" class="headerlink" title="面向对象基础语法"></a>面向对象基础语法</h4><p>目标：  </p>
<ol>
<li><code>dir</code>内置函数</li>
<li>定义简单的类（只包含方法）</li>
<li>方法中的<code>self</code>参数</li>
<li>初始化方法</li>
<li>内置方法和属性</li>
</ol>
<h5 id="dir内置函数"><a href="#dir内置函数" class="headerlink" title="dir内置函数"></a>dir内置函数</h5><p>在Python中对象几乎是无处不在的，如何验证对象呢：</p>
<ol>
<li>在<strong>标识符/数据</strong> 后面接一个<code>.</code>，然乎按下tab键，会显示该对象能调用的常用方法列表</li>
<li>使用内置函数<code>dir</code>，在dir()函数中传入<strong>标识符/数据</strong>，可以查看对象内的所有属性与方法</li>
</ol>
<p><code>__方法名__</code>格式的方法是Python中提供的内置方法/属性<br>常用几个：</p>
<ol>
<li><code>__new__</code> : 方法，创建对象时，会被自动调用</li>
<li><code>__init__</code>: 方法，对象被初始化时，会被自动调用</li>
<li><code>__del__</code>： 方法，对象被从内存中销毁前，会被自动调用</li>
<li><code>__str__</code>： 方法，返回对象描述信息，print函数输出使用</li>
</ol>
<h5 id="定义简单类"><a href="#定义简单类" class="headerlink" title="定义简单类"></a>定义简单类</h5><p>语法格式：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class ClassName:</span><br><span class="line">    def fun1(self,parameter_list):</span><br><span class="line">        pass</span><br><span class="line">    def fun2(self,parameter_list):</span><br><span class="line">        pass</span><br><span class="line">        class name:</span><br><span class="line">    def fun1(self,parameter_list):</span><br><span class="line">        pass</span><br><span class="line">    def fun2(self,parameter_list):</span><br><span class="line">        pass</span><br></pre></td></tr></table></figure></p>
<p>方法定义无特殊之处，但参数列表第一个必须是<code>self</code></p>
<h5 id="创建对象"><a href="#创建对象" class="headerlink" title="创建对象"></a>创建对象</h5><p>语法格式：<code>object_name=ClassName()</code></p>
<h5 id="简单面向对象程序"><a href="#简单面向对象程序" class="headerlink" title="简单面向对象程序"></a>简单面向对象程序</h5><ol>
<li>需求<ul>
<li><strong>小猫</strong>爱<strong>吃</strong>鱼，小猫要<strong>喝</strong>水</li>
</ul>
</li>
<li>分析<ul>
<li>定义一个猫类<code>Cat</code></li>
<li>定义两个方法<code>eat</code>和<code>drink</code></li>
<li>按照需求– 不需要定义属性</li>
</ul>
</li>
<li>实现  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  class Cat:</span><br><span class="line">    def eat(self): # 哪一个对象调用了方法，self就是那一个对象的引用</span><br><span class="line">        print(&apos;eat fish!&apos;)</span><br><span class="line">    def drink(self):</span><br><span class="line">        print(&apos;drink water!&apos;)</span><br><span class="line"></span><br><span class="line">cat1=Cat() # 创建对象</span><br><span class="line">cat1.drink() # 调用方法</span><br><span class="line"></span><br><span class="line">- - - - - - - - - - - </span><br><span class="line">drink water!</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h5 id="关于引用的概念"><a href="#关于引用的概念" class="headerlink" title="关于引用的概念"></a>关于引用的概念</h5><p>在上述代码中，使用类创建对象后，<code>cat1</code>变量中仍然记录的是对象在内存中的地址，即<code>cat1</code>变量引用了新建的猫对象<br>使用<code>print</code>输出对象变量，默认情况下，是能够输出这个变量引用的对象是由哪一个类创建的对象，以及在内存中的十六进制地址<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(cat1)</span><br><span class="line">addr=id(cat1)</span><br><span class="line">print(&apos;%d , %x&apos; %(addr,addr))</span><br><span class="line">- - - - - - - - - - - </span><br><span class="line">&lt;__main__.Cat object at 0x7f0166bddcf8&gt;</span><br><span class="line">139643995413752 , 7f0166bddcf8</span><br></pre></td></tr></table></figure></p>
<p>思考：如果再创建另一个对象，这两个对象是一样的吗？  </p>
<ul>
<li>答案是否的，但如果用等号赋值，那么就是用一个引用了</li>
</ul>
<h5 id="方法中的self参数"><a href="#方法中的self参数" class="headerlink" title="方法中的self参数"></a>方法中的self参数</h5><p>案例改造：给对象增加属性<br>实现很容易，但并不建议使用，对象属性的封装应该在类的内部<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat1.name=&apos;Tom&apos; # 直接给新增的属性赋值就可以了</span><br><span class="line">print(dir(cat1))</span><br><span class="line"></span><br><span class="line">- - - - - - - - - - - </span><br><span class="line">[&apos;__class__&apos;, &apos;__delattr__&apos;, &apos;__dict__&apos;, &apos;__dir__&apos;, &apos;__doc__&apos;, &apos;__eq__&apos;, &apos;__format__&apos;, &apos;__ge__&apos;, &apos;__getattribute__&apos;, &apos;__gt__&apos;, &apos;__hash__&apos;, &apos;__init__&apos;, &apos;__le__&apos;, &apos;__lt__&apos;, &apos;__module__&apos;, &apos;__ne__&apos;, &apos;__new__&apos;, &apos;__reduce__&apos;, &apos;__reduce_ex__&apos;, &apos;__repr__&apos;, &apos;__setattr__&apos;, &apos;__sizeof__&apos;, &apos;__str__&apos;, &apos;__subclasshook__&apos;, &apos;__weakref__&apos;, &apos;drink&apos;, &apos;eat&apos;, &apos;name&apos;]</span><br></pre></td></tr></table></figure></p>
<p>之前的代码中介绍过self表示调用方法的那个对象的引用，那么也可以在类中用self访问新增的属性，但是在类中没有定义这个属性就很不方便了。</p>
<h5 id="初始化方法"><a href="#初始化方法" class="headerlink" title="初始化方法"></a>初始化方法</h5><p>当使用类名()创建对象时，对自动执行以下操作：</p>
<ol>
<li>在内存空间中为对象分配空间 - - 创建对象</li>
<li>为对象的属性设置初始值 - - 初始化方法（init）</li>
</ol>
<p> 这个初始化方法就是<code>__init__</code>方法，这个方法时对象的内置方法，专门用来定义一个类中具有哪些属性的方法<br> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> class Cat:</span><br><span class="line">    def __init__(self): # 创建对象时自动调用</span><br><span class="line">        print(&apos;this is init&apos;)</span><br><span class="line"></span><br><span class="line">tom=Cat()</span><br><span class="line"></span><br><span class="line">- - - -  - - - -</span><br><span class="line">this is init</span><br></pre></td></tr></table></figure></p>
<h5 id="在初始化方法内部定义属性"><a href="#在初始化方法内部定义属性" class="headerlink" title="在初始化方法内部定义属性"></a>在初始化方法内部定义属性</h5><p> 在<code>__init__</code>方法内部使用<code>self.属性名=属性的初始值</code>就可以定义属性，在创建对象时新的对象都会拥有该属性<br> 这样，我们就可以把外部新增属性的代码写进初始化方法<code>__init__</code>中即可</p>
<h5 id="使用参数设置属性初始值"><a href="#使用参数设置属性初始值" class="headerlink" title="使用参数设置属性初始值"></a>使用参数设置属性初始值</h5><p>其实就是希望在新建对象时同时赋初始值，这些初始值需自己指定<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Cat:</span><br><span class="line">    def __init__(self,name):</span><br><span class="line">        self.name=name</span><br><span class="line"></span><br><span class="line">cat1=Cat(&apos;Tom&apos;)</span><br><span class="line">print(cat1.name)</span><br><span class="line"></span><br><span class="line">- - - - - - - - - - - - -</span><br><span class="line">Tom</span><br></pre></td></tr></table></figure></p>
<h5 id="内置方法和属性"><a href="#内置方法和属性" class="headerlink" title="内置方法和属性"></a>内置方法和属性</h5><ol>
<li><p><code>__del__</code>方法<br>和<code>__init__</code>方法相反，但类似，也可以在函数体中做希望做的事，在所有代码执行结束之后会自动调用，也可以使用<code>del 对象名</code>手动销毁，<code>__del__</code>函数体中的方法自动执行，这即是我们希望对象在被销毁前做的事。</p>
</li>
<li><p>生命周期<br>从使用类名创建，生命周期开始，到<code>__del__</code>方法调用，生命周期结束。注意这些内置方法不用手动定义也会自动执行，我们手动定义也只是希望在自动调用时希望做的事。</p>
</li>
</ol>
<h5 id="str-方法"><a href="#str-方法" class="headerlink" title="__str__方法"></a><code>__str__</code>方法</h5><p>前面提到过使用<code>print</code>输出对象变量 时，默认情况下会输出这个变量引用的对象是由哪一个类创建的，以及16进制地址<br>在开发中，如果希望使用<code>print</code>输出对象变量时，能打印出自定义的内容，就可以利用<code>__str__</code>这个内置方法。<strong>注意：<code>__str__</code>方法必须返回一个字符串</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Cat:</span><br><span class="line">    def __init__(self,name):</span><br><span class="line">        self.name=name</span><br><span class="line">    def __del__(self):</span><br><span class="line">        print(&apos;goodbye!&apos;)</span><br><span class="line">    def __str__(self):</span><br><span class="line">        return &quot;I&apos;m a cat %s&quot; %self.name</span><br><span class="line"></span><br><span class="line">cat1=Cat(&apos;Tom&apos;)</span><br><span class="line">print(cat1)</span><br><span class="line"></span><br><span class="line">- - - - - - - - - - - - </span><br><span class="line">I&apos;m a cat Tom</span><br><span class="line">goodbye!</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>决策树</title>
    <url>/2019/09/26/%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://www.bilibili.com/video/av62758431" target="_blank" rel="noopener">学习地址</a></center></h3><hr>
<ol>
<li><a href="https://www.bilibili.com/video/av62758431" target="_blank" rel="noopener">理论篇</a></li>
<li><a href="https://www.bilibili.com/video/av35523476" target="_blank" rel="noopener">实战篇</a></li>
</ol>
<hr>
<h4 id="关于数据"><a href="#关于数据" class="headerlink" title="关于数据"></a><font size="5" color="red">关于数据</font></h4><p>数据集矩阵X:</p>
<ul>
<li>和之前接触到的数据集一样，m个样本（X<sup>(1)</sup>，X<sup>(2)</sup>，X<sup>(3)</sup>，…），每个样本有n个特征，即数据集是一个<code>m×n</code>的矩阵。  </li>
</ul>
<p>数据集划分：</p>
<ul>
<li>我们可以从这个原始数据集中<strong>随机</strong>选取70%作为训练集/测试集，另30%作为验证集。  </li>
</ul>
<p>决策树模型</p>
<ul>
<li>决策树模型类似一个二叉树，非叶节点都是<strong>判定点/决策点</strong>，所有样本都在叶节点，根据非叶节点划分，预测最终所属分类</li>
<li>决策树是一种基本的分类与回归方法，学习通常包含三个步骤：<strong>特征选择</strong>、<strong>决策树的生成</strong>和<strong>决策树的剪枝</strong>。    </li>
<li>一个关键的议题就是该从这n个特征中选取哪个特征作为根节点的分裂点呢？这个涉及到决策树一个很重要的一个概念：<strong>信息熵</strong></li>
</ul>
<p>输出y</p>
<ul>
<li>决策树生成并优化之后，所有样本数据就可以通过这个二叉树进行分类了，确定最终归属</li>
</ul>
<h4 id="熵理论"><a href="#熵理论" class="headerlink" title="熵理论"></a><font size="5" color="red">熵理论</font></h4><p>首先引入之前做的总结：<a href="https://sfz-lyq.cn/2019/12/05/Untitled/">浅谈互信息与熵</a>  </p>
<p><strong>熵</strong>是从热力学引入过来的，最初描述一个系统的混乱程度。在这里，可以理解为一个事件的<strong>不确定性</strong>。<br>先从<strong>信息量</strong>说起，信息量顾名思义就是一个信息或者说一个子事件(可能)的信息量，比如：马云破产、国足夺冠，这种概率很小的事件包含的信息量很大。<br>而我们要研究一整个范围称为一个事件的话，这些子事件就是若干可能的情况，比如我们研究天气，有若干种可能的情况，每种情况有一个<strong>概率</strong>，但不难理解总的概率之和为1。<br>那么，假设某种可能事件x发生的概率为p(x)，那么这个可能事件所包含的信息量的度量方式就是：<code>I(X)=-log p(x)</code>。再回到熵，我们要知道我们所研究的这整个事件的不确定性，其实称之为信息熵就是这些子事件/可能的 信息量的<strong>期望</strong>。用H(X)来度量整体事件X的信息熵，<code>H(X)=-p(x)log p(x    )</code>    </p>
<p><font size="3" color="green">条件熵</font></p>
<ul>
<li>定义H(Y|X)，也等于H(X,Y) - H(X)，其中H(X,Y)表示X，Y的联合熵，具体计算详见之前引用</li>
<li>含义：X确定的情况下，确定Y所需要的信息量，即Y的不确定性</li>
<li>需要注意的是，联合概率联合熵是对称的，而条件熵和条件概率一样是不对称的</li>
</ul>
<h4 id="决策树理论"><a href="#决策树理论" class="headerlink" title="决策树理论"></a><font size="5px" color="red">决策树理论</font></h4><ol>
<li>决策树是一种树形判定结构，每个非叶节点都是表示对一种属性(特征)的测试，每个分支代表一个测试输出，而每个叶节点代表一种类别</li>
<li>决策树学习采用自顶向下的递归方法，其基本思想是以信息熵为度量构造一颗熵值下降最快的树，到叶子节点处的熵值为零（完全确定），每个叶节点中的实例都属于同一类</li>
<li>容易理解这颗判定树越到底层不确定性就越少</li>
<li>决策树的优点是可以进行自学习</li>
</ol>
<h5 id="信息增益-ID3"><a href="#信息增益-ID3" class="headerlink" title="信息增益 - ID3"></a><font size="3px" color="red">信息增益 - ID3</font></h5><p>上图：<br><img src="https://img-blog.csdnimg.cn/20191218163122118.jpg?shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="信息增益"></p>
<h5 id="互信息"><a href="#互信息" class="headerlink" title="互信息"></a><font size="3px" color="red">互信息</font></h5><p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2ltYWdlczIwMTUuY25ibG9ncy5jb20vYmxvZy83ODg3NTMvMjAxNjEwLzc4ODc1My0yMDE2MTAyNzE1MTIxMDg0My03NDUzNDgwMjYucG5n?x-oss-process=image/format,png" alt="互信息"></p>
<h4 id="决策树标记"><a href="#决策树标记" class="headerlink" title="决策树标记"></a><font size="5px" color="red">决策树标记</font></h4><ul>
<li>记： 数据集记为D，样本数m=|D|</li>
<li>记：最终叶节点个数为K，即有K个分类C<sub>K</sub></li>
<li>记：根据不同的特征将数据集划分为{D<sub>1</sub>, … , D<sub>n</sub> }</li>
<li>记：子集D<sub>i</sub>中属于类C<sub>k</sub>的样本集合为D<sub>ik</sub></li>
<li>以上若干标记符号加绝对值符号表示取集合数量</li>
</ul>
<h4 id="ID3流程"><a href="#ID3流程" class="headerlink" title="ID3流程"></a><font size="5px" color="red">ID3流程</font></h4><ol>
<li>计算数据集D的经验熵H(D)=-&sum;(|C<sub>k</sub>| ÷ |D|) × In(|C<sub>k</sub>| ÷ |D|)</li>
<li>遍历所有特征，对于每个特征A<sub>i</sub>，计算经验条件熵H(D|A<sub>i</sub>)</li>
<li>计算信息增益G(D,A<sub>i</sub>)=H(D)-H(D|A<sub>i</sub>)</li>
<li>选择信息增益最大的当做根节点的分裂特征</li>
<li>关于经验条件熵的计算如图<br><img src="https://img-blog.csdnimg.cn/20191218204149145.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="经验条件熵"><br><img src="https://img-blog.csdnimg.cn/20191218182947620.jpg?shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="信息增益计算"></li>
</ol>
<h5 id="信息增益率C4-5-与-Gini系数"><a href="#信息增益率C4-5-与-Gini系数" class="headerlink" title="信息增益率C4.5 与 Gini系数"></a><font size="3px" color="red">信息增益率C4.5 与 Gini系数</font></h5><p>G<sub>r</sub>(D,A)=g(D,A)÷H(A)<br>Gini=&sum;p<sub>k</sub>(1-p<sub>k</sub>) = 1-&sum;p<sub>k</sub><sup>2</sup> = 1-&sum;(|C<sub>k</sub>|÷|D|)<sup>2</sup></p>
<h4 id="决策树评价"><a href="#决策树评价" class="headerlink" title="决策树评价"></a><font size="5px" color="red">决策树评价</font></h4><p><img src="https://img-blog.csdnimg.cn/20191218211751802.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="决策树评价"></p>
<h4 id="决策树实例"><a href="#决策树实例" class="headerlink" title="决策树实例"></a><font size="5px" color="red">决策树实例</font></h4><h5 id="scikit-learn建模基本流程"><a href="#scikit-learn建模基本流程" class="headerlink" title="scikit-learn建模基本流程"></a><font size="3px" color="red">scikit-learn建模基本流程</font></h5><p><img src="https://img-blog.csdnimg.cn/2019121822071283.jpg?shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="sklearn借口调用"></p>
<h5 id="DecisionTree重要参数"><a href="#DecisionTree重要参数" class="headerlink" title="DecisionTree重要参数"></a><font size="3px" color="red">DecisionTree重要参数</font></h5><ol>
<li><code>criterion</code>：用来决定计算方法，可选[<code>&quot;entropy&quot;</code>, <code>&quot;gini&quot;</code>]，<strong>默认基尼系数</strong></li>
<li>比起基尼系数，信息熵对不纯度更加敏感，对不纯度的惩罚最强。但是<strong>在实际使用中，信息熵和基尼系数的效果基本相同</strong>。信息熵的计算比基尼系数缓慢一些，因为基尼系数不涉及对数。</li>
<li><code>random_state</code>: 随机性，一旦这个参数指定值那么每次运行结果都是确定的，<strong>默认随机</strong></li>
<li><code>splitter</code>: 控制决策树中随机选项，可选[<code></code>best, <code>random</code>]，<code>best</code>表示虽然决策树在分枝时虽然是随机的但优先选取更重要的特征进行分枝（重要性可以通过属性<code>feature_importances_</code>查看），而<code>random</code>在分枝时会更随机，树可能会更深，同时也<strong>可以防止 过拟合</strong></li>
</ol>
<h5 id="建树"><a href="#建树" class="headerlink" title="建树"></a><font size="3px" color="red">建树</font></h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeClassifier # 导入分类树</span><br><span class="line">from sklearn.datasets import load_wine  # 数据集</span><br><span class="line">from sklearn.model_selection import train_test_split # 区分训练集和测试集</span><br></pre></td></tr></table></figure>
<h5 id="简单实例化"><a href="#简单实例化" class="headerlink" title="简单实例化"></a><font size="3px" color="red">简单实例化</font></h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 导入数据集</span><br><span class="line">wine=load_wine() # Load and return the wine dataset (classification)</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">wine.data 是特征</span><br><span class="line">wine.feature_names 是属性</span><br><span class="line">print(wine.feature_names) [&apos;alcohol&apos;, &apos;malic_acid&apos;, &apos;ash&apos;,..., &apos;proline&apos;] totally 13</span><br><span class="line">wine.target 是标签</span><br><span class="line">print(wine.target_names) # [&apos;class_0&apos; &apos;class_1&apos; &apos;class_2&apos;]</span><br><span class="line">print(wine.data.shape) # (178, 13)</span><br><span class="line">print(wine.target.shape) # (178,)</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line"># 完整数据集</span><br><span class="line">data=pd.concat([pd.DataFrame(wine.data),pd.DataFrame(wine.target)],axis=1)</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">print(data.shape) # (178, 14)  # DataFrame</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line"># 划分测试集和训练集，其中30%作为测试集，注意赋值顺序固定</span><br><span class="line">X_train,X_test,Y_train,Y_test=train_test_split(wine.data,wine.target,test_size=0.3)</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">print(X_train.shape) # (124, 13)</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line"># 实例化</span><br><span class="line">clf=DecisionTreeClassifier(criterion=&apos;entropy&apos;)</span><br><span class="line"># 使用训练集训练模型</span><br><span class="line">clf=clf.fit(X_train,Y_train)</span><br><span class="line"># 调用接口导出我们所需要的数据，比如精确度</span><br><span class="line">score=clf.score(X_test,Y_test)</span><br><span class="line">print(score) # 0.9259259259259259</span><br></pre></td></tr></table></figure>
<h5 id="剪枝参数"><a href="#剪枝参数" class="headerlink" title="剪枝参数"></a><font size="3px" color="red">剪枝参数</font></h5><p>为了让决策树有更好的泛化性（在测试集上表现同样好），需要对决策树进行剪枝。剪枝策略对决策树影响巨大，正确的剪枝策略是优化决策树算法的核心。</p>
<ol>
<li><code>max_depth=</code>: 限制树的最大深度，超过设定深度的树枝全部剪掉</li>
<li><code>min_samples_leaf=</code>和<code>min_samples_split=</code>: 前者限定一个节点在分枝后每个子节点都必须包含至少min_samples_leaf个训练样本，否则将会被剪掉；后者限定一个节点至少包含min_samples_split个训练样本，这个节点才会被分枝，否者分枝不会被发生</li>
<li><code>max_features=</code>和<code>min_impurity_decrease=</code>: 前者限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃，用来限制高维度过拟合，但如果不知道属性重要性强行剪去特征可能会导致学习不足；后者限制信息增益的大小，信息增益小于设定数值分枝不会发生</li>
</ol>
<h5 id="超参数曲线"><a href="#超参数曲线" class="headerlink" title="超参数曲线"></a><font size="3px" color="red">超参数曲线</font></h5><p>实际就是根据调参将结果绘制出来，衡量模型表现<br>例如：以深度作为参数，精确度作为衡量指标<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeClassifier # 导入分类树</span><br><span class="line">from sklearn.datasets import load_wine  # 数据集</span><br><span class="line">from sklearn.model_selection import train_test_split # 区分训练集和测试集</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"># 导入数据集</span><br><span class="line">wine=load_wine() # Load and return the wine dataset (classification)</span><br><span class="line"># 划分数据集</span><br><span class="line">X_train,X_test,Y_train,Y_test=train_test_split(wine.data,wine.target,test_size=0.3)</span><br><span class="line"></span><br><span class="line">score=[]</span><br><span class="line">for i in range(10):</span><br><span class="line">    clf = DecisionTreeClassifier(max_depth=i+1</span><br><span class="line">                                 ,criterion=&apos;entropy&apos;</span><br><span class="line">                                 ,random_state=30</span><br><span class="line">                                )</span><br><span class="line">    clf = clf.fit(X_train, Y_train)</span><br><span class="line">    score .append(clf.score(X_test, Y_test))</span><br><span class="line"></span><br><span class="line">plt.plot(range(1,11),score,&apos;r&apos;,label=&quot;max_depth&quot;)</span><br><span class="line">plt.legend(loc=&quot;best&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://img-blog.csdnimg.cn/20191219140332401.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="超参数曲线"><br>根据上图可以明显地观察到<code>max_depth</code>取值为3就可以了</p>
<h5 id="重要属性和接口"><a href="#重要属性和接口" class="headerlink" title="重要属性和接口"></a><font size="3px" color="red">重要属性和接口</font></h5><p>这里的属性是指算法模型训练之后能够查看的各种模型的性质</p>
<ol>
<li><code>clf.apply(X_test)</code>：返回每个测试样本所在叶子节点的索引</li>
<li><code>clf.predict(X_test)</code>：返回每个测试样本的预测结果 </li>
</ol>
<h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p><img src="https://img-blog.csdnimg.cn/20191219141024408.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="总结"></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>NLP</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Python自然语言处理NLP(六)</title>
    <url>/2019/09/26/Python%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86NLP-%E5%85%AD/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://www.bilibili.com/video/av29796449/?p=28" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a>文本分类</h4><h5 id="分类的使用"><a href="#分类的使用" class="headerlink" title="分类的使用"></a>分类的使用</h5><ol>
<li>姓名判别性别</li>
<li>文本分类</li>
<li>词性分类</li>
<li>句子分割</li>
<li>识别对话性为</li>
</ol>
<h5 id="分类算法"><a href="#分类算法" class="headerlink" title="分类算法"></a>分类算法</h5><ol>
<li>朴素贝叶斯分类器</li>
<li>决策树</li>
<li>KNN</li>
<li>神经网络</li>
<li>SVM</li>
</ol>
<h4 id="使用朴素贝叶斯进行性别鉴定"><a href="#使用朴素贝叶斯进行性别鉴定" class="headerlink" title="使用朴素贝叶斯进行性别鉴定"></a>使用朴素贝叶斯进行性别鉴定</h4><ol>
<li>使用单个特征<br>特征：名字的最后一个字母<br>类别：男性、女性<br>贝叶斯公式：P(B|A)=P(AB)/P(A)=P(A|B)*P(B)/P(A)<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&apos;&apos;&apos; 性别鉴定器 &apos;&apos;&apos;</span><br><span class="line">from nltk.corpus import names</span><br><span class="line">import nltk</span><br><span class="line">import random</span><br><span class="line"># 特征提取器</span><br><span class="line">def gender_features(word):</span><br><span class="line">    return &#123;&apos;last_letter&apos;:word[-1]&#125;</span><br><span class="line"></span><br><span class="line">names_set=[(name,&apos;male&apos;) for name in names.words(&apos;male.txt&apos;)]+[(name,&apos;female&apos;) for name in names.words(&apos;female.txt&apos;)]</span><br><span class="line">random.shuffle(names_set) # 随机打乱</span><br><span class="line"># print(names_set[:10]) # [(&apos;Samantha&apos;, &apos;female&apos;), (&apos;Nanette&apos;, &apos;female&apos;), (&apos;Layney&apos;, &apos;female&apos;), (&apos;Bertie&apos;, &apos;male&apos;), (&apos;Godfry&apos;, &apos;male&apos;),...</span><br><span class="line">features=[(gender_features(n),g) for (n,g) in names_set] # 提取特征</span><br><span class="line">train_set,test_set=features[500:],features[:500] # 设置训练集和测试集</span><br><span class="line">classifier=nltk.NaiveBayesClassifier.train((train_set)) # 分类器</span><br><span class="line">print(classifier.classify((gender_features(&apos;Neo&apos;)))) # male</span><br><span class="line">print(nltk.classify.accuracy(classifier,test_set)) # 判断正确率 0.77</span><br><span class="line"># 大型数据集划分</span><br><span class="line">from nltk.classify import apply_features</span><br><span class="line">train_set=apply_features(gender_features,names[500:]) # 不用将数据存到内存中，直接在原始数据集中划分</span><br><span class="line">test_set=apply_features(gender_features,names[:500])</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>隐马尔可夫模型HMM</title>
    <url>/2019/09/26/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8BHMM/</url>
    <content><![CDATA[<h2 id="隐马尔可夫模型HMM"><a href="#隐马尔可夫模型HMM" class="headerlink" title="隐马尔可夫模型HMM"></a><center>隐马尔可夫模型HMM</center></h2><h3 id="马尔可夫模型"><a href="#马尔可夫模型" class="headerlink" title="马尔可夫模型"></a>马尔可夫模型</h3><p>本质上，马尔可夫模型就是一个转移矩阵，寻找一个事物在一段时间内的变化模式。这个模式可能发生在很多领域，看起来无关，但实际上存在一定的关系。比如传统的渔民会根据海藻的变化情况预测天气的变化情况。</p>
<h3 id="隐马尔可夫模型"><a href="#隐马尔可夫模型" class="headerlink" title="隐马尔可夫模型"></a>隐马尔可夫模型</h3><p>传统的马尔可夫模型只会根据前几天的一个天气情况进行明天的天气预测。而隐马尔可夫模型会将海藻的变化这样一个隐藏因素考虑进去。</p>
<h4 id="生成模式"><a href="#生成模式" class="headerlink" title="生成模式"></a>生成模式</h4><h5 id="确定模式"><a href="#确定模式" class="headerlink" title="确定模式"></a>确定模式</h5><p>比如红绿灯，状态是确定的，每一个状态都是唯一依赖于前一个状态。只要知道了前一个状态就可以知道后面的状态。</p>
<h5 id="非确定模式"><a href="#非确定模式" class="headerlink" title="非确定模式"></a>非确定模式</h5><p>也就是说下一个状态是不确定的，希望根据系统建模预测下一个状态。</p>
<h4 id="隐藏模式"><a href="#隐藏模式" class="headerlink" title="隐藏模式"></a>隐藏模式</h4><pre><code>通过直接观测天气是可能是无法预测明天的天气的，而隐藏的状态是可以观测水藻的状态来预测。
</code></pre><p>一个隐马尔可夫马尔可夫模型是一个三元组(pi,A,B)。&pi;表示初始化概率向量，A=（a<sub>ij</sub>）表示状态转移矩阵，Pr(x<sub>i<sub>t</sub></sub>|x<sub>j<sub>t-1</sub></sub>)，B=（b<sub>ij</sub>）表示混淆矩阵Pr(y<sub>i</sub>|x<sub>j</sub>)</p>
<h4 id="隐马尔可夫模型HMM的三大基本问题于解决方案"><a href="#隐马尔可夫模型HMM的三大基本问题于解决方案" class="headerlink" title="隐马尔可夫模型HMM的三大基本问题于解决方案"></a>隐马尔可夫模型HMM的三大基本问题于解决方案</h4><ol>
<li>对于一个观察序列匹配最可能的的系统–评估，使用前向算法解决</li>
<li>对于已生成的一个观察序列，确定最可能的隐藏状态序列–解码，使用维特比算法解决</li>
<li>对于已生成的观察序列，决定最可能的模型参数–学习，使用前向-后向算法解决。</li>
</ol>
<hr>
<p>参考资料：</p>
<ol>
<li><a href="https://www.cnblogs.com/baiboy/p/hmm2.html" target="_blank" rel="noopener">马尔可夫模型与隐马尔可夫模型</a></li>
<li><a href="https://www.cnblogs.com/jacklu/p/7753471.html" target="_blank" rel="noopener">隐马尔可夫模型(HMM)</a></li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Python自然语言处理NLP(五)</title>
    <url>/2019/09/25/Python%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86NLP-%E4%BA%94/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://www.bilibili.com/video/av29796449/?p=25" target="_blank" rel="noopener">学习地址</a></center></h3><h3 id="词性标注"><a href="#词性标注" class="headerlink" title="词性标注"></a>词性标注</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&apos;&apos;&apos; 词性标注器 &apos;&apos;&apos;</span><br><span class="line">from nltk.tokenize import word_tokenize</span><br><span class="line">from nltk.tag import pos_tag</span><br><span class="line">text1=word_tokenize(&apos;Never find someone like you&apos;)</span><br><span class="line">text2=word_tokenize(&apos;Because of you, I never stray too far from the sidewalk&apos;)</span><br><span class="line">print(pos_tag(text1)) # [(&apos;Never&apos;, &apos;RB&apos;), (&apos;find&apos;, &apos;VBP&apos;), (&apos;someone&apos;, &apos;NN&apos;), (&apos;like&apos;, &apos;IN&apos;), (&apos;you&apos;, &apos;PRP&apos;)]</span><br><span class="line">text3=nltk.Text(word.lower() for word in nltk.corpus.brown.words())</span><br><span class="line">print(type(text3),len(text3)) # &lt;class &apos;nltk.text.Text&apos;&gt; 1161192</span><br><span class="line">text3.similar(&apos;the&apos;) # a his this their its her an that our any all one these my in your no some other and</span><br></pre></td></tr></table></figure>
<ol>
<li>标注语料库：由词和词性这样的元组所组成的词列表，不同的语料库词性标记是不同的<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&apos;&apos;&apos; 标注语料库 &apos;&apos;&apos;</span><br><span class="line">tagged_token=nltk.tag.str2tuple(&apos;fly/NN&apos;)</span><br><span class="line">print(tagged_token) # (&apos;fly&apos;, &apos;NN&apos;)</span><br><span class="line"></span><br><span class="line">from nltk.corpus import brown</span><br><span class="line">brown_news_tagged=brown.tagged_words(categories=&apos;news&apos;,tagset=&apos;universal&apos;) # 将新闻类的词性元组拿出来</span><br><span class="line">#print(brown_news_tagged) # [(&apos;The&apos;, &apos;DET&apos;), (&apos;Fulton&apos;, &apos;NOUN&apos;), ...]</span><br><span class="line">tag_fd=nltk.FreqDist(tag for (word,tag) in brown_news_tagged) # 统计各种词性使用次数</span><br><span class="line">print(tag_fd.most_common()) # [(&apos;NOUN&apos;, 30654), (&apos;VERB&apos;, 14399), (&apos;ADP&apos;, 12355), (&apos;.&apos;, 11928), (&apos;DET&apos;, 11389), (&apos;ADJ&apos;, 6706), (&apos;ADV&apos;, 3349), (&apos;CONJ&apos;, 2717), (&apos;PRON&apos;, 2535), (&apos;PRT&apos;, 2264), (&apos;NUM&apos;, 2166), (&apos;X&apos;, 92)]</span><br><span class="line">tag_fd.plot() # 绘制词性使用情况</span><br><span class="line">word_tag_pair=list(nltk.bigrams(brown_news_tagged)) # 统计名词前面最常出现什么词</span><br><span class="line">print(nltk.FreqDist(a[1] for (a,b) in word_tag_pair if b[1]==&apos;NOUN&apos;).most_common()) # [(&apos;NOUN&apos;, 7959), (&apos;DET&apos;, 7373), (&apos;ADJ&apos;, 4761), (&apos;ADP&apos;, 3781), (&apos;.&apos;, 2796), (&apos;VERB&apos;, 1842), (&apos;CONJ&apos;, 938), (&apos;NUM&apos;, 894), (&apos;ADV&apos;, 186), (&apos;PRT&apos;, 94), (&apos;PRON&apos;, 19), (&apos;X&apos;, 11)]</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><img src="https://www.privacypic.com/images/2019/09/25/Figure_407628c4bd209a72bb.png" alt="Figure_407628c4bd209a72bb.png"></p>
<ol start="2">
<li><p>自动标注器</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> &apos;&apos;&apos; 默认标注器 &apos;&apos;&apos;</span><br><span class="line"> from nltk.corpus import brown</span><br><span class="line">raw=&apos;I like eggs and ham,I also like milk except sheep&apos;</span><br><span class="line">tokens=nltk.word_tokenize(raw)</span><br><span class="line">default_tagger=nltk.DefaultTagger(&apos;NN&apos;) # A tagger that assigns the same tag to every token.</span><br><span class="line">default_tagger.tag(tokens)</span><br><span class="line">temp=brown.tagged_sents(categories=&apos;news&apos;) # [[(&apos;The&apos;, &apos;AT&apos;), (&apos;Fulton&apos;, &apos;NP-TL&apos;), (&apos;County&apos;, &apos;NN-TL&apos;),.....]]</span><br><span class="line">print(default_tagger.evaluate(temp)) # 0.13089484257215028 (相当于与正确答案比较人为标注准确率)</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos; 查询标注器根据已有的标注资料按频数给新文章标注最有可能的结果 &apos;&apos;&apos;</span><br><span class="line">fd=nltk.FreqDist(brown.words(categories=&apos;news&apos;))</span><br><span class="line">cfd=nltk.ConditionalFreqDist(brown.tagged_words(categories=&apos;news&apos;))</span><br><span class="line">most_freq_words=fd.most_common()[:100]</span><br><span class="line">#print(most_freq_words) # [(&apos;the&apos;, 5580), (&apos;,&apos;, 5188),...]</span><br><span class="line">most_likely_tag=dict((word,cfd[word].max()) for (word,freq) in most_freq_words)</span><br><span class="line">baseline_tagger=nltk.UnigramTagger(model=most_likely_tag)</span><br><span class="line">print(baseline_tagger.evaluate(brown.tagged_sents(categories=&apos;news&apos;))) # 0.45578495136941344</span><br></pre></td></tr></table></figure>
</li>
<li><p>N-gram标注器</p>
</li>
<li><a href="https://www.bilibili.com/video/av29796449/?p=27" target="_blank" rel="noopener">隐马尔可夫标注器</a><br>隐马尔可夫模型：  <ol>
<li>生成模式：确定(当前状态唯一依赖于前一个状态)/非确定模式(下一个状态不确定，假设依赖于前几个状态确定转移矩阵)</li>
<li>隐藏模式：</li>
</ol>
</li>
</ol>
<p><img src="https://www.privacypic.com/images/2019/09/25/2019-09-25-19-46-30-d88046bd077240ec.png" alt="2019-09-25-19-46-30-d88046bd077240ec.png"><br><img src="https://www.privacypic.com/images/2019/09/25/2019-09-25-21-08-56-8e390d095629e17a.png" alt="2019-09-25-21-08-56-8e390d095629e17a.png"></p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Python自然语言处理NLP(四)</title>
    <url>/2019/09/25/Python%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86NLP-%E5%9B%9B/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://www.bilibili.com/video/av29796449/?p=21" target="_blank" rel="noopener">学习地址</a></center></h3><h3 id="中文分词"><a href="#中文分词" class="headerlink" title="中文分词"></a>中文分词</h3><ol>
<li><p>基于词典、词库匹配</p>
<ul>
<li>正向最大匹配</li>
<li>逆向最大匹配</li>
<li>双向最大匹配</li>
<li>设立切分标志法</li>
<li>最佳匹配</li>
</ul>
</li>
<li><p>基于词频度统计</p>
<ul>
<li>N-gram模型</li>
<li>隐马尔科夫模型</li>
<li>基于字标注的中文分词方法</li>
</ul>
</li>
<li><p>基于规则（语义、理解）的分词</p>
</li>
</ol>
<p><img src="https://www.privacypic.com/images/2019/09/25/2019-09-25-14-12-42-e5bf2b9fed9b2289.png" alt="2019-09-25-14-12-42-e5bf2b9fed9b2289.png"></p>
<h4 id="概率语言模型的分词方法"><a href="#概率语言模型的分词方法" class="headerlink" title="概率语言模型的分词方法"></a>概率语言模型的分词方法</h4><p><img src="https://www.privacypic.com/images/2019/09/25/2019-09-25-14-22-23-d8f00fbeb4e4ebc3.png" alt="2019-09-25-14-22-23-d8f00fbeb4e4ebc3.png"></p>
<h4 id="结巴分词jieba"><a href="#结巴分词jieba" class="headerlink" title="结巴分词jieba"></a>结巴分词jieba</h4><h5 id="安装：pip-install-jieba"><a href="#安装：pip-install-jieba" class="headerlink" title="安装：pip install jieba"></a>安装：<code>pip install jieba</code></h5><h5 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h5><ol>
<li>分词<br><code>jieba.cut</code>或者<code>jieba.lcut</code>直接获取列表<br><code>jieba.cut_for_search</code>或者<code>jieba.lcut_for_search</code><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import  jieba</span><br><span class="line">a=&apos;我是一个粉刷匠，粉刷本领强&apos;</span><br><span class="line">b=&apos;我来自北京青花大学&apos;</span><br><span class="line">a=jieba.cut(a,cut_all=False,HMM=True) # cut_all is full/accurate pattern, HMM is Hidden Markov Model</span><br><span class="line">b=jieba.cut(b,cut_all=True,HMM=False) # 全模式（各种可能都显示），不使用模型，默认精确模式会显示概率最大的切分方法</span><br><span class="line">print(type(a)) # &lt;class &apos;generator&apos;&gt;</span><br><span class="line">print(&apos; &apos;.join(a)) # 我 是 一个 粉刷 匠 ， 粉刷 本领 强</span><br><span class="line">print(&apos;/&apos;.join(b)) # 我/来自/北京/青花/大学</span><br><span class="line"></span><br><span class="line">b=&apos;小明毕业于中国科学院计算所，后在日本京都大学深造&apos;</span><br><span class="line">b=jieba.lcut_for_search(b)</span><br><span class="line">print(b) # [&apos;小明&apos;, &apos;毕业&apos;, &apos;于&apos;, &apos;中国&apos;, &apos;科学&apos;, &apos;学院&apos;, &apos;科学院&apos;, &apos;中国科学院&apos;, &apos;计算&apos;, &apos;计算所&apos;, &apos;，&apos;, &apos;后&apos;, &apos;在&apos;, &apos;日本&apos;, &apos;京都&apos;, &apos;大学&apos;, &apos;日本京都大学&apos;, &apos;深造&apos;]</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol start="2">
<li><p>词典</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jieba.load_userdict(path) # 添加词典，将一些新的词汇导入进来，如云计算大数据等</span><br><span class="line">jieba.add_word(word,freq=None,tag=None) # 手动添加词汇</span><br><span class="line">jieba.del_word(word) # 手动删除词汇</span><br><span class="line">jieba.suggest_freq(segment,tune=True) # 表示不切分segment，False表示切分</span><br></pre></td></tr></table></figure>
</li>
<li><p>关键词提取</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import jieba.analyse</span><br><span class="line">text=&apos;北京时间9月25日8时54分，中国在酒泉卫星发射中心用长征二号丁运载火箭，成功将云海一号02星发射升空，卫星顺利进入预定轨道，任务获得圆满成功&apos;</span><br><span class="line">for x,w in jieba.analyse.extract_tags(text,topK=20,withWeight=True): #基于TF-IDF提取关键词，topK参数默认20靠前的关键词，withWeigth默认False不返回关键词权重，其他参数查看手册</span><br><span class="line">    print(x,w)</span><br><span class="line">jieba.analyse.set_idf_path(file_path) # 使用自己的语料库</span><br><span class="line">jieba.analyse.set_stop_words(file_path) # 使用自己的停用词库过滤关键词</span><br><span class="line">print(&apos;~~~~~~~~~~~~~~~~~~~~&apos;)</span><br><span class="line">for x,w in jieba.analyse.textrank(text,topK=20,withWeight=True): # 基于TextRank，其余一致</span><br><span class="line">    print(x,w)</span><br><span class="line"></span><br><span class="line">- - - - - - - - - - - - - - - - - - - -</span><br><span class="line">长征二号 0.6318489841818182</span><br><span class="line">酒泉卫星发射中心 0.6003422941545454</span><br><span class="line">25 0.5433985228590908</span><br><span class="line">02 0.5433985228590908</span><br><span class="line">54 0.5433985228590908</span><br><span class="line">圆满成功 0.46771635178636367</span><br><span class="line">升空 0.44005317940000005</span><br><span class="line">云海 0.43745506968</span><br><span class="line">运载火箭 0.4153865325990909</span><br><span class="line">一号 0.38291882272909095</span><br><span class="line">预定 0.3607771958345455</span><br><span class="line">卫星 0.3272702973831818</span><br><span class="line">轨道 0.31525442301681816</span><br><span class="line">顺利 0.30144352380818185</span><br><span class="line">发射 0.2976179877936364</span><br><span class="line">任务 0.25559143141454543</span><br><span class="line">成功 0.24109631387181818</span><br><span class="line">获得 0.22140456289909088</span><br><span class="line">北京 0.21215465039636364</span><br><span class="line">进入 0.2073690901</span><br><span class="line">时间 0.18543361965727276</span><br><span class="line">中国 0.13760548575727272</span><br><span class="line">~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">进入 1.0</span><br><span class="line">预定 0.9283963898515567</span><br><span class="line">轨道 0.9229732862562409</span><br><span class="line">任务 0.9169044924501285</span><br><span class="line">发射 0.7989776734747804</span><br><span class="line">时间 0.7986657144130873</span><br><span class="line">北京 0.7932598978102895</span><br><span class="line">获得 0.6955318819575269</span><br><span class="line">升空 0.6591076489124493</span><br><span class="line">云海 0.43964818485620233</span><br></pre></td></tr></table></figure>
</li>
<li><p>词形标注与分词位置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import jieba.posseg</span><br><span class="line">text=&apos;我爱吃北京烤鸭&apos;</span><br><span class="line">for word,flag in jieba.posseg.cut(text):</span><br><span class="line">    print(word,flag)</span><br><span class="line">result=jieba.tokenize(&apos;永和服装有限公司&apos;)</span><br><span class="line">for tk in result:</span><br><span class="line">    print(tk[0],tk[1],tk[2]) # 分词，分词的起始和终止位置</span><br><span class="line"></span><br><span class="line">- - - - - - - - - - - - - - - - </span><br><span class="line">我 r</span><br><span class="line">爱 v</span><br><span class="line">吃 v</span><br><span class="line">北京烤鸭 n</span><br><span class="line">永和 0 2</span><br><span class="line">服装 2 4</span><br><span class="line">有限公司 4 8</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><ol>
<li><a href="https://blog.csdn.net/songbinxu/article/details/80209197" target="_blank" rel="noopener">自然语言处理NLP中的N-gram模型</a></li>
<li><a href="https://blog.csdn.net/bozhanggu2239/article/details/80157305" target="_blank" rel="noopener">Python的jieba分词及提取关键字</a></li>
</ol>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Python自然语言处理NLP(三)</title>
    <url>/2019/09/24/Python%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86NLP-%E4%B8%89/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://www.bilibili.com/video/av29796449/?p=16" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="原始英文文本"><a href="#原始英文文本" class="headerlink" title="原始英文文本"></a>原始英文文本</h4><ol>
<li><p>从网络上下载  -  <code>urllib.requests.urlopen</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&apos;&apos;&apos; txt在线文档下载 &apos;&apos;&apos;</span><br><span class="line">from urllib.request import urlopen</span><br><span class="line">import requests</span><br><span class="line">url=&apos;http://www.gutenberg.org/files/2554/2554-0.txt&apos;</span><br><span class="line"># response=requests.get(url)</span><br><span class="line"># print(response.text)</span><br><span class="line">response=urlopen(url)</span><br><span class="line">raw=response.read().decode(&apos;utf8&apos;) # 解码查看内容</span><br><span class="line">print(type(raw),len(raw)) # &lt;class &apos;str&apos;&gt; 1176967</span><br><span class="line">print(raw[:1000])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos; 分词 &apos;&apos;&apos;</span><br><span class="line">from nltk import word_tokenize</span><br><span class="line">tokens=word_tokenize(raw)</span><br><span class="line">print(type(tokens),len(tokens)) # &lt;class &apos;list&apos;&gt; 257727</span><br><span class="line">print(tokens[:10])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos; 创建text &apos;&apos;&apos;</span><br><span class="line">texts=nltk.Text(tokens)</span><br><span class="line">print(type(texts),len(texts)) # &lt;class &apos;nltk.text.Text&apos;&gt; 257727</span><br><span class="line">print(texts[1024:1062])</span><br><span class="line">a=texts.collocation_list(num=20) # 收集最常见的20个词组</span><br><span class="line"># print(a)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos; 根据内容定义开头与结尾 &apos;&apos;&apos;</span><br><span class="line">head=raw.find(&apos;PART I&apos;)</span><br><span class="line">tail=raw.rfind(&quot;End of Project Gutenberg’s Crime&quot;) # 查找最后一个匹配的位置</span><br><span class="line">print(head,tail) # 5336 1157812</span><br><span class="line">raw=raw[head:tail]</span><br><span class="line">print(raw.find(&apos;PART I&apos;)) # 0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;HTML 解析 &apos;&apos;&apos;</span><br><span class="line">from urllib.request import urlopen</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">url=&apos;https://news.163.com/19/0924/16/EPRPQI0H0001875O.html&apos;</span><br><span class="line">html=urlopen(url).read().decode(&apos;GBK&apos;) # 注意编码</span><br><span class="line">raw=BeautifulSoup(html,&apos;lxml&apos;).get_text() # 使用解析器解析</span><br><span class="line">tokens=word_tokenize(raw) # 获取所有分词，但是比较杂乱</span><br><span class="line">bs=BeautifulSoup(html,&apos;lxml&apos;).find(&apos;div&apos;,class_=&apos;post_text&apos;).get_text()</span><br><span class="line"># print(bs) # 找到指定模块</span><br><span class="line"># print(type(bs),len(bs)) # &lt;class &apos;str&apos;&gt; 957</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">- - - - - - - - - - - - - - - - - - - - </span><br><span class="line"> The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky</span><br><span class="line"></span><br><span class="line">This eBook is for the </span><br><span class="line">&lt;class &apos;list&apos;&gt; 257727</span><br><span class="line">[&apos;\ufeffThe&apos;, &apos;Project&apos;, &apos;Gutenberg&apos;, &apos;EBook&apos;, &apos;of&apos;, &apos;Crime&apos;, &apos;and&apos;, &apos;Punishment&apos;, &apos;,&apos;, &apos;by&apos;]</span><br><span class="line">[&apos;an&apos;, &apos;exceptionally&apos;, &apos;hot&apos;, &apos;evening&apos;, &apos;early&apos;, &apos;in&apos;, &apos;July&apos;, &apos;a&apos;, &apos;young&apos;, &apos;man&apos;, &apos;came&apos;, &apos;out&apos;, &apos;of&apos;, &apos;the&apos;, &apos;garret&apos;, &apos;in&apos;, &apos;which&apos;, &apos;he&apos;, &apos;lodged&apos;, &apos;in&apos;, &apos;S.&apos;, &apos;Place&apos;, &apos;and&apos;, &apos;walked&apos;, &apos;slowly&apos;, &apos;,&apos;, &apos;as&apos;, &apos;though&apos;, &apos;in&apos;, &apos;hesitation&apos;, &apos;,&apos;, &apos;towards&apos;, &apos;K.&apos;, &apos;bridge&apos;, &apos;.&apos;, &apos;He&apos;, &apos;had&apos;, &apos;successfully&apos;]</span><br></pre></td></tr></table></figure>
</li>
<li><p>读取本地文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fid=open(path)</span><br><span class="line">conten=fid.read()</span><br><span class="line">fid.close()</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><img src="https://www.privacypic.com/images/2019/09/24/2019-09-24-17-34-16-8f5c8a90f242de56.png" alt="2019-09-24-17-34-16-8f5c8a90f242de56.png"></p>
<ol start="3">
<li>Unicode字符<br><img src="https://www.privacypic.com/images/2019/09/24/2019-09-24-20-48-13-1d3ce4e05843be87.png" alt="2019-09-24-20-48-13-1d3ce4e05843be87.png"><br><img src="https://www.privacypic.com/images/2019/09/24/2019-09-24-20-49-05-db0f0f77154eb3f3.png" alt="2019-09-24-20-49-05-db0f0f77154eb3f3.png"></li>
</ol>
<h4 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h4><p><img src="https://www.privacypic.com/images/2019/09/24/2019-09-24-21-00-53-caffe6d8707bbf8c.png" alt="2019-09-24-21-00-53-caffe6d8707bbf8c.png"></p>
<h4 id="规范化文本"><a href="#规范化文本" class="headerlink" title="规范化文本"></a>规范化文本</h4><ol>
<li><p>词干提取器</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&apos;&apos;&apos; 词干提取器 &apos;&apos;&apos;</span><br><span class="line">a=&apos;she is a nice girl, best ever i have been meeting&apos;</span><br><span class="line">porter=nltk.PorterStemmer()</span><br><span class="line">landcaster=nltk.LancasterStemmer()</span><br><span class="line">a=a.split()</span><br><span class="line">print(a)</span><br><span class="line">print(a)</span><br><span class="line">b=[porter.stem(w) for w in a]</span><br><span class="line">c=[landcaster.stem(w) for w in a]</span><br><span class="line">print(b)</span><br><span class="line">print(c)</span><br><span class="line"></span><br><span class="line">- - - - - - - - - - - - - - -</span><br><span class="line">[&apos;she&apos;, &apos;is&apos;, &apos;a&apos;, &apos;nice&apos;, &apos;girl,&apos;, &apos;best&apos;, &apos;ever&apos;, &apos;i&apos;, &apos;have&apos;, &apos;been&apos;, &apos;meeting&apos;]</span><br><span class="line">[&apos;she&apos;, &apos;is&apos;, &apos;a&apos;, &apos;nice&apos;, &apos;girl,&apos;, &apos;best&apos;, &apos;ever&apos;, &apos;i&apos;, &apos;have&apos;, &apos;been&apos;, &apos;meet&apos;]</span><br><span class="line">[&apos;she&apos;, &apos;is&apos;, &apos;a&apos;, &apos;nic&apos;, &apos;girl,&apos;, &apos;best&apos;, &apos;ev&apos;, &apos;i&apos;, &apos;hav&apos;, &apos;been&apos;, &apos;meet&apos;]</span><br></pre></td></tr></table></figure>
</li>
<li><p>词性归并</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&apos;&apos;&apos; 词性归并 &apos;&apos;&apos;</span><br><span class="line">a=&apos;do does done&apos;</span><br><span class="line">a=a.split()</span><br><span class="line">wnl=nltk.WordNetLemmatizer()</span><br><span class="line">b=[wnl.lemmatize(w) for w in a]</span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line">- - - - - - - - - </span><br><span class="line">[&apos;do&apos;, &apos;doe&apos;, &apos;done&apos;]</span><br></pre></td></tr></table></figure>
</li>
<li><p>分割</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&apos;&apos;&apos; 断句 &apos;&apos;&apos;</span><br><span class="line"># for file in nltk.corpus.gutenberg.fileids():</span><br><span class="line">#     print(file)</span><br><span class="line">text=nltk.corpus.gutenberg.raw(&apos;chesterton-thursday.txt&apos;)</span><br><span class="line">sents=nltk.sent_tokenize(text)</span><br><span class="line">print(sents[2:5])</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos; 链表与字符串 &apos;&apos;&apos;</span><br><span class="line">a=&apos;I have a pen, I have an apple, an apple-pen&apos;</span><br><span class="line">a=a.split()</span><br><span class="line">print(a)</span><br><span class="line">a=&apos; &apos;.join(a)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">- - - - - - - - - - - - - - - - - -</span><br><span class="line">[&apos;Like the white lock of Whistler, that lit our aimless gloom,\nMen showed their own white feather as proudly as a plume.&apos;, &apos;Life was a fly that faded, and death a drone that stung;\nThe world was very old indeed when you and I were young.&apos;, &apos;They twisted even decent sin to shapes not to be named:\nMen were ashamed of honour; but we were not ashamed.&apos;]</span><br><span class="line">[&apos;I&apos;, &apos;have&apos;, &apos;a&apos;, &apos;pen,&apos;, &apos;I&apos;, &apos;have&apos;, &apos;an&apos;, &apos;apple,&apos;, &apos;an&apos;, &apos;apple-pen&apos;]</span><br><span class="line">I have a pen, I have an apple, an apple-pen</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用模拟退火进行评分<br><img src="https://www.privacypic.com/images/2019/09/24/2019-09-24-21-55-42-f36257f887431392.png" alt="2019-09-24-21-55-42-f36257f887431392.png"></p>
</li>
</ol>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Python自然语言处理NLP(二)</title>
    <url>/2019/09/23/Python%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86NLP-%E4%B8%80/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://www.bilibili.com/video/av29796449/?p=11" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="语料库"><a href="#语料库" class="headerlink" title="语料库"></a>语料库</h4><ul>
<li><p>古腾堡gutenberg</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from nltk.corpus import gutenberg</span><br><span class="line">emmma=gutenberg.words(&apos;austen-emma.txt&apos;) # j简爱</span><br><span class="line"># print(type(emmma),len(emmma)) # 192427</span><br><span class="line">for fileid in gutenberg.fileids(): # austen-emma.txt, austen-persuasion.txt, austen-sense.txt, ...</span><br><span class="line">    num_char=len(gutenberg.raw(fileid)) # sum every char</span><br><span class="line">    num_words=len(gutenberg.words(fileid)) # sum of word num</span><br><span class="line">    num_sents=len(gutenberg.sents(fileid)) # sum of sentence num</span><br><span class="line">    num_vocab=len(set([w.lower() for w in gutenberg.words(fileid)]))</span><br><span class="line">    print(fileid,&apos;:&apos;,num_char//num_words,num_words//num_sents,num_words//num_vocab)</span><br></pre></td></tr></table></figure>
</li>
<li><p>网络聊天webtext</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from nltk.corpus import webtext </span><br><span class="line">for fileid in webtext.fileids():</span><br><span class="line">    print(fileid,&apos;: &apos;,webtext.raw(fileid)[:50]) # 每种类型的网络用语内容前50个单词</span><br></pre></td></tr></table></figure>
</li>
<li><p>即时聊天语料库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from nltk.corpus import nps_chat</span><br><span class="line">for fileid in nps_chat.fileids():</span><br><span class="line">    print(fileid) # # 10-19-20s_706posts.xml, 10-19-30s_705posts.xml, 10-19-40s_686posts.xml</span><br><span class="line">chatroom=nps_chat.posts(&apos;10-19-20s_706posts.xml&apos;)</span><br><span class="line">print(type(chatroom),len(chatroom))</span><br><span class="line">print(chatroom[123]) # [&apos;i&apos;, &apos;do&apos;, &quot;n&apos;t&quot;, &apos;want&apos;, &apos;hot&apos;, &apos;pics&apos;, &apos;of&apos;, &apos;a&apos;, &apos;female&apos;, &apos;,&apos;, &apos;I&apos;, &apos;can&apos;, &apos;look&apos;, &apos;in&apos;, &apos;a&apos;, &apos;mirror&apos;, &apos;.&apos;]</span><br></pre></td></tr></table></figure>
</li>
<li><p>布朗语料库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from nltk.corpus import brown</span><br><span class="line">for fileid in brown.fileids(): # ca01, ca02, ca03, ca04, ...</span><br><span class="line">    print(fileid)</span><br><span class="line">for x in brown.categories(): # adventure, belles_lettres, editorial, fiction, government, hobbies, humor, ...</span><br><span class="line">    print(x)</span><br><span class="line">news_text1=brown.words(categories=&apos;news&apos;)</span><br><span class="line">print(news_text1) # [&apos;The&apos;, &apos;Fulton&apos;, &apos;County&apos;, &apos;Grand&apos;, &apos;Jury&apos;, &apos;said&apos;, ...]</span><br><span class="line">news_text2=brown.words(fileids=[&apos;cg22&apos;])</span><br><span class="line">print(news_text2) # [&apos;Does&apos;, &apos;our&apos;, &apos;society&apos;, &apos;have&apos;, &apos;a&apos;, &apos;runaway&apos;, &apos;,&apos;, ...]</span><br><span class="line">news_text3=brown.sents(categories=[&apos;news&apos;,&apos;editorial&apos;,&apos;reviews&apos;])</span><br><span class="line">print(news_text3)</span><br></pre></td></tr></table></figure>
</li>
<li><p>路透社语料库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from nltk.corpus import reuters</span><br><span class="line">print(len(reuters.fileids())) # 10788</span><br><span class="line">print(len(reuters.categories())) # 90</span><br></pre></td></tr></table></figure>
</li>
<li><p>就职演讲语料</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from nltk.corpus import inaugural</span><br><span class="line"></span><br><span class="line">cfd=nltk.ConditionalFreqDist(</span><br><span class="line">    (target,fileid[:4])</span><br><span class="line">    for fileid in inaugural.fileids()</span><br><span class="line">    for w in inaugural.words(fileid)</span><br><span class="line">    for target in [&apos;america&apos;,&apos;citizen&apos;]</span><br><span class="line">    if w.lower().startswith(target)</span><br><span class="line">)</span><br><span class="line">cfd.plot() # Construct a new empty conditional frequency distribution</span><br></pre></td></tr></table></figure>
<p><img src="https://www.privacypic.com/images/2019/09/23/Figure_39b8d1601601d5d69b.png" alt="Figure_39b8d1601601d5d69b.png"></p>
<h4 id="条件频率分布"><a href="#条件频率分布" class="headerlink" title="条件频率分布"></a>条件频率分布</h4><p>nltk有两种方法可以生成频率图，一种是上图所示代码，另外一种用表格形式生成<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from nltk.corpus import udhr</span><br><span class="line"></span><br><span class="line">cfd=nltk.ConditionalFreqDist(</span><br><span class="line">    (lang,len(word))</span><br><span class="line">    for lang in [&apos;English&apos;,&apos;German_Deutsch&apos;]</span><br><span class="line">    for word in udhr.words(lang+&apos;-Latin1&apos;)</span><br><span class="line">)</span><br><span class="line">cfd.tabulate(conditions=[&apos;English&apos;,&apos;German_Deutsch&apos;],samples=np.arange(10),cumulative=False)</span><br><span class="line"></span><br><span class="line">- - - - - - - - - - - - -</span><br><span class="line">                 0   1   2   3   4   5   6   7   8   9 </span><br><span class="line">       English   0 185 340 358 114 169 117 157 118  80 </span><br><span class="line">German_Deutsch   0 171  92 351 103 177 119  97 103  62</span><br></pre></td></tr></table></figure></p>
<h4 id="词典"><a href="#词典" class="headerlink" title="词典"></a>词典</h4><ul>
<li><p>停用词：高频词汇但是没有什么实际意义，如’a’,’an’,’the’ …，在实际应用中可以将这些高频词汇过滤掉，剩下一些关键词更有研究意义。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def content_fraction(text): # 提取关键词</span><br><span class="line">    stopwords = stopwords.words(&apos;english&apos;)</span><br><span class="line">    content=[w for w in text if w.lower() not in stopwords] # 过滤停用词</span><br><span class="line">    return content</span><br></pre></td></tr></table></figure>
</li>
<li><p>发音词典</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from nltk.corpus import  cmudict</span><br><span class="line"></span><br><span class="line">entries=cmudict.entries()</span><br><span class="line">#print(len(entries)) # 133737</span><br><span class="line">for entry in entries[39943:39951]:</span><br><span class="line">    print(entry)</span><br><span class="line">- - - - - - - - - - - - - - -</span><br><span class="line">(&apos;explorer&apos;, [&apos;IH0&apos;, &apos;K&apos;, &apos;S&apos;, &apos;P&apos;, &apos;L&apos;, &apos;AO1&apos;, &apos;R&apos;, &apos;ER0&apos;])</span><br><span class="line">(&apos;explorers&apos;, [&apos;IH0&apos;, &apos;K&apos;, &apos;S&apos;, &apos;P&apos;, &apos;L&apos;, &apos;AO1&apos;, &apos;R&apos;, &apos;ER0&apos;, &apos;Z&apos;])</span><br><span class="line">(&apos;explores&apos;, [&apos;IH0&apos;, &apos;K&apos;, &apos;S&apos;, &apos;P&apos;, &apos;L&apos;, &apos;AO1&apos;, &apos;R&apos;, &apos;Z&apos;])</span><br><span class="line">(&apos;exploring&apos;, [&apos;IH0&apos;, &apos;K&apos;, &apos;S&apos;, &apos;P&apos;, &apos;L&apos;, &apos;AO1&apos;, &apos;R&apos;, &apos;IH0&apos;, &apos;NG&apos;])</span><br><span class="line">(&apos;explosion&apos;, [&apos;IH0&apos;, &apos;K&apos;, &apos;S&apos;, &apos;P&apos;, &apos;L&apos;, &apos;OW1&apos;, &apos;ZH&apos;, &apos;AH0&apos;, &apos;N&apos;])</span><br><span class="line">(&apos;explosions&apos;, [&apos;IH0&apos;, &apos;K&apos;, &apos;S&apos;, &apos;P&apos;, &apos;L&apos;, &apos;OW1&apos;, &apos;ZH&apos;, &apos;AH0&apos;, &apos;N&apos;, &apos;Z&apos;])</span><br><span class="line">(&apos;explosive&apos;, [&apos;IH0&apos;, &apos;K&apos;, &apos;S&apos;, &apos;P&apos;, &apos;L&apos;, &apos;OW1&apos;, &apos;S&apos;, &apos;IH0&apos;, &apos;V&apos;])</span><br><span class="line">(&apos;explosively&apos;, [&apos;EH2&apos;, &apos;K&apos;, &apos;S&apos;, &apos;P&apos;, &apos;L&apos;, &apos;OW1&apos;, &apos;S&apos;, &apos;IH0&apos;, &apos;V&apos;, &apos;L&apos;, &apos;IY0&apos;])</span><br></pre></td></tr></table></figure>
</li>
<li><p>词汇工具<code>from nltk.corpus import toolbox</code></p>
</li>
</ul>
<h4 id="同义词"><a href="#同义词" class="headerlink" title="同义词"></a>同义词</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from nltk.corpus import wordnet as wn</span><br><span class="line">similar=wn.synsets(&apos;sweet&apos;)</span><br><span class="line">print(similar) # 查看所属同义词集</span><br><span class="line">print(wn.synset(&apos;sweet.n.01&apos;).definition()) # 查看这种词集的定义</span><br><span class="line">print(wn.synset(&apos;car.n.01&apos;).examples()) # 这种定义下的一个例句</span><br><span class="line">print(wn.synset(&apos;car.n.01&apos;).lemmas()) # 查看同义词集词条</span><br><span class="line">print(wn.lemma(&apos;car.n.01.automobile&apos;).synset()) # 查找所属同义词集</span><br><span class="line"></span><br><span class="line">- - - - - - - - - - - - - - - - - - </span><br><span class="line">[Synset(&apos;sweet.n.01&apos;), Synset(&apos;dessert.n.01&apos;), Synset(&apos;sweet.n.03&apos;), Synset(&apos;sweet.n.04&apos;), Synset(&apos;sweetness.n.02&apos;), Synset(&apos;sweet.a.01&apos;), Synset(&apos;angelic.s.03&apos;), Synset(&apos;dulcet.s.02&apos;), Synset(&apos;sweet.s.04&apos;), Synset(&apos;gratifying.s.01&apos;), Synset(&apos;odoriferous.s.03&apos;), Synset(&apos;sweet.a.07&apos;), Synset(&apos;fresh.a.06&apos;), Synset(&apos;fresh.s.09&apos;), Synset(&apos;sugared.s.01&apos;), Synset(&apos;sweetly.r.01&apos;)]</span><br><span class="line">English phonetician; one of the founders of modern phonetics (1845-1912)</span><br><span class="line">[&apos;he needs a car to get to work&apos;]</span><br><span class="line">[Lemma(&apos;car.n.01.car&apos;), Lemma(&apos;car.n.01.auto&apos;), Lemma(&apos;car.n.01.automobile&apos;), Lemma(&apos;car.n.01.machine&apos;), Lemma(&apos;car.n.01.motorcar&apos;)]</span><br><span class="line">Synset(&apos;car.n.01&apos;)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Python自然语言处理NLP(一)</title>
    <url>/2019/09/22/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86NLP-%E4%B8%80/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://www.bilibili.com/video/av29796449/?p=7" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="NLTK与NLP"><a href="#NLTK与NLP" class="headerlink" title="NLTK与NLP"></a>NLTK与NLP</h4><p><img src="https://www.privacypic.com/images/2019/09/22/2019-09-22-14-49-04-00fe412b3b518643.png" alt="2019-09-22-14-49-04-00fe412b3b518643.png"></p>
<p>NLTK设计目标：</p>
<ol>
<li>简易性</li>
<li>一致性</li>
<li>可扩展性</li>
<li>可模块化</li>
</ol>
<p><img src="https://www.privacypic.com/images/2019/09/22/2019-09-22-19-37-43-7736344b4708a885.png" alt="2019-09-22-19-37-43-7736344b4708a885.png"></p>
<h4 id="导入包"><a href="#导入包" class="headerlink" title="导入包"></a>导入包</h4><ol>
<li>首先需要安装nltk包，本文使用开发环境为<code>centos7.0 + pyhton3.5  + pycharm2019.01</code>，故可直接在解释器设置页面中安装。</li>
<li>安装成功之后就可以导入包<code>import nltk</code>了。</li>
<li>下载nltk附带语料库<code>nltk.download()</code>，不支持中文。</li>
<li>下载成功后导入与语料<code>from nltk.book import *</code></li>
<li>提示找不到某个语料库，自动在某些路径搜索失败，按提示下载到<code>nltk_data</code>文件中即可，如<code>nltk.download(&#39;gutenberg&#39;)</code></li>
</ol>
<h4 id="使用NLTK语料"><a href="#使用NLTK语料" class="headerlink" title="使用NLTK语料"></a>使用NLTK语料</h4><p><a href="https://www.privacypic.com/image/w6mpB" target="_blank" rel="noopener"><img src="https://www.privacypic.com/images/2019/09/22/2019-09-22-15-14-33-b5fb3c03d43fab53.png" alt="2019-09-22-15-14-33-b5fb3c03d43fab53.png"></a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from nltk.book import * # 导入语料库</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos; search word/text &apos;&apos;&apos;</span><br><span class="line">text1.concordance(&apos;monstrous&apos;) # 按句子查找</span><br><span class="line">print(text5.count(&apos;lol&apos;)) # 704</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos; search similary word &apos;&apos;&apos;</span><br><span class="line">text1.similar(&apos;monstrous&apos;,num=20) # 找到相同上下文或者意思相近的词语,指定数量</span><br><span class="line">text2.similar(&apos;monstrous&apos;)</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos; 搜索共同上下文 &apos;&apos;&apos; # Find contexts where the specified words appear; list most frequent common contexts first.</span><br><span class="line">text2.common_contexts([&apos;monstrous&apos;,&apos;very&apos;]) # a_pretty a_lucky be_glad am_glad is_pretty</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos; word distribute &apos;&apos;&apos; # Produce a plot showing the distribution of the words through the text</span><br><span class="line">text4.dispersion_plot([&apos;citizen&apos;,&apos;democracy&apos;,&apos;freedom&apos;,&apos;duties&apos;,&apos;America&apos;])</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos; generate text &apos;&apos;&apos; # 自动生成文章，在较新的版本中没有这个方法</span><br><span class="line">text3.generate() # Print random text, generated using a trigram language model</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos; 计数词汇 &apos;&apos;&apos;</span><br><span class="line">print(len(text3))</span><br><span class="line">print(sorted(set(text3))) # 去重排序</span><br><span class="line">print(len(set(text3)))</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos; 重复词密度 &apos;&apos;&apos;</span><br><span class="line">print(text3.count(&apos;smote&apos;))</span><br><span class="line">print(len(text3)/len(set(text3)))</span><br></pre></td></tr></table></figure>
<p><img src="https://www.privacypic.com/images/2019/09/22/Figure_3635e1b0a3c548e9c5.png" alt="Figure_3635e1b0a3c548e9c5.png"></p>
<h4 id="词链表"><a href="#词链表" class="headerlink" title="词链表"></a>词链表</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(sent1,sent2)</span><br><span class="line">print(sent1+sent2)</span><br><span class="line">print(type(sent1)) # &lt;class &apos;list&apos;&gt;</span><br><span class="line"></span><br><span class="line">- - - - - - - - - - - - - - - - - - - - - </span><br><span class="line">[&apos;Call&apos;, &apos;me&apos;, &apos;Ishmael&apos;, &apos;.&apos;] [&apos;The&apos;, &apos;family&apos;, &apos;of&apos;, &apos;Dashwood&apos;, &apos;had&apos;, &apos;long&apos;, &apos;been&apos;, &apos;settled&apos;, &apos;in&apos;, &apos;Sussex&apos;, &apos;.&apos;]</span><br><span class="line">[&apos;Call&apos;, &apos;me&apos;, &apos;Ishmael&apos;, &apos;.&apos;, &apos;The&apos;, &apos;family&apos;, &apos;of&apos;, &apos;Dashwood&apos;, &apos;had&apos;, &apos;long&apos;, &apos;been&apos;, &apos;settled&apos;, &apos;in&apos;, &apos;Sussex&apos;, &apos;.&apos;]</span><br><span class="line">&lt;class &apos;list&apos;&gt;</span><br></pre></td></tr></table></figure>
<p>可以看到sentence是list类型，那么就可以对其做list支持的各种操作(索引，切片，增删改查等)。</p>
<h4 id="简单统计"><a href="#简单统计" class="headerlink" title="简单统计"></a>简单统计</h4><ul>
<li>频率分布</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fdist1=FreqDist(sent3) # 统计各个单词频数，生成字典，按频数降序</span><br><span class="line">for key in fdist1:</span><br><span class="line">    print(key,&apos;:&apos;,fdist1[key])</span><br><span class="line"># print(fdist1.keys())</span><br><span class="line">fdist1.plot(5,cumulative=False) # Plot samples from the frequency distribution displaying the most frequent sample first，指定频率前5个的单词，cumulative表示是否累计</span><br><span class="line">print(fdist1.hapaxes()) # 查看低频词只出现一次的词</span><br><span class="line"></span><br><span class="line">fdist2=FreqDist(sent3)</span><br><span class="line">print(fdist2.items())</span><br><span class="line">print(fdist2.max()) # 频数最多的单词</span><br><span class="line"></span><br><span class="line">- - -  - - - -  - </span><br><span class="line">the : 3  </span><br><span class="line">In : 1  </span><br><span class="line">created : 1  </span><br><span class="line">. : 1  </span><br><span class="line">and : 1  </span><br><span class="line">beginning : 1 </span><br><span class="line">God : 1   </span><br><span class="line">earth : 1  </span><br><span class="line">heaven : 1  </span><br><span class="line">[&apos;earth&apos;, &apos;In&apos;, &apos;and&apos;, &apos;beginning&apos;, &apos;created&apos;, &apos;.&apos;, &apos;heaven&apos;, &apos;God&apos;]</span><br><span class="line">dict_items([(&apos;In&apos;, 1), (&apos;created&apos;, 1), (&apos;and&apos;, 1), (&apos;earth&apos;, 1), (&apos;God&apos;, 1), (&apos;beginning&apos;, 1), (&apos;the&apos;, 3), (&apos;.&apos;, 1), (&apos;heaven&apos;, 1)])</span><br><span class="line">the</span><br></pre></td></tr></table></figure>
<p><img src="https://www.privacypic.com/images/2019/09/22/Figure_373b18e3b653cdaa70.png" alt="Figure_373b18e3b653cdaa70.png"></p>
<ul>
<li>细粒度的选择词<br>a. { w | w &isin;V &and; P(w) }<br>b. [ w for w in V if P(w) ] </li>
</ul>
<ul>
<li>词语搭配</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from nltk.book import *</span><br><span class="line">from nltk.util import bigrams</span><br><span class="line"></span><br><span class="line">print(list(bigrams([&apos;more&apos;,&apos;is&apos;,&apos;said&apos;,&apos;than&apos;,&apos;done&apos;])))</span><br><span class="line">a=text2.collocation_list(num=20) # Print collocations derived from the text, ignoring stopwords.</span><br><span class="line">print(a) # 收集最常见的词组</span><br><span class="line"></span><br><span class="line">- - - - -  - - - - - </span><br><span class="line">[(&apos;more&apos;, &apos;is&apos;), (&apos;is&apos;, &apos;said&apos;), (&apos;said&apos;, &apos;than&apos;), (&apos;than&apos;, &apos;done&apos;)]</span><br><span class="line">[&apos;Colonel Brandon&apos;, &apos;Sir John&apos;, &apos;Lady Middleton&apos;, &apos;Miss Dashwood&apos;, &apos;every thing&apos;, &apos;thousand pounds&apos;, &apos;dare say&apos;, &apos;Miss Steeles&apos;, &apos;said Elinor&apos;, &apos;Miss Steele&apos;, &apos;every body&apos;, &apos;John Dashwood&apos;, &apos;great deal&apos;, &apos;Harley Street&apos;, &apos;Berkeley Street&apos;, &apos;Miss Dashwoods&apos;, &apos;young man&apos;, &apos;Combe Magna&apos;, &apos;every day&apos;, &apos;next morning&apos;]</span><br></pre></td></tr></table></figure>
<p><img src="https://www.privacypic.com/images/2019/09/22/2019-09-22-19-34-35-54af257f4020f8ee.png" alt="2019-09-22-19-34-35-54af257f4020f8ee.png"></p>
<h4 id="关于字符串转义问题"><a href="#关于字符串转义问题" class="headerlink" title="关于字符串转义问题"></a>关于字符串转义问题</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">s=&apos;123\n456&apos;</span><br><span class="line">print(s)</span><br><span class="line">print(s.replace(&apos;\n&apos;,r&apos;\n&apos;))</span><br><span class="line">print(&apos;\\&apos;)</span><br><span class="line">ss=&apos;123\\456&apos;</span><br><span class="line">print(ss.replace(&apos;\\&apos;,r&apos;\\&apos;))</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;123</span><br><span class="line">&gt;&gt;&gt;456</span><br><span class="line">&gt;&gt;&gt;123\n456</span><br><span class="line">&gt;&gt;&gt;\</span><br><span class="line">&gt;&gt;&gt;123\\456</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>吴恩达机器学习第七次编程作业-K-means Clustering and Principal Component Analysis</title>
    <url>/2019/09/16/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%83%E6%AC%A1%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/</url>
    <content><![CDATA[<h3 id="Programming-Exercise-7-K-means-Clustering-and-Principal-Component-Analysis"><a href="#Programming-Exercise-7-K-means-Clustering-and-Principal-Component-Analysis" class="headerlink" title=" Programming Exercise 7:   K-means Clustering and Principal Component Analysis"></a><center> <a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/text?lessonId=1053422072&amp;courseId=1004570029" target="_blank" rel="noopener">Programming Exercise 7:   K-means Clustering and Principal Component Analysis</a></center></h3><hr>
<p>环境： Python3.5 , Pycharm2019.01</p>
<p>参考资料：</p>
<ol>
<li><a href="https://www.cnblogs.com/Allen-rg/p/9606824.html" target="_blank" rel="noopener">numpy.argmin 使用</a></li>
<li><a href="https://blog.csdn.net/kingroc/article/details/73088510" target="_blank" rel="noopener">Python绘制点线</a></li>
<li><a href="https://blog.csdn.net/Cherish_x/article/details/90550001" target="_blank" rel="noopener">第七次作业：k-means算法</a></li>
<li><a href="https://www.cnblogs.com/twilight77/p/7675512.html" target="_blank" rel="noopener">Python 玩转随机数</a></li>
<li><a href="https://www.cnblogs.com/MIS-67/p/9912505.html" target="_blank" rel="noopener">K-means算法应用：图片压缩</a></li>
<li><a href="https://www.cnblogs.com/DOLFAMINGO/p/9362637.html" target="_blank" rel="noopener">吴恩达机器学习笔记 — 降维与主成分分析法(PCA)
</a></li>
<li><a href="https://blog.csdn.net/a10767891/article/details/80288463" target="_blank" rel="noopener">PCA 原理：为什么用协方差矩阵</a></li>
<li><a href="https://blog.csdn.net/babywong/article/details/50085239" target="_blank" rel="noopener">PCA为什么要用协方差矩阵？</a></li>
<li><a href="https://www.cnblogs.com/pinard/p/6251584.html" target="_blank" rel="noopener">奇异值分解(SVD)原理与在降维中的应用</a></li>
<li><a href="https://blog.csdn.net/cowry5/article/details/80503380" target="_blank" rel="noopener">吴恩达机器学习作业Python实现(七)</a></li>
</ol>
<p><a href="https://paste.ubuntu.com/p/CJncKmWdwc/" target="_blank" rel="noopener">K-means Clustering完整代码</a><br><a href="https://paste.ubuntu.com/p/Kq9F5bKmcz/" target="_blank" rel="noopener">Principal Component Analysis完整代码</a></p>
<hr>
<h3 id="Part-1-K-Means-Clustering"><a href="#Part-1-K-Means-Clustering" class="headerlink" title="Part 1:  K-Means Clustering"></a>Part 1:  K-Means Clustering</h3><h4 id="实现K-Means算法"><a href="#实现K-Means算法" class="headerlink" title="实现K-Means算法"></a>实现K-Means算法</h4><h5 id="Finding-closest-centroids"><a href="#Finding-closest-centroids" class="headerlink" title="Finding closest centroids"></a>Finding closest centroids</h5><p>根据所给出的簇中心将所有的样本点进行标记，返回一维数组代表每个样本所属的簇，簇编号从1开始。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Find_Closets_Centroid(X,centroid):</span><br><span class="line">    idx=np.zeros(X.shape[0],dtype=int)</span><br><span class="line">    for i in range(X.shape[0]):</span><br><span class="line">        tmp_x=(X[i]-centroid)**2</span><br><span class="line">        idx[i]=np.argmin(np.sum(tmp_x,axis=1)) # 先按行相加得到[1,3]的距离数组，再找出最小值下标</span><br><span class="line">    return idx  # 下标从0开始</span><br><span class="line">    </span><br><span class="line">K=3 # 簇中心数量</span><br><span class="line">init_centroid=[[3,3],[6,2],[8,5]] </span><br><span class="line">idx=Find_Closets_Centroid(X,init_centroid)</span><br><span class="line"># print(idx[:3]) # except to see [0 2 1]</span><br></pre></td></tr></table></figure></p>
<h5 id="Computing-centroid-means"><a href="#Computing-centroid-means" class="headerlink" title="Computing centroid means"></a>Computing centroid means</h5><p>既然每个样本都有一个所属簇，那么分别计算这些不同簇的样本点集的均值作为新的簇中心。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Computer_Centroids(X,idx,K):</span><br><span class="line">    centroids=np.zeros((K,X.shape[1]))</span><br><span class="line">    for i in range(K):</span><br><span class="line">        centroids[i]=np.mean(X[np.where(idx==i)],axis=0) # 先将样本点集区分出来成为新的二维数组，再按列取均值</span><br><span class="line">    return centroids</span><br><span class="line"></span><br><span class="line">centroids=Computer_Centroids(X,idx,K)</span><br><span class="line">print(centroids) # except to see [[2.42830111 3.15792418],[5.81350331 2.63365645],[7.11938687 3.6166844 ]]</span><br></pre></td></tr></table></figure></p>
<h4 id="K-means-on-example-dataset"><a href="#K-means-on-example-dataset" class="headerlink" title="K-means on example dataset"></a>K-means on example dataset</h4><p>手写K-Means算法，观察簇中心在数据集上的变化情况。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Record_Centroid(i,centroids,history_centroids): # 记录簇中心变化</span><br><span class="line">    history_centroids_tmp = centroids.copy()</span><br><span class="line">    history_centroids[0][i] = history_centroids_tmp[0]</span><br><span class="line">    history_centroids[1][i] = history_centroids_tmp[1]</span><br><span class="line">    history_centroids[2][i] = history_centroids_tmp[2]</span><br><span class="line">    return history_centroids</span><br><span class="line"></span><br><span class="line">def kMeans(X,K,init_centroid,max_iter): # K-Means算法运行过程</span><br><span class="line">    centroids=init_centroid</span><br><span class="line">    history_centroids=np.zeros((K,max_iter,X.shape[1])) # 申请三维数组记录K个簇变化</span><br><span class="line">    idx=np.zeros(X.shape[0],dtype=int)</span><br><span class="line">    for i in range(max_iter):</span><br><span class="line">        idx=Find_Closets_Centroid(X,centroids)</span><br><span class="line">        history_centroids=Record_Centroid(i,centroids,history_centroids)</span><br><span class="line">        centroids=Computer_Centroids(X,idx,K) # new centroids</span><br><span class="line">    return centroids,idx,history_centroids</span><br><span class="line"></span><br><span class="line">def Plot_kMeans_Progress(X,history_centroids,idx,K): # 观察K-Means簇中心变化</span><br><span class="line">    cluster1=X[np.where(idx==0)] # 最终所属的簇</span><br><span class="line">    cluster2 = X[np.where(idx == 1)]</span><br><span class="line">    cluster3 = X[np.where(idx == 2)]</span><br><span class="line">    plt.plot(cluster1[:,0],  cluster1[:,1],&apos;r.&apos;, label=&apos;Cluster 1&apos;)</span><br><span class="line">    plt.plot(cluster2[:, 0], cluster2[:, 1],&apos;g.&apos;, label=&apos;Cluster 2&apos;)</span><br><span class="line">    plt.plot(cluster3[:, 0], cluster3[:, 1],&apos;b.&apos;, label=&apos;Cluster 3&apos;)</span><br><span class="line">    plt.plot(history_centroids[0][:,0],  history_centroids[0][:,1],&apos;kX-&apos;,alpha=0.5) # alpha设置透明度</span><br><span class="line">    plt.plot(history_centroids[1][:, 0], history_centroids[1][:, 1],&apos;kX-&apos;,alpha=0.5)</span><br><span class="line">    plt.plot(history_centroids[2][:, 0], history_centroids[2][:, 1],&apos;kX-&apos;,alpha=0.5)</span><br><span class="line">    plt.xlim(-1,9)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.title(&apos;Iteration number 10&apos;,fontsize=12)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://www.privacypic.com/images/2019/09/16/Figure_27a503542666fa1765.png" alt="Figure_27a503542666fa1765.png"></p>
<h4 id="Random-initialization"><a href="#Random-initialization" class="headerlink" title="Random initialization"></a>Random initialization</h4><p>给定样本集和簇中心数量随机选取中心簇作为初始化。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Random_Init_Centroids(X,K):</span><br><span class="line">    rand_idx=np.arange(X.shape[0]) # 生成排列</span><br><span class="line">    np.random.shuffle(rand_idx)  # 随机打乱，每次都不一样</span><br><span class="line">    return X[rand_idx[:K]] # 返回前K个样本点作为簇中心</span><br></pre></td></tr></table></figure></p>
<h4 id="Image-compression-with-K-means"><a href="#Image-compression-with-K-means" class="headerlink" title="Image compression with K-means"></a>Image compression with K-means</h4><p>使用K-Means算法将所给的图片进行压缩。<br>原理：</p>
<ul>
<li>关于图片：这里所使用的是png格式的彩图，即每个像素点24位，每8位分别代表RGB。像素点越多越清晰，这里图片大小为128*128，代表长宽也代表像素点个数。</li>
<li>使用plt.imread(picture_path)读入图片，将二进制信息以三维矩阵形式读出(128,128,3)。</li>
<li>重塑图片，使其次成为二维[128*128,3]，然后每行代表一个像素点，接着使用K-Means算法找到16个簇中心，然后用这些簇中心的值代替其所属簇的像素点的值。这样就将24位压缩成了16位，然后再重塑回三维[128,128,3]，显示效果对比。</li>
<li>使用不同的K，观察效果</li>
<li>A=A/255是特征缩放，以便计算距离，最终显示还要乘上255，还是24位</li>
</ul>
<p>为什么能实现压缩？</p>
<ul>
<li>之前说的像素点每个点都有一个值，范围是[0-255]，实际上压缩上述代码实现压缩之后图片大小貌似并没有变化，那么做这个意义何在？</li>
<li>其实，原图每个像素点可以看成一种颜色，我们做的事就是找出16种主要的颜色代替其余颜色。值的范围没有变，但我们就可以改变之前像素点所存储的信息了，我们先在只要4bit就可以知道这个像素点所属的簇，那么再找到这个簇的颜色就是这个像素点的颜色了。</li>
<li>故，我们需要额外的一点点空间存储簇的颜色，然后(以下面图片为例)图片大小就可以用<code>128*128*4</code>替换，而不是<code>218*128*24</code>。有种类似时间换空间的感觉。</li>
<li>下面代码也就是实现了完整的图像压缩核心部分以显示效果，并没有实际压缩图片。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Clustering_On_Pixels(path,K,max_iter):</span><br><span class="line">    A=plt.imread(path)</span><br><span class="line">    # print(type(A),A.shape) # except to see: &lt;class &apos;numpy.ndarray&apos;&gt; (128, 128, 3)</span><br><span class="line">    A=A/255 # Divide by 255 so that all values are in the range 0 - 1</span><br><span class="line">    X=A.reshape(A.shape[0]*A.shape[1],3)  # [128*128,3]</span><br><span class="line">    init_centroids=Random_Init_Centroids(X,K) # 初始化簇中心</span><br><span class="line">    centroids,idx,history=kMeans(X,K,init_centroids,max_iter) # 迭代找到16个簇中心</span><br><span class="line">    return A,X,centroids</span><br><span class="line"></span><br><span class="line">def Image_Compression(path):</span><br><span class="line">    K, max_iter = 16, 10  # try different values of K and max_iters here</span><br><span class="line">    A,X,centroids=Clustering_On_Pixels(path,K,max_iter)</span><br><span class="line">    idx=Find_Closets_Centroid(X,centroids)</span><br><span class="line">    X_recovered=centroids[idx] # 值代替</span><br><span class="line">    X_recovered=X_recovered.reshape(A.shape[0],A.shape[1],3) # 重塑回三维</span><br><span class="line">    plt.subplot(1,2,1)</span><br><span class="line">    plt.imshow(A*255)</span><br><span class="line">    plt.title(&apos;Original&apos;)</span><br><span class="line">    plt.subplot(1, 2, 2)</span><br><span class="line">    plt.imshow(X_recovered*255)</span><br><span class="line">    plt.title(&apos;Compressed, with %d colors&apos; %K,fontsize=10)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">Image_Compression(&apos;./ML/ex7/bird_small.png&apos;)</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/09/18/jLAaXp9EtyIukqC.png" alt="Figure_28.png"><br><img src="https://i.loli.net/2019/09/18/wDglFhRmxJnaKMq.png" alt="Figure_29.png"> </p>
<h3 id="Part2-Principal-Component-Analysis"><a href="#Part2-Principal-Component-Analysis" class="headerlink" title="Part2: Principal Component Analysis"></a>Part2: Principal Component Analysis</h3><h4 id="2-1-Example-Dataset"><a href="#2-1-Example-Dataset" class="headerlink" title="2.1 Example Dataset"></a>2.1 Example Dataset</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def  Get_data(path):</span><br><span class="line">    data=sio.loadmat(path)</span><br><span class="line">    # for key in data:</span><br><span class="line">    #     print(key)</span><br><span class="line">    return data,data[&apos;X&apos;]</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos; Visualizing example dataset for PCA &apos;&apos;&apos;</span><br><span class="line">data,X=Get_data(&apos;./ML/ex7/ex7data1.mat&apos;)</span><br><span class="line"># print(X.shape) # (50,2)</span><br><span class="line">plt.scatter(X[:,0], X[:,1], facecolors=&apos;none&apos;, edgecolors=&apos;b&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/09/18/TGYjgWk5QDCtlUq.png" alt="Figure_30.png"></p>
<h4 id="2-2-Implementing-PCA"><a href="#2-2-Implementing-PCA" class="headerlink" title="2.2 Implementing PCA"></a>2.2 Implementing PCA</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Feature_Normalize(X):</span><br><span class="line">    means = np.mean(X, axis=0)  # the mean of the each feature</span><br><span class="line">    std = np.std(X, axis=0, ddof=1)  # 取每列标准偏差</span><br><span class="line">    return (X-means)/std,means</span><br><span class="line"></span><br><span class="line">def PCA(X):</span><br><span class="line">    sigma=(X.T@X)/X.shape[0] # covariance matrix</span><br><span class="line">    U,S,V=np.linalg.svd(sigma) # wait to learn</span><br><span class="line">    return U,S,V</span><br><span class="line"></span><br><span class="line">X_normal,means=Feature_Normalize(X) # normalize X</span><br><span class="line">U,S,V=PCA(X_normal)</span><br><span class="line"># print(means.shape,S.shape,U.shape)</span><br><span class="line"># print(U[:,0]) # except to  see: [-0.70710678 -0.70710678]</span><br><span class="line">plt.plot([means[0], means[0] + 1.5*S[0]*U[0,0]], [means[1], means[1] + 1.5*S[0]*U[0,1]],c=&apos;r&apos;, linewidth=3, label=&apos;First Principal Component&apos;)</span><br><span class="line">plt.plot([means[0], means[0] + 1.5*S[1]*U[1,0]], [means[1], means[1] + 1.5*S[1]*U[1,1]],c=&apos;g&apos;, linewidth=3, label=&apos;Second Principal Component&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/09/18/BpEvQxb6NChtocg.png" alt="Figure_31.png"> </p>
<h4 id="Dimensionality-Reduction-with-PCA"><a href="#Dimensionality-Reduction-with-PCA" class="headerlink" title="Dimensionality Reduction with PCA"></a>Dimensionality Reduction with PCA</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Project_Data(X,U,K): # 数据投影</span><br><span class="line">    return X@U[:,:K]</span><br><span class="line">def Recover_Data(Z,U,K): # 数据重构</span><br><span class="line">    return Z@U[:,:K].T</span><br><span class="line">def Visualizing_Projections(X_normal,X_rec):</span><br><span class="line">    plt.scatter(X_normal[:, 0], X_normal[:, 1], facecolors=&apos;none&apos;, edgecolors=&apos;b&apos;, label=&apos;X_normal&apos;)</span><br><span class="line">    plt.scatter(X_rec[:, 0], X_rec[:, 1], facecolors=&apos;none&apos;, edgecolors=&apos;r&apos;, label=&apos;X_rec&apos;)</span><br><span class="line">    plt.xlim(-4, 3)</span><br><span class="line">    plt.ylim(-4, 3)</span><br><span class="line">    for i in range(X_normal.shape[0]):</span><br><span class="line">        plt.plot([X_normal[i, 0], X_rec[i, 0]], [X_normal[i, 1], X_rec[i, 1]], &apos;k--&apos;)</span><br><span class="line">    plt.legend(loc=2)</span><br><span class="line">    plt.grid(True)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos; Dimensionality Reduction with PCA &apos;&apos;&apos;</span><br><span class="line">Z=Project_Data(X_normal,U,1) # Projecting the data onto the principal components</span><br><span class="line"># print(Z.shape,Z[0]) # except to see: (50, 1) [1.48127391]</span><br><span class="line">X_rec=Recover_Data(Z,U,1)</span><br><span class="line"># print(X_rec.shape,X_rec[0]) # (50, 2) [-1.04741883 -1.04741883]</span><br><span class="line">Visualizing_Projections(X_normal,X_rec)</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/09/18/efmpULIPTAQ7Nrd.png" alt="Figure_32.png"></p>
<h4 id="Face-Image-Dataset"><a href="#Face-Image-Dataset" class="headerlink" title="Face Image Dataset"></a>Face Image Dataset</h4><h5 id="Faces-dataset"><a href="#Faces-dataset" class="headerlink" title="Faces dataset"></a>Faces dataset</h5><p><code>5000*1024</code>的灰度图像，在显示的时候重塑乘<code>32*32</code>的就行。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Show_Face(X,num):</span><br><span class="line">    plt.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9,hspace=0.02, wspace=0.02)</span><br><span class="line">    for i in range(num):</span><br><span class="line">        plt.subplot(10,10,i+1,xticks=[], yticks=[]) # 不显示刻度</span><br><span class="line">        plt.imshow(X[i].reshape(32,32).T,cmap = &apos;Greys_r&apos;,interpolation=&apos;nearest&apos;) # 指定为灰度，默认RGB</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://i.loli.net/2019/09/18/x2pArQO7HLsEDgF.png" alt="Figure_33.png"></p>
<h5 id="PCA-on-Faces"><a href="#PCA-on-Faces" class="headerlink" title="PCA on Faces"></a>PCA on Faces</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data,X=Get_data(&apos;./ML/ex7/ex7faces.mat&apos;)</span><br><span class="line"># Show_Face(X,100)</span><br><span class="line">X_noraml,means=Feature_Normalize(X)</span><br><span class="line">U,S,V=PCA(X_noraml)</span><br><span class="line"># print(U.shape) # (1024,1024)</span><br><span class="line">Show_Face(U.T,36) # change subplot=6*6</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/09/18/iHpLlSZszIJeqW9.png" alt="Figure_34.png"></p>
<h5 id="Reconstruct-Data"><a href="#Reconstruct-Data" class="headerlink" title="Reconstruct Data"></a>Reconstruct Data</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Z=Project_Data(X_noraml,U,100) # Dimensionality Reduction</span><br><span class="line"># print(Z.shape) # (5000, 100)</span><br><span class="line">X_rec=Recover_Data(Z,U,100)</span><br><span class="line"># print(X_rec.shape) # (5000, 1024)</span><br><span class="line"># Show_Face(X_noraml,100)</span><br><span class="line">Show_Face(X_rec,100)</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/09/18/MwynF8Tb72ex5pV.png" alt="Figure_35.png"></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>作业</tag>
      </tags>
  </entry>
  <entry>
    <title>降维</title>
    <url>/2019/09/15/%E9%99%8D%E7%BB%B4/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1052316982&amp;courseId=1004570029" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><p><strong> 数据压缩</strong>： </p>
<ul>
<li>简单来说样本的特征维数可能很多，这样造成数据量特别大，在实际应用中我们可能只侧重与某些特征就能得出想要的结果了；而我们不太侧重的那些特征就可以删除，减轻工作量，提高效率。  </li>
<li>还有一种，就像想方设法用更适合可取的数据代替当前样本，达到同样的目的，比如对内存的需求，运算量等。</li>
<li>实例： 2D压缩到1D，3D压缩到2D</li>
</ul>
<p><strong> 可视化数据 </strong>：</p>
<ul>
<li>假如数据特征维数很高，通常不太可能作出简单的图来理解，故需要选取某些维数进行作图，或者，自己使用某种方法将原来的高维数数据转化为低维数据再进行作图，以便理解。</li>
</ul>
<h4 id="主成份分析法PCA"><a href="#主成份分析法PCA" class="headerlink" title="主成份分析法PCA"></a>主成份分析法PCA</h4><h5 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h5><p>PCA算法： Principal Component Analysis<br> 试图找出一个低维平面，使得所有数据都可以投影在上面，并且投影误差平方最小化，用新的维度空间代替原来高维空间，达到降维的目的。<br> <img src="https://i.loli.net/2019/09/15/MQstVbeuDjzqyI9.png" alt="2019-09-15 15-24-39 的屏幕截图.png"></p>
<p> PCA并不是线形回归，线形回归是拟合一条直线(在二维特征集中)使得样本到这条直线的平方误差最小；而PCA是使得样本到这条直线的投影距离最小。在三维数据集中，PCA尝试的是找出一个二维平面，使得所有样本数据投影到这个平面的投影距离最小，那么就可以用这个二维平面来表示这些数据。</p>
<h5 id="应用-1"><a href="#应用-1" class="headerlink" title="应用"></a>应用</h5><p>首先需要进行数据预处理：特征缩放或均值标准化<br><img src="https://i.loli.net/2019/09/15/tGOTLs2WbYDVKIP.png" alt="2019-09-15 16-01-43 的屏幕截图.png"><br> S<sub>j</sub>是特征j的标准偏差</p>
<p>然后，使用主成份分析法进行降维映射。<br>主成份分析法需要用到一些高等代数如奇异值分解等运算和证明。</p>
<h4 id="主成份数量选择"><a href="#主成份数量选择" class="headerlink" title="主成份数量选择"></a>主成份数量选择</h4><p><img src="https://i.loli.net/2019/09/15/GeznbWclXRayBZr.png" alt="2019-09-15 16-27-38 的屏幕截图.png"><br><img src="https://i.loli.net/2019/09/15/nmhIb6qGTKsyr7V.png" alt="2019-09-15 16-32-40 的屏幕截图.png"><br><img src="https://i.loli.net/2019/09/15/psqnHTgktYfd5ea.png" alt="2019-09-15 16-33-30 的屏幕截图.png"></p>
<h4 id="压缩重现"><a href="#压缩重现" class="headerlink" title="压缩重现"></a>压缩重现</h4><p><img src="https://i.loli.net/2019/09/15/klvfAF8TweaobuB.png" alt="2019-09-15 16-37-10 的屏幕截图.png"></p>
<h4 id="应用PCA的建议"><a href="#应用PCA的建议" class="headerlink" title="应用PCA的建议"></a>应用PCA的建议</h4><ol>
<li>不建议使用PCA防止过拟合。   </li>
<li>不建议项目一开始就使用PCA，而是先按普通方法去做，再考虑PCA优化有什么不同</li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>无监督学习： K-Means</title>
    <url>/2019/09/14/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1052194132&amp;courseId=1004570029" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><ul>
<li>在监督学习中，我们所使用到的样本都是带有<strong>标签</strong>的，我们需要一个假设函数和决策线来进行分类，也就是说监督学习是有期望输出的，并且可以和实际值比较比较。   </li>
<li>在无监督学习中，样本不在具有标签，只是一堆散落的值而已。我们要做的是设计或者使用算法，并将这些无标签的数据输入到算法中，期望算法能够找出一些隐含在数据中的结构。如聚类算法，见下图<br><img src="https://i.loli.net/2019/09/14/pwXaWET4PUuhA5N.png" alt="2019-09-14 16-30-10 的屏幕截图.png"> </li>
</ul>
<h4 id="K-Means算法，最广泛运用的聚类算法"><a href="#K-Means算法，最广泛运用的聚类算法" class="headerlink" title="K-Means算法，最广泛运用的聚类算法"></a>K-Means算法，最广泛运用的聚类算法</h4><h5 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h5><p><img src="https://i.loli.net/2019/09/14/ljUJbHhMfi1rOIT.png" alt="2019-09-14 16-48-28 的屏幕截图.png"><br>如何将上图样本分类两个簇(cluster)，实际上簇的数量即K-Means中的K，后面会介绍如何选择K。  </p>
<p>这里，我们先随机两个点X作为簇中心 ， 遍历所有点计算离这两个点的距离并归类<br><img src="https://i.loli.net/2019/09/14/A5aDZXtxUn6vGuy.png" alt="2019-09-14 16-53-31 的屏幕截图.png"></p>
<p>然后计算红色点簇的中心和蓝色点簇的中心，将之前的簇中心移动到新的中心位置。重复上述操作，直到簇中心不再改变。</p>
<h5 id="K-Means算法输入"><a href="#K-Means算法输入" class="headerlink" title="K-Means算法输入"></a>K-Means算法输入</h5><p>两个参数：K和X，K表示簇的数量，X为n维向量，不再需要x<sub>0</sub>=1<br><img src="https://i.loli.net/2019/09/14/nFMzIRck9oANmdj.png" alt="2019-09-14 17-01-33 的屏幕截图.png"> </p>
<h5 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h5><p><img src="https://i.loli.net/2019/09/14/FUoHPBqsw9gRSbI.png" alt="2019-09-14 17-09-15 的屏幕截图.png"><br>两个循环分别表示例子中所述的两个步骤。<br>c<sup>(i)</sup>表示第i个样本所属的簇索引编号。u<sub>k</sub>表示簇中心。<br>可能会出线的一种情况，某个簇中心所在的集合为空，这样可以直接移除这个簇中心。</p>
<h5 id="K-Means算法解决分离不佳的簇问题"><a href="#K-Means算法解决分离不佳的簇问题" class="headerlink" title="K-Means算法解决分离不佳的簇问题"></a>K-Means算法解决分离不佳的簇问题</h5><p>衣服尺寸设计问题<br><img src="https://i.loli.net/2019/09/14/yA8QqkVBMtToOZK.png" alt="2019-09-14 17-17-11 的屏幕截图.png"></p>
<h4 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h4><p>最小化所有的样本点到其所属的簇中心距离之和<br><img src="https://i.loli.net/2019/09/15/1KGYF3tdIa4Bzkc.png" alt="2019-09-15 10-44-01 的屏幕截图.png"></p>
<h4 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h4><p>K-Means算法第一步就是如何选取K个簇中心，避免局部最优。  </p>
<ul>
<li>首先，不难理解聚类中心K的数量应该小于训练样本数量m，这里假设K给定</li>
<li>随机选取K个训练样本作为簇中心&mu;<sub>1</sub> … &mu;<sub>k</sub></li>
<li>运行K-Means算法计算代价函数J</li>
<li>重复上述操作100次或者10次，迭代次数根据训练样本数而定；找出代价函数最小的那次。<br><img src="https://i.loli.net/2019/09/15/vnPxB1rZMwyhieF.png" alt="2019-09-15 11-00-27 的屏幕截图.png"></li>
</ul>
<h4 id="选取聚类数量"><a href="#选取聚类数量" class="headerlink" title="选取聚类数量"></a>选取聚类数量</h4><p>肘部法则： 改变K，计算最优代价函数，画图（代价函数关于K的变化）,找到那个肘部即拐点，之后变化平缓了。<br><img src="https://i.loli.net/2019/09/15/CsONdD5vplfyFQc.png" alt="2019-09-15 11-14-48 的屏幕截图.png"></p>
<p>但，大多数时候我们会得到右图的结果，这并不好决定。需要自己决定分类的目的是什么。</p>
<hr>
<center><font size="5px" color="red">补充</font></center>

<hr>
<h3 id="sklearn中的聚类算法K-Means"><a href="#sklearn中的聚类算法K-Means" class="headerlink" title="sklearn中的聚类算法K-Means"></a><font size="5px" color="red">sklearn中的聚类算法K-Means</font></h3><h4 id="聚类与分类"><a href="#聚类与分类" class="headerlink" title="聚类与分类"></a><font size="3px" color="red">聚类与分类</font></h4><table>
<thead>
<tr>
<th></th>
<th>聚类</th>
<th>分类</th>
</tr>
</thead>
<tbody>
<tr>
<td>核心</td>
<td>将数据分成多个组，探索每个组的数据是否有联系</td>
<td>从已经分组的数据中去学习，把新数据放到已经分好的组中去</td>
</tr>
<tr>
<td>学习类型</td>
<td>无监督，无需<strong>标签</strong>进行训练</td>
<td>有监督，需要<strong>标签</strong>进行训练</td>
</tr>
<tr>
<td>典型算法</td>
<td>K-Means，DBSCAN，层次聚类，光谱聚类</td>
<td>决策树，贝叶斯，逻辑回归</td>
</tr>
<tr>
<td>算法输出</td>
<td>聚类结果是不确定的，不一定总是能够反映数据的真实分类，同样的聚类，根据不同的业务需求可能是一个好结果，也可能是一个坏结果</td>
<td>分类结果是确定的，分类的优劣是客观的，不是根据业务或算法需求决定</td>
</tr>
</tbody>
</table>
<h4 id="sklearn中的聚类算法"><a href="#sklearn中的聚类算法" class="headerlink" title="sklearn中的聚类算法"></a><font size="3px" color="red">sklearn中的聚类算法</font></h4><p>聚类算法在sklearn中有两种表现形式，一种是类（和我们目前为止学过的分类算法以及数据预处理方法们都一样），需要<strong>实例化</strong>，训练并使用接口和属性来调用结果。另一种是函数（function），只需要输入特征矩阵和超参数，即可返回聚类的结果和各种指标。</p>
<table>
<thead>
<tr>
<th>类</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>sklearn.cluster.Birch</td>
<td>实现Birtch聚类算法</td>
</tr>
<tr>
<td>sklearn.cluster.DBSCAN</td>
<td>从矢量数组或距离矩阵执行DBSCAN聚类</td>
</tr>
<tr>
<td>cluster.KMeans</td>
<td>K均值聚类</td>
</tr>
<tr>
<td>cluster.MiniBatchKMeans</td>
<td>小批量K均值聚类</td>
</tr>
<tr>
<td>cluster.SpectralClustering</td>
<td>光谱聚类，将聚类应用于规范化拉普拉斯的投影</td>
</tr>
<tr>
<td>cluste.AgglomerativeClustering</td>
<td>凝聚聚类</td>
</tr>
<tr>
<td><strong>函数</strong></td>
<td><strong>含义</strong></td>
</tr>
<tr>
<td>cluster.k_means</td>
<td>k均值聚类</td>
</tr>
<tr>
<td>cluster.ward_tree</td>
<td>光谱聚类</td>
</tr>
<tr>
<td>cluster.affinity_propagation</td>
<td>执行亲和传播数据聚类</td>
</tr>
<tr>
<td>cluster.mean_shift</td>
<td>使用平坦核函数的平均移位聚类</td>
</tr>
</tbody>
</table>
<h4 id="KMeans"><a href="#KMeans" class="headerlink" title="KMeans"></a><font size="3px" color="red">KMeans</font></h4><p><img src="https://img-blog.csdnimg.cn/20191224170942253.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="KMeans"></p>
<font size="4px" color="green">Kmeans没有损失函数</font>  


<h4 id="实例"><a href="#实例" class="headerlink" title="实例"></a><font size="3px" color="red">实例</font></h4><ul>
<li><code>sklearn.cluster.KMeans</code><br>重要参数<code>n_clusters</code>：n_clusters是KMeans中的k，表示着我们告诉模型我们要分几类。这是KMeans当中唯一一个必填的参数，默认为8类。</li>
</ul>
<p>数据：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from sklearn.datasets import make_blobs</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">X, y = make_blobs(n_samples=500,n_features=2,centers=4,random_state=1)</span><br><span class="line">color = [&quot;red&quot;,&quot;pink&quot;,&quot;orange&quot;,&quot;gray&quot;]</span><br><span class="line">for i in range(4):</span><br><span class="line">    plt.scatter(X[y==i, 0], X[y==i, 1]</span><br><span class="line">           ,marker=&apos;o&apos;</span><br><span class="line">           ,s=8</span><br><span class="line">           ,c=color[i]</span><br><span class="line">           )</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://img-blog.csdnimg.cn/20191224173418537.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="数据"><br>先简单实例化：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from sklearn.cluster import KMeans</span><br><span class="line">cluster = KMeans(n_clusters=3, random_state=0).fit(X)  # 先设置3类，训练好模型</span><br><span class="line"></span><br><span class="line"># 重要属性</span><br><span class="line">y_pred = cluster.labels_ # 每个样本所对应的类,(500,)</span><br><span class="line"></span><br><span class="line">pre = cluster.fit_predict(X) # 训练并预测所有数据所属类</span><br><span class="line">print(pre==y_pred) # 全是True</span><br><span class="line"></span><br><span class="line">centroid=cluster.cluster_centers_</span><br><span class="line">print(centroid) # 簇心</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">[[-7.09306648 -8.10994454]</span><br><span class="line"> [-1.54234022  4.43517599]</span><br><span class="line"> [-8.0862351  -3.5179868 ]]</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line">inertia=cluster.inertia_ # 总距离平方和</span><br><span class="line">print(inertia) # 1903.4503741659223</span><br></pre></td></tr></table></figure></p>
<p>查看结果：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">color = [&quot;red&quot;,&quot;pink&quot;,&quot;orange&quot;,&quot;gray&quot;]</span><br><span class="line">for i in range(3): # 聚类结果</span><br><span class="line">    plt.scatter(X[y_pred==i,0],X[y_pred==i,1]</span><br><span class="line">                ,marker=&apos;o&apos;</span><br><span class="line">                ,s=8</span><br><span class="line">                ,c=color[i])</span><br><span class="line"></span><br><span class="line">plt.scatter(centroid[:,0],centroid[:,1],marker=&apos;X&apos;,s=15,c=&apos;black&apos;) # 绘制质心</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://img-blog.csdnimg.cn/20191224175030244.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="聚类结果"><br>根据整体平方和inertia和聚类结果可以发现橙色区域聚类效果可以更优，inertia可以更小。</p>
<p>尝试其他的n_clusters，可以发现簇数量越大，总距离平方和<code>cluster.inertia_</code>越小，但<code>cluster.inertia_</code>并不能用来评估算法好坏</p>
<h4 id="聚类算法的模型评估指标"><a href="#聚类算法的模型评估指标" class="headerlink" title="聚类算法的模型评估指标"></a><font size="3px" color="red">聚类算法的模型评估指标</font></h4><p>在<strong>分类</strong>中，有直接结果（标签）的输出，并且分类的结果有正误之分，所以我们使用预测的<strong>准确度</strong>，<strong>混淆矩阵</strong>，<strong>ROC曲线</strong>等等指标来进行评估，但无论如何评估，都是在”模型找到正确答案“的能力。而回归中，由于要拟合数据，我们有SSE均方误差，有损失函数来衡量模型的拟合程度。但这些衡量指标都不能够使用于聚类。</p>
<p><font size="5px" color="greey">面试高危问题：如何衡量聚类算法的效果？</font><br>聚类模型的结果不是某种标签输出，并且<strong>聚类的结果是不确定的</strong>，其优劣由业务需求或者算法需求来决定，并且没有永远的正确答案。那我们如何衡量聚类的效果呢？<br>KMeans的目标是确保<strong>簇内差异小，簇外差异大</strong>，可以通过衡量簇内差异来衡量聚类的效果。而<code>Inertia</code>是用距离来衡量簇内差异的指标，但是Inertia越小模型越好吗？   </p>
<ul>
<li>首先，它不是有界的。我们只知道，Inertia是越小越好，是0最好，但我们不知道，一个较小的Inertia究竟有没有达到模型的极限，能否继续提高。</li>
<li>第二，它的计算太容易受到特征数目的影响，数据维度很大的时候，Inertia的计算量会陷入维度诅咒之中，计算量会爆炸，不适合用来一次次评估模型。</li>
<li>第三，Inertia对数据的分布有假设，它假设数据满足凸分布（即数据在二维平面图像上看起来是一个凸函数的样子），并且它假设数据是各向同性的（isotropic），即是说数据的属性在不同方向上代表着相同的含义。但是现实中的数据往往不是这样。所以使用Inertia作为评估指标，会让聚类算法在一些细长簇，环形簇，或者不规则形状的流形时表现不佳：<br><img src="https://img-blog.csdnimg.cn/2019122420540115.jpg" alt="环形簇"><br>可以看出，实际上期望效果应该是内环作为一簇，外环作为一簇，而如果使用<code>cluster.inertia_</code>作为衡量指标，效果就会很差</li>
</ul>
<h5 id="轮廓系数"><a href="#轮廓系数" class="headerlink" title="轮廓系数"></a><font size="3px" color="red">轮廓系数</font></h5><p>最常用的聚类算法的评价指标。它是对每个样本来定义的，它能够同时衡量：</p>
<ol>
<li>样本与其自身所在的簇中的其他样本的相似度<strong>a</strong>，等于样本与同一簇中所有其他点之间的平均距离</li>
<li>样本与其他簇中的样本的相似度<strong>b</strong>，等于样本与下一个最近的簇中得所有点之间的平均距离</li>
</ol>
<p>根据聚类的要求”簇内差异小，簇外差异大“，我们希望b永远大于a，并且大得越多越好。<br>单个样本的轮廓系数计算为：<br><img src="https://img-blog.csdnimg.cn/20191224210005965.png" alt="轮廓系数"><br>即：<br><img src="https://img-blog.csdnimg.cn/20191224210052149.jpg" alt="轮廓系数"><br>很容易理解轮廓系数范围是(-1,1)，其中值越接近1表示样本与自己所在的簇中的样本很相似，并且与其他簇中的样本不相似，当样本点与簇外的样本更相似的时候，轮廓系数就为负。当轮廓系数为0时，则代表两个簇中的样本相似度一致，两个簇本应该是一个簇。</p>
<p><font color="red">在sklearn中，我们使用模块metrics中的类<code>silhouette_score</code>来计算轮廓系数，它返回的是一个数据集中，所有样本的<strong>轮廓系数的均值</strong>。</font></p>
<ul>
<li>学习曲线<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from sklearn.cluster import KMeans</span><br><span class="line">from sklearn.metrics import silhouette_score</span><br><span class="line">from sklearn.metrics import silhouette_samples</span><br><span class="line"></span><br><span class="line">score=[]</span><br><span class="line">for i in range(3,7):</span><br><span class="line">    y_pred=KMeans(n_clusters=i,random_state=10).fit_predict(X)</span><br><span class="line">    score.append(silhouette_score(X,y_pred))</span><br><span class="line">    </span><br><span class="line">plt.plot(range(3,7),score)</span><br><span class="line">plt.xticks([3,4,5,6])</span><br><span class="line">plt.show()</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">print(silhouette_samples(X,y_pred)) </span><br><span class="line">这个是返回所有样本的聚类情况，为True或者False，如果是False则说明这个样本的轮廓系数为负，聚类失败</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20191224211620466.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="学习曲线"></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ML</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title>Python爬虫小练习</title>
    <url>/2019/09/13/Python%E7%88%AC%E8%99%AB%E5%B0%8F%E7%BB%83%E4%B9%A0/</url>
    <content><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>前一节在学完正则表达式后视频给了一个小练习：爬取某个网站的图片。就几行代码，不难。</p>
<p>自己本学期的一个小目标：做一个敏感信息过滤器。<br>首先得学会语音和图片处理，然后使用模型训练过滤器，最后进行分类预测。<br>核心在于训练数据抓取与处理，模型的选择。</p>
<p>本文，通过爬取某个网站的图片，初步接触爬虫技术。</p>
<h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><ol>
<li>导入相关Python包<code>requests</code></li>
<li>选择要爬取的图片网站链接</li>
<li>分析网站源码观察图片链接特点</li>
<li>爬取网页源码</li>
<li>使用正则表达式过滤图片链接</li>
<li>根据图片链接获取图片信息</li>
<li>写入指定文件并给图片命名</li>
</ol>
<h4 id="导入相关包"><a href="#导入相关包" class="headerlink" title="导入相关包"></a>导入相关包</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import re # 处理正则表达式</span><br><span class="line">import requests # 处理爬取请求</span><br></pre></td></tr></table></figure>
<h4 id="选择图片网站"><a href="#选择图片网站" class="headerlink" title="选择图片网站"></a>选择图片网站</h4><p>url=<a href="http://www.xiaohuar.com/hua/" target="_blank" rel="noopener">‘http://www.xiaohuar.com/hua/‘</a><br><img src="https://i.loli.net/2019/09/13/WN7tUJdFpca8nBy.png" alt="2019-09-13 22-18-54 的屏幕截图.png"></p>
<h4 id="分析源码"><a href="#分析源码" class="headerlink" title="分析源码"></a>分析源码</h4><p>贴上部分源码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;ul&gt;</span><br><span class="line">            &lt;li&gt;&lt;a href=&quot;&quot;   &gt;&lt;img src=&quot;http://www.xiaohuar.com/skin/default/images/mr_2.gif&quot; width=&quot;61&quot; height=&quot;30&quot; /&gt;&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">            &lt;li&gt;&lt;a href=&quot;http://www.xiaohuar.com/hua/&quot;    &gt;&lt;img src=&quot;http://www.xiaohuar.com/skin/default/images/zx_2.gif&quot; width=&quot;61&quot; height=&quot;30&quot; /&gt;&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">             &lt;li&gt;&lt;a href=&quot;http://www.xiaohuar.com/2014.html&quot;&gt;&lt;img src=&quot;http://www.xiaohuar.com/skin/default/images/jx_2.gif&quot; width=&quot;61&quot; height=&quot;30&quot; /&gt;&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">              &lt;li&gt;&lt;a href=&quot;http://www.xiaohuar.com/xiaocao/&quot;   &gt;&lt;img src=&quot;http://www.xiaohuar.com/skin/default/images/jx_3.gif&quot; width=&quot;61&quot; height=&quot;30&quot; /&gt;&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">         &lt;/ul&gt;</span><br></pre></td></tr></table></figure></p>
<p>我们需要的是.jpg格式的图片链接，而这些链接前缀为<code>src=&quot;&quot;</code>，我们只需要<code>&quot;&quot;</code>里面内容即可，但有的链接并不是完整http形式，如：<code>&lt;img width=&quot;210&quot; alt=&quot;烟台大学校花王煜&quot; src=&quot;/d/file/20190912/8774b906215adeae8963173b1e0df146.jpg&quot;&gt;</code>，故我们爬取之后需要加上网址前缀。</p>
<h4 id="爬取网页源码"><a href="#爬取网页源码" class="headerlink" title="爬取网页源码"></a>爬取网页源码</h4><p>导入<code>requests</code>包之后，使用<code>requests.get(url)</code>即可获得链接资源信息。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">resp=requests.get(url_1) # 获取源码</span><br><span class="line">we_data=resp.text  # 将源码信息保存下来</span><br><span class="line"># print(resp) # if net work well then except to see: &lt;Response [200]&gt;</span><br></pre></td></tr></table></figure></p>
<h4 id="过滤图片链接"><a href="#过滤图片链接" class="headerlink" title="过滤图片链接"></a>过滤图片链接</h4><p>这是前一节<a href="https://sfz-lyq.cn/2019/09/12/Python%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/#more">正则表达式</a>的概念。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">reg=re.compile(r&apos;src=&quot;(.+?.jpg)&quot;&apos;) # re.compile(r&apos;src=\&apos;(.+?.jpg)\&apos;&apos;)</span><br><span class="line">res=re.findall(reg,we_data) # 返回获取的网址列表</span><br><span class="line"># print(res)</span><br></pre></td></tr></table></figure></p>
<h4 id="获取图片信息并写入文件"><a href="#获取图片信息并写入文件" class="headerlink" title="获取图片信息并写入文件"></a>获取图片信息并写入文件</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">num=0 # 给图片编号</span><br><span class="line">for i in res:</span><br><span class="line">    if i.startswith(&apos;/d&apos;): # 如果链接不是完整的http形式，则手动加上前缀</span><br><span class="line">        i=&apos;http://www.xiaohuar.com&apos; +i # </span><br><span class="line">    # print(i)</span><br><span class="line">    b=requests.get(i) # 获取图片信息</span><br><span class="line">    with open(r&apos;./craw_practise1_xiaohua/%s.jpg&apos; %num,&apos;wb&apos;) as f: # 写入文件，需要新建文件夹</span><br><span class="line">        f.write(b.content)</span><br><span class="line">        f.close()</span><br><span class="line">        print(&apos;num_%s complete&apos; % num)</span><br><span class="line">        num=num+1</span><br></pre></td></tr></table></figure>
<p>所爬取的图片就不放上来了。<br>在爬取其他网站时，稍加更改也有用，但有的图片过大，爬取很慢。需要使用<code>os</code>包和其他相关包改进，最后封装成脚本。还有很多需要学习。  </p>
<p><a href="https://paste.ubuntu.com/p/62jTHJ9bD5/" target="_blank" rel="noopener">完整代码</a> </p>
<h4 id="读取本地图片"><a href="#读取本地图片" class="headerlink" title="读取本地图片"></a>读取本地图片</h4><p>注意，我们所爬取的图片保存格式是<code>.jpg</code>，故在使用相关包时候留意是否只支持<code>.png</code>格式的图片。<br>我们可以在新建的本地文件夹下查看图片，也可以直接读取出来。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt # 图片处理包</span><br><span class="line"></span><br><span class="line">def show_picture(file,num):</span><br><span class="line">    for i in range(num):</span><br><span class="line">        path = file + str(i) + &apos;.jpg&apos;  # str(i) = (&apos;%d&apos; %i)</span><br><span class="line">        img=plt.imread(path) # 需要pillow包的支持，不然只能读.png格式的图片</span><br><span class="line">        plt.subplot(num/7,7,i+1) # 在一个画布中划分子画布，指定显示区域</span><br><span class="line">        plt.imshow(img)</span><br><span class="line">        plt.axis(&apos;off&apos;) #不显示坐标轴</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">show_picture(file=&apos;./craw_practise1_xiaohua/&apos;,num=49)</span><br></pre></td></tr></table></figure></p>
<p><img src="https://i.loli.net/2019/09/15/nVAPt3f9Qb6OIdU.png" alt="Figure_26.png"></p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ol>
<li><a href="https://www.cnblogs.com/piaojianxue/p/10822024.html" target="_blank" rel="noopener">Python中读取、显示和保存图片的方法</a></li>
<li><a href="https://www.cnblogs.com/devilmaycry812839668/p/9291591.html" target="_blank" rel="noopener">python3 读入一个jpg格式的图片，并转换长宽像素个数，然后进行绘制</a></li>
<li><a href="https://blog.csdn.net/huangxudongzzZ/article/details/90256923" target="_blank" rel="noopener">Python 多个图同时在不同窗口显示</a></li>
<li><a href="https://sfz-lyq.cn//2019/07/07/Python%E5%9B%BE%E5%BD%A2%E7%BB%98%E5%88%B6/">Python图形绘制</a></li>
</ol>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title>Python正则表达式</title>
    <url>/2019/09/12/Python%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://www.bilibili.com/video/av41861090?from=search&amp;seid=13092767094576234643" target="_blank" rel="noopener">学习地址</a><center></center></center></h3><p>参考资料：</p>
<ol>
<li><a href="https://www.runoob.com/python/python-reg-expressions.html" target="_blank" rel="noopener">Python 正则表达式</a></li>
<li><a href="https://www.bilibili.com/video/av62160077?from=search&amp;seid=17515103713354790628" target="_blank" rel="noopener">正则表达式精讲</a></li>
</ol>
<hr>
<h4 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h4><ol>
<li>处理文本(字符串)</li>
<li>语言规则</li>
<li>前端通用</li>
</ol>
<h4 id="导入模块"><a href="#导入模块" class="headerlink" title="导入模块"></a>导入模块</h4><p><code>import re</code><br>全称：regular expression</p>
<h4 id="正则表达式语法"><a href="#正则表达式语法" class="headerlink" title="正则表达式语法"></a>正则表达式语法</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">^         匹配字符串开头如 ^p</span><br><span class="line">$         匹配字符串结尾如 $n</span><br><span class="line">\A        指定字符串必须出线在开头</span><br><span class="line">\Z        指定字符串必须出线在结尾</span><br><span class="line">.         匹配任意一个字符</span><br><span class="line">[...]     匹配字符集</span><br><span class="line">\d        匹配一个数字</span><br><span class="line">\D        匹配一个非数字</span><br><span class="line">\s        匹配一个空白字符</span><br><span class="line">\S        匹配一个非空白字符</span><br><span class="line">\w        匹配一个数字或字母</span><br><span class="line">\W        匹配一个非单词字符</span><br><span class="line">[^...]    [^abc] 匹配除了a,b,c之外的字符</span><br><span class="line">*         匹配前一个字符0次或无限次</span><br><span class="line">+         匹配前一个字符1次或无限次</span><br><span class="line">?         匹配前一个字符0次或1次</span><br></pre></td></tr></table></figure>
<h4 id="正则表达式修饰符-可选标志"><a href="#正则表达式修饰符-可选标志" class="headerlink" title="正则表达式修饰符 - 可选标志"></a>正则表达式修饰符 - 可选标志</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">re.I	使匹配对大小写不敏感</span><br><span class="line">re.L	做本地化识别（locale-aware）匹配</span><br><span class="line">re.M	多行匹配，影响 ^ 和 $</span><br><span class="line">re.S	使 . 匹配包括换行在内的所有字符</span><br><span class="line">re.U	根据Unicode字符集解析字符。这个标志影响 \w, \W, \b, \B.</span><br><span class="line">re.X	该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解。</span><br></pre></td></tr></table></figure>
<h4 id="常用方法"><a href="#常用方法" class="headerlink" title="常用方法"></a>常用方法</h4><h5 id="1-re-findall"><a href="#1-re-findall" class="headerlink" title="1. re.findall()"></a>1. re.findall()</h5><p>说明：Return a list of all non-overlapping matches in the string,If one or more capturing groups are present in the pattern, return a list of groups; this will be a list of tuples if the pattern has more than one group. - 以列表的形式返回所有不重叠的所匹配到的子串，如果匹配失败返回空列表[]；如果模式串中有多个组，以列表形式返回所有匹配到的元组<br>示例1：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a=&apos;aaabaabc&apos;</span><br><span class="line">print(re.findall(&apos;aa&apos;,a))</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;[&apos;aa&apos;, &apos;aa&apos;]</span><br></pre></td></tr></table></figure></p>
<p>示例2：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a=&apos;1a23bc456def&apos;</span><br><span class="line">print(re.findall(&apos;(\d+)(..)&apos;,a))</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;[(&apos;1&apos;, &apos;a2&apos;), (&apos;3&apos;, &apos;bc&apos;), (&apos;456&apos;, &apos;de&apos;)]</span><br></pre></td></tr></table></figure></p>
<p>元字符： 在正则表达式中具有特殊意义的专用字符，比如<code>\d</code>表示数字<br>示例：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a=&apos;1a2b3c&apos;</span><br><span class="line">print(re.findall(&apos;\d&apos;,a)) # 如果想搜索两个数字需要用\d\d，依次类推，但比较麻烦，可以使用+号</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;[&apos;1&apos;, &apos;2&apos;, &apos;3&apos;]</span><br></pre></td></tr></table></figure></p>
<h5 id="2-‘-’-、-‘-’-和-‘-’-号的使用"><a href="#2-‘-’-、-‘-’-和-‘-’-号的使用" class="headerlink" title="2. ‘+’ 、 ‘*’  和 ‘.’ 号的使用"></a>2. ‘+’ 、 ‘*’  和 ‘.’ 号的使用</h5><p>说明：’+’ 匹配一个或多个字符， ‘*’  匹配0个或多个字符 ，’.’ 匹配任意一个字符</p>
<p>示例：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a=&apos;12a34b567c&apos;</span><br><span class="line">print(re.findall(&apos;\d+&apos;,a)) # 连续匹配直到非数字</span><br><span class="line">print(re.findall(&apos;\d*&apos;,a)) </span><br><span class="line">print(re.findall(&apos;\d+.&apos;,a)) # 与 print(re.findall(&apos;\d*.&apos;,a)) 结果相同</span><br><span class="line">&gt;&gt;&gt;[&apos;12&apos;, &apos;34&apos;, &apos;567&apos;]</span><br><span class="line">&gt;&gt;&gt;[&apos;12&apos;, &apos;&apos;, &apos;34&apos;, &apos;&apos;, &apos;567&apos;, &apos;&apos;, &apos;&apos;]</span><br><span class="line">&gt;&gt;&gt;[&apos;12a&apos;, &apos;34b&apos;, &apos;567c&apos;]</span><br></pre></td></tr></table></figure></p>
<h5 id="3-范匹配与精确匹配"><a href="#3-范匹配与精确匹配" class="headerlink" title="3. 范匹配与精确匹配"></a>3. 范匹配与精确匹配</h5><p>范匹配： 在搜索想要的内容时把其他的内容也匹配进来了<br>精确匹配： 在范匹配的基础上得到想要的内容，方法是用<code>()</code>将想要的内容括起来<br>示例：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a=&apos;1a23bc456def&apos;</span><br><span class="line">print(re.findall(&apos;\d+(..)&apos;,a)) # 精确匹配</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;[&apos;a2&apos;, &apos;bc&apos;, &apos;de&apos;]</span><br></pre></td></tr></table></figure></p>
<h5 id="4-字符集"><a href="#4-字符集" class="headerlink" title="4. 字符集"></a>4. 字符集</h5><p>说明：用<code>[]</code>指定搜索字符集<br>示例1：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a=&apos;1a23bc456def&apos;</span><br><span class="line">print(re.findall(&apos;[0-9]&apos;,a))</span><br><span class="line">print(re.findall(&apos;[0-9]+&apos;,a))</span><br><span class="line">print(re.findall(&apos;[0-9a-z]&apos;,a))</span><br><span class="line">print(re.findall(&apos;[0-9]&#123;2&#125;&apos;,a)) # 指定匹配长度为2，&#123;2,5&#125;指定长度为2/3/4/5，注意不会重复匹配</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;[&apos;1&apos;, &apos;2&apos;, &apos;3&apos;, &apos;4&apos;, &apos;5&apos;, &apos;6&apos;]</span><br><span class="line">&gt;&gt;&gt;[&apos;1&apos;, &apos;23&apos;, &apos;456&apos;]</span><br><span class="line">&gt;&gt;&gt;[&apos;1&apos;, &apos;a&apos;, &apos;2&apos;, &apos;3&apos;, &apos;b&apos;, &apos;c&apos;, &apos;4&apos;, &apos;5&apos;, &apos;6&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;]</span><br><span class="line">&gt;&gt;&gt;[&apos;23&apos;, &apos;45&apos;]</span><br></pre></td></tr></table></figure></p>
<h5 id="5-贪婪匹配-默认-与非贪婪匹配"><a href="#5-贪婪匹配-默认-与非贪婪匹配" class="headerlink" title="5. 贪婪匹配(默认)与非贪婪匹配"></a>5. 贪婪匹配(默认)与非贪婪匹配</h5><p>说明：当有多种匹配情况时，尽量往后靠，比如指定长度{2,5}，会直接先匹配5<br>示例：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a=&apos;1a23bc456def&apos;</span><br><span class="line">print(re.findall(&apos;[0-9]&#123;1,3&#125;&apos;,a))  # 贪婪匹配，直接匹配456而非截取45</span><br><span class="line">print(re.findall(&apos;[0-9]&#123;1,3&#125;?&apos;,a),re.S) # 非贪婪匹配,re.S为可选参数表示忽略换行符的影响</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;[&apos;1&apos;, &apos;23&apos;, &apos;456&apos;]</span><br><span class="line">&gt;&gt;&gt;[&apos;1&apos;, &apos;2&apos;, &apos;3&apos;, &apos;4&apos;, &apos;5&apos;, &apos;6&apos;]</span><br></pre></td></tr></table></figure></p>
<h5 id="6-re-match"><a href="#6-re-match" class="headerlink" title="6. re.match()"></a>6. re.match()</h5><p>说明：Try to apply the pattern at the start of the string, returning a match object, or None if no match was found - 匹配开头，只匹配一次，成功返回对象，否则返回None<br>语法： <code>re.match(pattern, string, flags=0)</code>,  pattern表示匹配的正则表达式， string 表示要匹配的字符串。flags    标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等。</p>
<p>示例：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a=&apos;1a 23bc 456def&apos;</span><br><span class="line">print(re.match(&apos;a&apos;,a))</span><br><span class="line">print(re.match(&apos;1a 23bc&apos;,a))</span><br><span class="line">print(re.match(&apos;1a 23bc&apos;,a,).group()) # 查看匹配结果</span><br><span class="line">print(re.match(&apos;1a 23bc&apos;,a).span())  # 返回匹配下标范围如果失败报错</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;None</span><br><span class="line">&gt;&gt;&gt;&lt;_sre.SRE_Match object; span=(0, 7), match=&apos;1a 23bc&apos;&gt;</span><br><span class="line">&gt;&gt;&gt;1a 23bc</span><br><span class="line">&gt;&gt;&gt;(0, 7)</span><br></pre></td></tr></table></figure></p>
<h5 id="7-re-search"><a href="#7-re-search" class="headerlink" title="7. re.search()"></a>7. re.search()</h5><p>说明：re.search 扫描整个字符串并返回第一个成功的匹配。匹配成功re.search方法返回一个匹配的对象，否则返回None。  </p>
<p>示例：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import re</span><br><span class="line">print(re.search(&apos;www&apos;, &apos;www.runoob.com&apos;).span())  # 在起始位置匹配</span><br><span class="line">print(re.search(&apos;com&apos;, &apos;www.runoob.com&apos;).span())         # 不在起始位置匹配</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;(0,3)</span><br><span class="line">&gt;&gt;&gt;(11,14)</span><br></pre></td></tr></table></figure></p>
<h5 id="8-re-compile"><a href="#8-re-compile" class="headerlink" title="8.  re.compile()"></a>8.  re.compile()</h5><p>说明：Compile a regular expression pattern, returning a pattern object - compile 函数用于编译正则表达式，生成一个正则表达式（ Pattern ）对象。</p>
<p>示例：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a=&apos;1a 23bc 456def&apos;</span><br><span class="line">reg=re.compile(&apos;\d+&apos;) # 避免重复</span><br><span class="line">print(re.findall(reg,a))</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;[&apos;1&apos;, &apos;23&apos;, &apos;456&apos;]</span><br></pre></td></tr></table></figure></p>
<h5 id="9-re-sub"><a href="#9-re-sub" class="headerlink" title="9. re.sub()"></a>9. re.sub()</h5><p>说明：将字符串中正则表达式所匹配到的内容替换为指定内容。<br>语法：<code>re.sub(pattern, repl, string, count=0, flags=0)</code>，pattern : 正则中的模式字符串；repl : 替换的字符串，也可为一个函数；string : 要被查找替换的原始字符串；count : 模式匹配后替换的最大次数，默认 0 表示替换所有的匹配。</p>
<p>示例：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a=&apos;1a 23bc 456def&apos;</span><br><span class="line">print(&apos;Before:&apos;,a)</span><br><span class="line">a=re.sub(&apos;\s&apos;,&apos;&apos;,a) # 匹配任意空白字符,等价于[\t\n\r\f]</span><br><span class="line">print(&apos;After:&apos;,a)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;Before: 1a 23bc 456def</span><br><span class="line">&gt;&gt;&gt;After: 1a23bc456def</span><br></pre></td></tr></table></figure></p>
<h5 id="10-re-split"><a href="#10-re-split" class="headerlink" title="10. re.split()"></a>10. re.split()</h5><p>说明：split 方法按照能够匹配的子串将字符串分割后返回列表<br>语法：<code>re.split(pattern, string, maxsplit=0, flags=0])</code>，maxsplit    分隔次数，maxsplit=1 分隔一次，默认为 0，不限制次数</p>
<p>示例：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a=&apos;1a 23bc 456def&apos;</span><br><span class="line">print(re.split(&apos;\W&apos;,a))</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;[&apos;1a&apos;, &apos;23bc&apos;, &apos;456def&apos;]</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>牛客练习赛51 部分题解</title>
    <url>/2019/09/09/%E7%89%9B%E5%AE%A2%E7%BB%83%E4%B9%A0%E8%B5%9B51-%E9%83%A8%E5%88%86%E9%A2%98%E8%A7%A3/</url>
    <content><![CDATA[<h3 id="牛客练习赛51"><a href="#牛客练习赛51" class="headerlink" title="牛客练习赛51"></a><center><a href="https://ac.nowcoder.com/acm/contest/1083#question" target="_blank" rel="noopener">牛客练习赛51</a></center></h3><h4 id="A-abc"><a href="#A-abc" class="headerlink" title="A    abc"></a><a href="https://ac.nowcoder.com/acm/contest/1083/A" target="_blank" rel="noopener">A    abc</a></h4><p>吉首大学校赛原题G，之前写过<a href="https://sfz-lyq.cn/2019/07/18/%E5%90%89%E9%A6%96%E5%A4%A7%E5%AD%A62019%E5%B9%B4%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%AB%9E%E8%B5%9B-%E9%83%A8%E5%88%86%E9%A2%98%E8%A7%A3/#more">题解</a></p>
<h4 id="B-子串查询"><a href="#B-子串查询" class="headerlink" title="B    子串查询"></a>B    子串查询</h4><p>题意： 给你一个1e5长度的字符串。Q次查询，每次输入一个子串，求原串是否能正序组成子串?”YES”：”NO”。<br>思路：</p>
<ul>
<li>O(nlogn)解法，开26个set，每个字母出线的位置按顺序扔进相应set中。然后遍历子串，在每个set中按顺序查询最早出线的位置，位置递增。</li>
<li>O(n)解法，nex[1e5][26]表示s串中第i个位置后面第一个字符为(j+’a’)的位置(0&lt;=j&lt;26)。可以用一个队列辅助来求出nex数组，那么，now=nex[now][t[i]-‘a’]。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include&lt;bits/stdc++.h&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">typedef long long ll;</span><br><span class="line">const int INF=0x3f3f3f;</span><br><span class="line">const int N=1e5+10;</span><br><span class="line">char s[N];</span><br><span class="line">set&lt;int&gt;q[26];</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    int n,m;</span><br><span class="line">    while(~scanf(&quot;%d%d&quot;,&amp;n,&amp;m))</span><br><span class="line">    &#123;</span><br><span class="line">        for(int i=0;i&lt;26;i++)</span><br><span class="line">             q[i].clear();</span><br><span class="line">        scanf(&quot;%s&quot;,s);</span><br><span class="line">        for(int i=0;s[i]!=&apos;\0&apos;;i++)</span><br><span class="line">            q[s[i]-&apos;a&apos;].insert(i);</span><br><span class="line">        set&lt;int&gt;::iterator it;</span><br><span class="line">        while(m--)</span><br><span class="line">        &#123;</span><br><span class="line">            scanf(&quot;%s&quot;,s);</span><br><span class="line">            int flag=1,id=0;</span><br><span class="line">            for(int i=0;s[i]!=&apos;\0&apos;;i++)</span><br><span class="line">            &#123;</span><br><span class="line">               it=q[s[i]-&apos;a&apos;].lower_bound(id);</span><br><span class="line">               if(it!=q[s[i]-&apos;a&apos;].end())</span><br><span class="line">                 id=(*it)+1;</span><br><span class="line">               else flag=0;</span><br><span class="line">            &#125;</span><br><span class="line">            if(flag) puts(&quot;YES&quot;);</span><br><span class="line">            else puts(&quot;NO&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br><span class="line">/*</span><br><span class="line">abcaa</span><br><span class="line">aabc</span><br><span class="line">*/</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="C-勾股定理"><a href="#C-勾股定理" class="headerlink" title="C    勾股定理"></a><a href="https://ac.nowcoder.com/acm/contest/1083/c" target="_blank" rel="noopener">C    勾股定理</a></h4><p>原题：<a href="https://blog.csdn.net/NYIST_TC_LYQ/article/details/52280149?utm_source=blogxgwz5" target="_blank" rel="noopener">关于基本勾股数规律的探讨总结与例题！</a></p>
<h4 id="D-羊吃草"><a href="#D-羊吃草" class="headerlink" title="D    羊吃草"></a><a href="https://ac.nowcoder.com/acm/contest/1083/D" target="_blank" rel="noopener">D    羊吃草</a></h4><p>题意：400头羊在长度为400的数轴上吃草，每头羊可以选择自己喜爱的区间任意一个整点吃草，现在400次查询，每次查询一个区间求这个区间最多可以有多少头羊同时吃草。<br>思路：匹配问题，每个点可以被很多羊吃，那么建图。查询一个区间[l,r]则遍历每个点，跑匈牙利算法即可。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include&lt;bits/stdc++.h&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">typedef long long ll;</span><br><span class="line">const int INF=0x3f3f3f;</span><br><span class="line">const int N=405+10;</span><br><span class="line">struct node</span><br><span class="line">&#123;</span><br><span class="line">    int l,r;</span><br><span class="line">&#125; a[N];</span><br><span class="line">int dp[N][N];</span><br><span class="line">int used[N],linked[N];</span><br><span class="line">int n,q;</span><br><span class="line">bool dfs(int u)</span><br><span class="line">&#123;</span><br><span class="line">    for(int i=1; i&lt;=n; i++)</span><br><span class="line">        if(dp[u][i])</span><br><span class="line">        &#123;</span><br><span class="line">            if(!used[i])</span><br><span class="line">            &#123;</span><br><span class="line">                used[i]=1;</span><br><span class="line">                if(linked[i]==-1||dfs(linked[i]))</span><br><span class="line">                &#123;</span><br><span class="line">                    linked[i]=u;</span><br><span class="line">                    return true;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    return false;</span><br><span class="line">&#125;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    while(~scanf(&quot;%d%d&quot;,&amp;n,&amp;q))</span><br><span class="line">    &#123;</span><br><span class="line">        for(int i=1; i&lt;=n; i++) scanf(&quot;%d&quot;,&amp;a[i].l);</span><br><span class="line">        for(int i=1; i&lt;=n; i++) scanf(&quot;%d&quot;,&amp;a[i].r);</span><br><span class="line">        memset(dp,0,sizeof(dp));</span><br><span class="line">        for(int i=1; i&lt;=n; i++)</span><br><span class="line">            for(int j=a[i].l; j&lt;=a[i].r; j++)</span><br><span class="line">                dp[j][i]=1;</span><br><span class="line">        int l,r;</span><br><span class="line">        while(q--)</span><br><span class="line">        &#123;</span><br><span class="line">            scanf(&quot;%d%d&quot;,&amp;l,&amp;r);</span><br><span class="line">            memset(linked,-1,sizeof(linked));</span><br><span class="line">            int ans=0;</span><br><span class="line">            for(int i=l; i&lt;=r; i++)</span><br><span class="line">            &#123;</span><br><span class="line">                memset(used,0,sizeof(used));</span><br><span class="line">                if(dfs(i)) ans++;</span><br><span class="line">            &#125;</span><br><span class="line">            printf(&quot;%d\n&quot;,ans);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>考虑到时间性价比，后面的题就不去思考了。放上<a href="https://ac.nowcoder.com/discuss/248083?type=101&amp;order=0&amp;pos=11&amp;page=0" target="_blank" rel="noopener">题解</a>。</p>
]]></content>
      <categories>
        <category>题解</category>
      </categories>
      <tags>
        <tag>题解</tag>
        <tag>牛客网</tag>
      </tags>
  </entry>
  <entry>
    <title>吴恩达机器学习第六次编程作业-支持向量机</title>
    <url>/2019/09/05/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E5%85%AD%E6%AC%A1%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/</url>
    <content><![CDATA[<h3 id="编程作业：支持向量机"><a href="#编程作业：支持向量机" class="headerlink" title="编程作业：支持向量机"></a><center><a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/text?lessonId=1053422070&amp;courseId=1004570029" target="_blank" rel="noopener">编程作业：支持向量机</a></center></h3><hr>
<p>工具：Python3.5，Pycharm2019.1<br>参考资料：</p>
<ol>
<li><a href="https://blog.csdn.net/rentao315/article/details/78782734" target="_blank" rel="noopener">numpy元素的区间查找</a></li>
<li><a href="https://blog.csdn.net/wsj998689aa/article/details/47027365" target="_blank" rel="noopener">各种核函数</a></li>
<li><a href="https://www.cnblogs.com/luyaoblog/p/6775342.html" target="_blank" rel="noopener">python中支持向量机SVM的使用</a></li>
<li><a href="https://blog.csdn.net/Cowry5/article/details/80465922" target="_blank" rel="noopener">吴恩达机器学习作业：SVM支持向量机</a></li>
<li><a href="https://blog.csdn.net/lijil168/article/details/68167864" target="_blank" rel="noopener">支持向量机SVM和人工神经网络ANN的比较</a></li>
<li><a href="https://blog.csdn.net/xiangz_csdn/article/details/72794132" target="_blank" rel="noopener">我是怎样理解支持向量机（SVM）与神经网络的</a></li>
<li><a href="https://www.cnblogs.com/youyou0/p/8921719.html" target="_blank" rel="noopener">python txt文件常用读写操作</a></li>
<li><a href="https://blog.csdn.net/a237664181a/article/details/88845435" target="_blank" rel="noopener">Python3中字符串、列表、数组的转换方法</a></li>
<li><a href="https://blog.csdn.net/u013553529/article/details/78567696" target="_blank" rel="noopener">Python 输出百分比的两种方式</a></li>
<li><a href="https://blog.csdn.net/yeziand01/article/details/93601068" target="_blank" rel="noopener">Python 输出对齐</a></li>
</ol>
<p><a href="https://paste.ubuntu.com/p/SwsyKdv7qB/" target="_blank" rel="noopener">SVM with Gaussian Kernels—data3 完整代码</a><br><a href="https://paste.ubuntu.com/p/gPCqcFmbW3/" target="_blank" rel="noopener">Spam Classification完整代码</a></p>
<hr>
<p>任务： 使用support vector machines (SVMs)建立一个垃圾邮件分类器。</p>
<h4 id="Support-Vector-Machines"><a href="#Support-Vector-Machines" class="headerlink" title="Support Vector Machines"></a>Support Vector Machines</h4><p>总体分两个模块：</p>
<ol>
<li>直觉感受是SVM如何工作的并且了解高斯核函数和SVM是如何一起工作的</li>
<li>使用支持向量机建立一个垃圾邮件分类器</li>
</ol>
<h5 id="绘制样本数据集1"><a href="#绘制样本数据集1" class="headerlink" title="绘制样本数据集1"></a>绘制样本数据集1</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Get_Data():</span><br><span class="line">    data=sio.loadmat(&quot;./ex6/ex6data1.mat&quot;)</span><br><span class="line">    # for key in data:</span><br><span class="line">    #     print(key)</span><br><span class="line">    return data</span><br><span class="line"></span><br><span class="line">def Plot_Data(data): # 提取数据</span><br><span class="line">    data1=data[&apos;X&apos;][np.where(data[&apos;y&apos;].ravel()==1)] # positive examples</span><br><span class="line">    data2=data[&apos;X&apos;][np.where(data[&apos;y&apos;].ravel()==0)]</span><br><span class="line">    plt.plot(data1[:,0],data1[:,1],&apos;b+&apos;)</span><br><span class="line">    plt.plot(data2[:,0],data2[:,1],&apos;yo&apos;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/09/05/lkgNmEB2TAFenLp.png" alt="Figure_19.png"><br>有一个极端样本数据大概在 (0.1, 4.1)位置。<br>当然，设置坐标轴范围可以使得更加贴合文档</p>
<h4 id="训练SVM并尝试不同参数C"><a href="#训练SVM并尝试不同参数C" class="headerlink" title="训练SVM并尝试不同参数C"></a>训练SVM并尝试不同参数C</h4><p>文档是直接调用已写好的函数包，这里由于在linux环境下使用python，在sklearn库中也有SVM的封装。代码见下，详解见后。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Train_SVM(data,X,y):</span><br><span class="line">    classifier = svm.SVC(C=1, kernel=&apos;linear&apos;, gamma=&apos;auto_deprecated&apos;, decision_function_shape=&apos;ovo&apos;)</span><br><span class="line">    classifier.fit(X, y.ravel())</span><br><span class="line">    x1 = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)  # 第一特征，可以各多选取一些样本点，使得决策线更清晰</span><br><span class="line">    x2 = np.linspace(min(X[:, 1]), max(X[:, 1]), 100)</span><br><span class="line">    x1, x2 = np.meshgrid(x1, x2)</span><br><span class="line">    # print(x1.shape,x2.shape) # 100*100</span><br><span class="line">    grid = np.stack((x1.flat, x2.flat), axis=1)</span><br><span class="line">    # print(grid.shape) # 10000*2</span><br><span class="line">    grid_predict = classifier.predict(grid)</span><br><span class="line">    grid_predict = grid_predict.reshape(x1.shape)  # 还原成网格形状</span><br><span class="line">  # plt.xlim(0, 4.5)</span><br><span class="line">  # plt.ylim(1.5, 5)</span><br><span class="line">    plt=Plot_Data(data)</span><br><span class="line">    plt.contour(x1, x2, grid_predict)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p>
<p>尝试C=1，100的效果分别如图：<br><img src="https://i.loli.net/2019/09/06/DvOkGoByA2IjmM8.png" alt="Figure_20.png"><br><img src="https://i.loli.net/2019/09/06/N5MaXV9suFpxSej.png" alt="Figure_21.png"><br>关于以上代价解释：</p>
<ul>
<li>kernel为核函数，可选[‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ ]， kernel=’rbf’时（default），为高斯核。</li>
<li>C越大分类效果越好，但可能过拟合（default C=1.0）</li>
<li>gamma值越小，分类界面越连续；gamma值越大，分类界面越“散”，分类效果越好，但有可能会过拟合。</li>
<li>decision_function_shape为分类器，可选[‘ovo’,’ovr’]分别表示一对一分类器，一对多分类。默认为’ovr’，’one vs rest’。</li>
<li>meshgrid表示生成网格</li>
<li>图片中决策线比较粗糙，在绘制决策线的时候两个特征各多取100个样本点在计算网格交叉就行</li>
</ul>
<h4 id="SVM-with-Gaussian-Kernels"><a href="#SVM-with-Gaussian-Kernels" class="headerlink" title="SVM with Gaussian Kernels"></a>SVM with Gaussian Kernels</h4><p>任务：使用高斯核的SVM来做非线性回归。<br>步骤一： 实现高斯核函数，定义如下<br><img src="https://i.loli.net/2019/09/07/35k8Mxl2i6UKtWg.png" alt="2019-09-07 09-36-00 的屏幕截图.png"><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def GaussianKernel(x1,x2,sigma): # 求向量x1与向量x2的相似度</span><br><span class="line">    sim=math.exp(-((x1-x2)**2).sum()/(2*sigma*sigma))</span><br><span class="line">    return sim</span><br></pre></td></tr></table></figure></p>
<p>步骤二： 可视化样本数据集二<br>步骤三：使用高斯核函数训练SVM并绘制图像，与步骤二合并<br>代码和线形核函数差不多，将’linear’改为’rbf’，gamma=30，’ovo’改为’ovr’。这里默认C=1，后面改变C的值观察分类效果。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Plot_Boundary(X,classifier):</span><br><span class="line">    x1 = np.linspace(min(X[:, 0]), max(X[:, 0]), 500)  # 第一特征</span><br><span class="line">    x2 = np.linspace(min(X[:, 1]), max(X[:, 1]), 500)</span><br><span class="line">    x1, x2 = np.meshgrid(x1, x2)</span><br><span class="line">    # print(x1.shape,x2.shape) # 100*100</span><br><span class="line">    grid = np.stack((x1.flat, x2.flat), axis=1)</span><br><span class="line">    # print(grid.shape) # 10000*2</span><br><span class="line">    grid_predict = classifier.predict(grid)</span><br><span class="line">    grid_predict = grid_predict.reshape(x1.shape)  # 还原成网格形状</span><br><span class="line">    plt = Plot_Data(data)</span><br><span class="line">    # plt.xlim(0, 4.5)</span><br><span class="line">    # plt.ylim(1.5, 5)</span><br><span class="line">    plt.contour(x1, x2, grid_predict)</span><br><span class="line">   # plt.title(&apos;SVM Decision Boundary with C = 100 &apos;, fontsize=12)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos; Training Linear SVM(try C=1,100) &apos;&apos;&apos;</span><br><span class="line">def Train_SVM(data,X,y):</span><br><span class="line">    classifier = svm.SVC(C=100, kernel=&apos;rbf&apos;, gamma=30, decision_function_shape=&apos;ovr&apos;)</span><br><span class="line">    classifier.fit(X, y.ravel())</span><br><span class="line">    Plot_Boundary(X,classifier)</span><br></pre></td></tr></table></figure></p>
<p><img src="https://i.loli.net/2019/09/07/lHRYvwd6zPyLbjG.png" alt="Figure_22.png"><br><img src="https://i.loli.net/2019/09/07/Ol2yHNnoD8VAPXv.png" alt="Figure_23.png"><br><img src="https://i.loli.net/2019/09/07/WXRrvm2Di43dzNU.png" alt="Figure_24.png"><br>可以看出当C=1时，拟合不足，当C=100时有些过拟合，当C =10，gamma=50即np.pow(0.1,-2)/2的时候效果较好。gamma效果和C类似。</p>
<p>步骤四：更进一步，使用样本数据集三，用训练样本X训练SVM，交叉验证集Xval并找到最适合的参数。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Find_Best_Param(X,y,Xval,yval):</span><br><span class="line">    _C=0;_sigma=0;_predict=0.0</span><br><span class="line">    for C in [0.01,0.03,0.1,0.3,1,3,10,30]:</span><br><span class="line">        for sigma in [0.01,0.03,0.1,0.3,1,3,10,30]:</span><br><span class="line">            classifier=svm.SVC(C=C,kernel=&apos;rbf&apos;,gamma=np.power(sigma,-2.0)/2,decision_function_shape=&apos;ovr&apos;)</span><br><span class="line">            classifier.fit(X,y)</span><br><span class="line">            predict=classifier.score(Xval,yval) # Returns the mean accuracy on the given test data and labels</span><br><span class="line">            # print(C, sigma, predict)</span><br><span class="line">            if predict&gt;_predict:</span><br><span class="line">                _predict,_C,_sigma=predict,C,sigma</span><br><span class="line">    return _C,_sigma,_predict</span><br><span class="line"></span><br><span class="line">C,sigma,prediction=Find_Best_Param(X,y.ravel(),data[&apos;Xval&apos;],data[&apos;yval&apos;].ravel())</span><br><span class="line">print(C,sigma,prediction) # expect to see: 1 0.1 0.965</span><br><span class="line">Train_SVM(data,X,y,C,sigma)</span><br></pre></td></tr></table></figure></p>
<p><img src="https://i.loli.net/2019/09/07/hEB53caR7IsNFzd.png" alt="Figure_25.png"><br>由于在编程的时候碰到一些问题，解释如下：</p>
<ul>
<li>.score()可以查看手册，在参考资料中依葫芦画瓢</li>
<li>pow()函数最好用实数，整数可能报错</li>
<li>将可能的C及&sigma;用列表或元组[]存起来遍历，如果用{},会忽略某些元素要和C语言区分开</li>
<li>此处完整代码见顶，其余模块的完整代码大同小异</li>
</ul>
<h4 id="Spam-Classification"><a href="#Spam-Classification" class="headerlink" title="Spam Classification"></a>Spam Classification</h4><p>任务：训练一个分类器来辨别一封邮件x是否为垃圾邮件(y=0)，需要从邮件中提取特征建立特征向量。</p>
<p>步骤一： 邮件预处理<br><img src="https://i.loli.net/2019/09/09/XNqacHJAFu5dgmQ.png" alt="2019-09-09 15-12-31 的屏幕截图.png"><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Process_Email(file_contents):</span><br><span class="line">    file_contents = file_contents.lower() # Lower case</span><br><span class="line">    file_contents=re.sub(r&apos;&lt;[^&lt;&gt;]&gt;&apos;,&apos; &apos;,file_contents) # &lt;...&gt;</span><br><span class="line">    file_contents=re.sub(&apos;(http|https)://[^\s]*&apos;,&apos;heepaddr&apos;,file_contents) # Handle URLS</span><br><span class="line">    file_contents=re.sub(&apos;[^\s]+@[^\s]+&apos;, &apos;emailaddr&apos;, file_contents) # andle Email Addresses</span><br><span class="line">    file_contents=re.sub(&apos;[0-9]+&apos;,&apos;number&apos;,file_contents) # Handle Numbers</span><br><span class="line">    file_contents=re.sub(&apos;[$]+&apos;,&apos;dollar&apos;,file_contents) # Handle $ sign</span><br><span class="line"></span><br><span class="line">    stemmer = nltk.stem.porter.PorterStemmer()</span><br><span class="line">    tokens = re.split(&apos;[ \@\$\/\#\.\-\:\&amp;\*\+\=\[\]\?\!\(\)\&#123;\&#125;\,\&apos;\&quot;\&gt;\_\&lt;\;\%]&apos;, file_contents)</span><br><span class="line">    tokenlist=[]</span><br><span class="line">    for token in tokens:</span><br><span class="line">        token = re.sub(&apos;[^a-zA-Z0-9]&apos;, &apos;&apos;, token) # 删除任何非字母数字的字符</span><br><span class="line">        stemmed = stemmer.stem(token)  # Use the Porter stemmer to 提取词根</span><br><span class="line">        if len(token)==0:continue</span><br><span class="line">        tokenlist.append(stemed)</span><br><span class="line">        print(token,end=&quot; &quot;)</span><br><span class="line">    return tokenlist</span><br></pre></td></tr></table></figure></p>
<p>关于以上代码：</p>
<ol>
<li>涉及到Python正则表达式，这个尚处与知识盲区</li>
<li>大体思路是整体变小写，去除HTML标签，换掉数字地址和邮箱等，换掉各种符号。最后分割成单词列表，每个单词去除特殊符号再提取词根</li>
<li>参考资料顶部已给出</li>
</ol>
<p>步骤二：将单词映射成数字，单词表中已给出映射关系，然后新建向量，单词列表中的单词在邮件中出线过则为1，否则为0。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def TransForm(token,vocabList):</span><br><span class="line">    # print(type(vocabList))  # &lt;class &apos;list&apos;&gt;</span><br><span class="line">    # index = [i for i in range(len(vocabList)) if vocabList[i] in token] # except to see: 62 45 1899</span><br><span class="line">    index = [vocabList.index(x) for x in token if vocabList.count(x) ] # except to see: 62 53 1899</span><br><span class="line">    print(len(token),len(index),len(vocabList))</span><br><span class="line">    return index</span><br><span class="line"></span><br><span class="line">def Extracting_Features(word_index,vocabList):</span><br><span class="line">    vector=np.zeros(len(vocabList))</span><br><span class="line">    vector[[i for i in word_indices]]=1</span><br><span class="line">    return vector</span><br><span class="line"></span><br><span class="line">file_contents=Read_File(&apos;./ex6/emailSample1.txt&apos;)</span><br><span class="line">vocabList=Read_File(&apos;./ex6/vocab.txt&apos;)</span><br><span class="line">vocabList = vocabList.split()  # default &apos; &apos; as split sign</span><br><span class="line">for x in vocabList: # 去除数字</span><br><span class="line">    if x[0]&gt;=&apos;0&apos; and x[0]&lt;=&apos;9&apos;:vocabList.remove(x)</span><br><span class="line">word_indices=TransForm(Process_Email(file_contents),vocabList)</span><br><span class="line">features=Extracting_Features(word_indices,vocabList)</span><br></pre></td></tr></table></figure></p>
<h4 id="Training-SVM-for-Spam-Classification"><a href="#Training-SVM-for-Spam-Classification" class="headerlink" title="Training SVM for Spam Classification"></a>Training SVM for Spam Classification</h4><p>使用所给的邮件特征向量进行训练和测试SVM，文件分别为： spamTrain.mat，spamTest.mat<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Train_and_Test_SpamClass():</span><br><span class="line">    spam_train, X_train, y_train = Get_Data(&apos;./ex6/spamTrain.mat&apos;)</span><br><span class="line">    spam_test=sio.loadmat(&apos;./ex6/spamTest.mat&apos;) # ,X_test,y_test</span><br><span class="line">    X_test,y_test=spam_test[&apos;Xtest&apos;],spam_test[&apos;ytest&apos;]</span><br><span class="line">    print(&apos;(this may take 1 to 2 minutes) ...&apos;)</span><br><span class="line">    classifier = Train_SVM(spam_train, X_train, y_train, C=0.1, sigma=0.1)</span><br><span class="line">    print(&apos;Training Accuracy: &#123;:.2%&#125;&apos;.format(classifier.score(X_train, y_train))) # (this may take 1 to 2 minutes) ...</span><br><span class="line">Training Accuracy: 99.83%</span><br><span class="line">    print(&apos;Test Accuracy: &#123;:.2%&#125;&apos; .format(classifier.score(X_test,y_test))) # Test Accuracy: 98.90%</span><br></pre></td></tr></table></figure></p>
<h4 id="预测邮件"><a href="#预测邮件" class="headerlink" title="预测邮件"></a>预测邮件</h4><p>根据emailSample1.txt和emailSample2.txt提取的特征向量进行预测此邮件是否为垃圾邮件。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Predict_email(classifier,email):</span><br><span class="line">    X=Preprocessing_Emails(email)</span><br><span class="line">    return classifier.predict(X.reshape(1,-1))</span><br><span class="line"></span><br><span class="line">path=[&apos;./ex6/emailSample1.txt&apos;,&apos;./ex6/emailSample2.txt&apos;,&apos;./ex6/spamSample1.txt&apos;,&apos;./ex6/spamSample2.txt&apos;]</span><br><span class="line">classifier=Train_and_Test_SpamClass()</span><br><span class="line">for Sample_path in path:</span><br><span class="line">    file_contents = Read_File(Sample_path)</span><br><span class="line">    print(&apos;%-23s: %2s&apos; %(Sample_path,Predict_email(classifier,file_contents)))</span><br><span class="line">print(&apos;1 indicates spam, 0 indicates not spam&apos;)</span><br></pre></td></tr></table></figure></p>
<p>输出如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(this may take 1 to 2 minutes) ...</span><br><span class="line">./ex6/emailSample1.txt : [0]</span><br><span class="line">./ex6/emailSample2.txt : [0]</span><br><span class="line">./ex6/spamSample1.txt  : [1]</span><br><span class="line">./ex6/spamSample2.txt  : [1]</span><br><span class="line">1 indicates spam, 0 indicates not spam</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>支持向量机</title>
    <url>/2019/09/03/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1052089362&amp;courseId=1004570029" target="_blank" rel="noopener">学习地址</a></center></h3><p>Support Vector Machine（SVM）</p>
<hr>
<h4 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h4><p>回顾逻辑回归的激活函数sigmoid<br><img src="https://i.loli.net/2019/09/03/Eh78yUXjPuGqWCA.png" alt="2019-09-03 09-50-57 的屏幕截图.png"><br>逻辑回归单个样本的代价函数如下，分别考虑当y=0和y=1时代价函数的变化情况  在这个基础上稍微变化一下，取z=&theta;<sup>T</sup>x的某一点，作两条直线如图，这个新的代价函数为cost<sub>_1</sub>(z)和cost<sub>_0</sub>(z)，这里下标0和1只是代表y=0或y=1。<br>有了这些定义就可构建支持向量机<br><img src="https://i.loli.net/2019/09/03/TZuB8W53eJD6v4I.png" alt="2019-09-03 09-59-39 的屏幕截图.png"></p>
<p>那么，用cost<sub>_1</sub>(z)和cost<sub>_0</sub>(z)分别代替-log(h<sub>&theta;</sub>(x<sup>(i)</sup>))和(-log(1-h<sub>&theta;</sub>(x<sup>(i)</sup>)))得到一个变体代价函数，对于1/m这只是个常数，完全不影响最优解，故可以省略。<br>将逻辑回归代价函数替换成A，正则化项替换成B，那么J(&theta;)=A+&lambda;B。&lambda;代表正则化项所占的权重，即&lambda;越大说明正则化项越重要。对于SVM，同样我们使用C来表示这个常数，我们需要优化的是CA+B，如果C很小那么C*A也很小，同样可以表示正则化项很重要,实际上C=1/&lambda;时和A+&lambda;B效果是一样的。<br>SVM中A则是使用cost替换了激活函数h<sub>&theta;</sub>(z)之后的A<sup>‘</sup>。那么就得到SVM需要优化的代价函数如下：<br><img src="https://i.loli.net/2019/09/03/K7wzSObad83Z2ut.png" alt="2019-09-03 10-28-31 的屏幕截图.png"></p>
<p>SVM并不会像逻辑回归输出概率，而是通过优化代价函数得到参数&theta;然后直接预测y=0或者y=1。<br><img src="https://i.loli.net/2019/09/03/ONkj7cviSRm5DfW.png" alt="2019-09-03 10-46-24 的屏幕截图.png"></p>
<h4 id="直观上对大间隔的理解"><a href="#直观上对大间隔的理解" class="headerlink" title="直观上对大间隔的理解"></a>直观上对大间隔的理解</h4><p>SVM有时也被成为大间距分类器。<br>实际上当&theta;<sup>T</sup>X&ge;0就可以正确把正负样本分开，但SVM并不仅仅将样本分开，而是考虑一个安全距离。下图已经给出关于z的代价函数。</p>
<p><img src="https://i.loli.net/2019/09/03/xGw2cj5Kh8kEsdy.png" alt="2019-09-03 11-12-46 的屏幕截图.png"><br>这个间距是指不同的分类效果离样本的距离，比如下图中黑色分隔线就是比较理想的一种情况。SVM会尽量把正负样本最大间距地隔开。<br><img src="https://i.loli.net/2019/09/03/K6a3roQE9xWkVNs.png" alt="2019-09-03 10-58-19 的屏幕截图.png"></p>
<h4 id="大间隔分类器的数学原理"><a href="#大间隔分类器的数学原理" class="headerlink" title="大间隔分类器的数学原理"></a>大间隔分类器的数学原理</h4><p>SVM是如何使得这个间距最大呢？<br>先看一个内积的例子：设u=[[u1,u2]]，v=[[v1,v2]]<br>那么向量u(两个坐标轴上投影长度分别为u1、u2)的长度||u||=&radic;<code>(u1*u1+u2*u2)</code>，这个长度就是u的范数<br>u和v的内积u<sup>T</sup>v如何计算？<br>定义p：v在u上的直角投影<br>那么u<sup>T</sup>v=p*||u||=u1v1+u2v2<br><img src="https://i.loli.net/2019/09/03/rwIdBiKDmWtz74k.png" alt="2019-09-03 11-58-55 的屏幕截图.png"></p>
<p>引入到SVM中，那么&theta;<sup>T</sup>x<sup>(i)</sup>=p<sup>(i)</sup>||&theta;||=&theta;<sub>1</sub>x<sub>1</sub><sup>(i)</sup>+&theta;<sub>2</sub>x<sub>2</sub><sup>(i)</sup>，p<sup>(i)</sup>是x<sup>(i)</sup>在向量&theta;上的投影<br><img src="https://i.loli.net/2019/09/03/hKy14bBMcukzGWd.png" alt="2019-09-03 18-44-28 的屏幕截图.png"></p>
<p>为什么SVM会选择间距最大的分类方式？<br>如下，如果想分出正负样本，选择左边的分类方式发现投影p<sup>(i)</sup>非常小，这就使得&amp;theta的范数必须非常大，而右边的分类方式则不存在这种问题。故决策边界离样本尽可能远。<br><img src="https://i.loli.net/2019/09/03/HyQJai1TNr2tgcu.png" alt="2019-09-03 18-55-54 的屏幕截图.png"></p>
<h4 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h4><p>用f<sub>_1</sub>代替特征x<sub>_1</sub>，f<sub>_2</sub>代替特征x<sub>_2</sub>，f<sub>_i</sub>代替特征x<sub>_j</sub>x<sub>_k</sub>。这样f的阶数可能是非常大的，如何更好地选择特征f？  </p>
<p>选择三个基点l<sup>(1)</sup>，l<sup>(2)</sup>，l<sup>(3)</sup>。<br>对于给定x：f<sub>_1</sub>=similarity(x,l<sup>(1)</sup>)=exp(-||x-l<sup>(1)</sup>||<sup>2</sup> / (2&sigma;<sup>2</sup>))<br>f<sub>_2</sub>…同理。  </p>
<p>这里相似度函数在数学上就是一个核函数，核函数有很多，这里使用的是高斯核函数，故也记作k(x,l<sup>(i)</sup>)<br><img src="https://i.loli.net/2019/09/04/sdSpa7bBYei6XQy.png" alt="2019-09-04 10-26-28 的屏幕截图.png"><br><img src="https://i.loli.net/2019/09/04/fn3WXegwvbVH18j.png" alt="2019-09-04 10-31-33 的屏幕截图.png"></p>
<p>如何选取基点l<sup>(i)</sup>呢？<br>实际上，基准点和训练样本点是一致的，即l<sup>(i)</sup>=x<sup>(i)</sup><br><img src="https://i.loli.net/2019/09/04/UVBmWdxTwf2a1R7.png" alt="2019-09-04 11-21-59 的屏幕截图.png"></p>
<p>优化目标整理：<br><img src="https://i.loli.net/2019/09/04/zvlxhSZVUHYLEbq.png" alt="2019-09-04 11-33-56 的屏幕截图.png"></p>
<p>如何选择参数C(=1/&lambda;)和&sigma;：<br><img src="https://i.loli.net/2019/09/04/LwWcnVtNFDvfjx3.png" alt="2019-09-04 11-39-18 的屏幕截图.png"></p>
<h4 id="使用SVM"><a href="#使用SVM" class="headerlink" title="使用SVM"></a>使用SVM</h4><p>有很多精准的计算库或者软件包进行求参优化问题，例如liblinear，libsvm等来求解参数&theta;，手动写的优化方法很难和专业库进行比优。  </p>
<p>但有几件事还是需要自己来具体实现</p>
<ul>
<li>参数C的选择</li>
<li>核函数(kernel)的选择(相似度函数similarity function)：例如，不带参数的核函数（线性核函数），预测y=1如果&theta;<sup>T</sup>x&ge;0；还可以选择带参数&sigma;的高斯核函数</li>
</ul>
<p><img src="https://i.loli.net/2019/09/04/QeTJbwYPcEjrXUS.png" alt="2019-09-04 15-58-00 的屏幕截图.png"></p>
<p>默塞尔定理，SVM数值计算包默认满足默塞尔定理以便优化方法正确计算<br>其他的核函数选择：</p>
<ul>
<li>多项式核函数：k(x,l)=(x<sup>T</sup>l+constant)<sup>2/3/4</sup>，constant=1/5/….</li>
<li>字符串核函数</li>
<li>卡方核函数</li>
<li>直方相交核函数 </li>
</ul>
<p>多分类器：<br><img src="https://i.loli.net/2019/09/04/mV8YSAvLOr65UhW.png" alt="2019-09-04 16-29-23 的屏幕截图.png"></p>
<p>选择逻辑回归还是SVM？</p>
<ul>
<li>特征数量n很大（n=10000），n&ge;m：选择逻辑回归或线形核函数</li>
<li>如果n很小（1-1000），m适中：选择高斯核函数</li>
<li>如果n很小，m很大：增加特征，使用逻辑回归或线形核函数</li>
</ul>
<p>神经网络可能都适用，但训练可能要更慢些。</p>
<hr>
<p><center><table><tr><td bgcolor="yellow">华丽分割</td></tr></table></center></p>
<p><center><font size="5px" color="red">以下为补充，更新于2019.12.25</font></center></p>
<hr>
<h3 id="sklearn中的支持向量机SVM"><a href="#sklearn中的支持向量机SVM" class="headerlink" title="sklearn中的支持向量机SVM"></a><font size="5px" color="red">sklearn中的支持向量机SVM</font></h3><h4 id="超平面"><a href="#超平面" class="headerlink" title="超平面"></a><font size="3px" color="red">超平面</font></h4><p>在几何中，<strong>超平面是一个空间的子空间</strong>，它是维度比所在空间小一维的空间。 如果数据空间本身是三维的， 则其超平面是二维平面，而如果数据空间本身是二维的，则其超平面是一维的直线。</p>
<p>在二分类问题中，如果一个超平面能够将数据划分为两个集合，其中每个集合中包含单独的一个类别，则称这个超平面是数据的<strong>决策边界</strong></p>
<p>样本点到决策边界的距离叫做<strong>边际(margin)</strong>，通常记作d，SVM是使得d越大越好，这样泛化误差就会小了。<strong>拥有更大边际的决策边界在分类中的泛化误差更小。支持向量机，就是通过找出边际最大的决策边界，来对数据进行分类的分类器</strong>。</p>
<h4 id="sklearn中的支持向量机"><a href="#sklearn中的支持向量机" class="headerlink" title="sklearn中的支持向量机"></a><font size="3px" color="red">sklearn中的支持向量机</font></h4><table>
<thead>
<tr>
<th>类</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>svm.LinearSVC</td>
<td>线性支持向量分类</td>
</tr>
<tr>
<td>svm.LinearSVR</td>
<td>线性支向量回归</td>
</tr>
<tr>
<td>svm.SVC</td>
<td>非线性多维支持向量分类</td>
</tr>
<tr>
<td>svm.SVR</td>
<td>非线性多维支持向量回归</td>
</tr>
<tr>
<td>svm.NuSVC</td>
<td>Nu支持向量分类</td>
</tr>
<tr>
<td>svm.NuSVR</td>
<td>Nu支持向量回归</td>
</tr>
<tr>
<td>svm.OneClassSVM</td>
<td>无监督异常值检测</td>
</tr>
<tr>
<td><strong>直接使用libsvm的函数</strong></td>
<td><strong>含义</strong></td>
</tr>
<tr>
<td>svm.libsvm.cross_validation()</td>
<td>SVM专用的交叉验证</td>
</tr>
<tr>
<td>svm.libsvm.decision_function()</td>
<td>SVM专用的预测边际函数（libsvm名称为predict_values）</td>
</tr>
<tr>
<td>svm.libsvm.fit()</td>
<td>使用libsvm训练模型</td>
</tr>
<tr>
<td>svm.libsvm.predict()</td>
<td>给定模型预测X的目标值</td>
</tr>
<tr>
<td>svm.libsvm.predict_proba()</td>
<td>预测概率</td>
</tr>
</tbody>
</table>
<p>注意： </p>
<ul>
<li>除了特别表明是线性的两个类LinearSVC和LinearSVR之外，其他的所有类都是同时支持线性和非线性的。</li>
<li>NuSVC和NuSVC可以手动调节支持向量的数目，其他参数都与最常用的SVC和SVR一致。注意OneClassSVM是无监督的类。</li>
</ul>
<h4 id="sklearn-svm-SVC"><a href="#sklearn-svm-SVC" class="headerlink" title="sklearn.svm.SVC"></a><font size="3px" color="red">sklearn.svm.SVC</font></h4><ul>
<li><code>class sklearn.svm.SVC (C=1.0, kernel=’rbf’, degree=3, gamma=’auto_deprecated’, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=’ovr’, random_state=None)</code></li>
</ul>
<p>参数解释：</p>
<ul>
<li><code>kernel</code>为核函数，可选<code>[‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ ]</code>， kernel=’rbf’时（default），为高斯核</li>
<li>错误项的惩罚系数。C越大，即<strong>对分错样本的惩罚程度越大</strong>，<code>C</code>越大分类效果越好，但可能过拟合泛化误差大（default C=1.0）</li>
<li><code>decision_function_shape</code>为分类器，可选<code>[‘ovo’,’ovr’]</code>分别表示一对一分类器，一对多分类。默认为’ovr’，’one vs rest’。</li>
</ul>
<h3 id="实践与调参"><a href="#实践与调参" class="headerlink" title="实践与调参"></a><font size="5px" color="red">实践与调参</font></h3><p>数据:</p>
<ul>
<li>数据集选用乳腺癌数据集。</li>
</ul>
<h4 id="kernel的选择"><a href="#kernel的选择" class="headerlink" title="kernel的选择"></a><font size="3px" color="red">kernel的选择</font></h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.svm import SVC</span><br><span class="line">from sklearn.datasets import load_breast_cancer</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from time import time</span><br><span class="line"></span><br><span class="line">data=load_breast_cancer()</span><br><span class="line">X=data.data</span><br><span class="line">y=data.target</span><br><span class="line"># print(X.shape,y.shape) # (569, 30) (569,)</span><br><span class="line"># print(np.unique(y)) # [0 1]</span><br><span class="line"></span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest=train_test_split(X,y,test_size=0.3,random_state=10)</span><br><span class="line">kernel=[&apos;linear&apos;, &apos;rbf&apos;, &apos;sigmoid&apos;] # poly，这核函数特别耗时间，所以不做演示</span><br><span class="line"></span><br><span class="line">for kernel in kernel:</span><br><span class="line">    time0=time()</span><br><span class="line">    clf=SVC(kernel=kernel</span><br><span class="line">            ,C=1.0</span><br><span class="line">            ,gamma=&apos;auto&apos;</span><br><span class="line">            # ,degree=1</span><br><span class="line">            ,cache_size=5000 # 允许使用的内存MB</span><br><span class="line">            ).fit(Xtrain,Ytrain)</span><br><span class="line">    print(&apos;The accuracy under kernel %s is %.2f&apos; %(kernel,clf.score(Xtest,Ytest)))</span><br><span class="line">    print(time()-time0)</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">The accuracy under kernel linear is 0.96</span><br><span class="line">1.7291104793548584</span><br><span class="line">The accuracy under kernel rbf is 0.65</span><br><span class="line">0.04491472244262695</span><br><span class="line">The accuracy under kernel sigmoid is 0.65</span><br><span class="line">0.004982948303222656</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>
<p>注意</p>
<ul>
<li><p><code>degree=</code>参数是多项式核<code>ploy</code>特有的，如果设置为1就表示线性核，这样上述代码的结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">The accuracy under kernel linear is 0.96</span><br><span class="line">1.6605808734893799</span><br><span class="line">The accuracy under kernel poly is 0.95</span><br><span class="line">0.0538640022277832</span><br><span class="line">The accuracy under kernel rbf is 0.65</span><br><span class="line">0.043853759765625</span><br><span class="line">The accuracy under kernel sigmoid is 0.65</span><br><span class="line">0.0060122013092041016</span><br></pre></td></tr></table></figure>
</li>
<li><p>可以看出，线性核的效果特别好，据此就可以说明所使用的数据就是线性的</p>
</li>
<li>但是存在一个问题是正确确实应该选择线性核，但是它的运行时间确是个缺陷，这个时候就要从<strong>数据预处理</strong>的角度出发了，实际上做了标准化后运行时间和其他核函数的结果都会很出色</li>
</ul>
<h4 id="其他参数的选取问题"><a href="#其他参数的选取问题" class="headerlink" title="其他参数的选取问题"></a><font size="3px" color="red">其他参数的选取问题</font></h4><p>确定单个参数：</p>
<ul>
<li>学习曲线</li>
</ul>
<p>协同参数：比如多项式核函数<code>ploy</code>有三个可调参数<code>gamma、degree、coef0</code></p>
<ul>
<li><strong>网格搜索</strong><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.svm import SVC</span><br><span class="line">from sklearn.datasets import load_breast_cancer</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from time import time</span><br><span class="line"></span><br><span class="line">data=load_breast_cancer()</span><br><span class="line">X=data.data</span><br><span class="line">y=data.target</span><br><span class="line"></span><br><span class="line">from sklearn.model_selection import StratifiedShuffleSplit # 专门用来设置交叉验证模式</span><br><span class="line">from sklearn.model_selection import GridSearchCV # 带交叉验证的网格搜索</span><br><span class="line"></span><br><span class="line">time0=time()</span><br><span class="line">gamma_range=np.logspace(-5,1,10) # Return numbers spaced evenly on a log scale.</span><br><span class="line">coef0_range=np.linspace(0,5,10)</span><br><span class="line"></span><br><span class="line">param_grid=dict(gamma=gamma_range,coef0=coef0_range)</span><br><span class="line"></span><br><span class="line">cv=StratifiedShuffleSplit(n_splits=5,test_size=0.3,random_state=10) # 数据集分n份，每一份数据轮流做测试集，即做5次交叉验证</span><br><span class="line">grid=GridSearchCV(SVC(kernel=&apos;poly&apos;,degree=1,cache_size=5000)</span><br><span class="line">                  ,param_grid=param_grid</span><br><span class="line">                  ,cv=cv)</span><br><span class="line"></span><br><span class="line">grid=grid.fit(X,y)</span><br><span class="line">print(&apos;The best parameters are %s with a score of %0.5f &apos; %(grid.best_params_,grid.best_score_))</span><br><span class="line">print(time()-time0)</span><br><span class="line"></span><br><span class="line"># 这段代码运行很慢，不过最终会返回最佳参数对与最高分数</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ML</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title>牛客练习赛50 部分题解</title>
    <url>/2019/09/02/%E7%89%9B%E5%AE%A2%E7%BB%83%E4%B9%A0%E8%B5%9B50-%E9%83%A8%E5%88%86%E9%A2%98%E8%A7%A3/</url>
    <content><![CDATA[<h3 id="牛客练习赛50"><a href="#牛客练习赛50" class="headerlink" title="牛客练习赛50"></a><center><a href="https://ac.nowcoder.com/acm/contest/1080#question" target="_blank" rel="noopener">牛客练习赛50</a></center></h3><p>老年选手已经拿不动刀了，但残存的一些斗志和激情足以支撑自己靠着仅剩的一点点经验继续下去。</p>
<h4 id="A-tokitsukaze-and-Connection"><a href="#A-tokitsukaze-and-Connection" class="headerlink" title="A    tokitsukaze and Connection"></a><a href="https://ac.nowcoder.com/acm/contest/1080/A" target="_blank" rel="noopener">A    tokitsukaze and Connection</a></h4><p>题意：给你一个字符串，判断所有相同的字母是否都在一起。<br>直接模拟即可。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include&lt;bits/stdc++.h&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">const int N=1e5+10;</span><br><span class="line">char s[N];</span><br><span class="line">int vis[101];</span><br><span class="line">int main()&#123;</span><br><span class="line">    int n;</span><br><span class="line">    while(~scanf(&quot;%d&quot;,&amp;n))</span><br><span class="line">    &#123;</span><br><span class="line">        scanf(&quot;%s&quot;,s);</span><br><span class="line">        memset(vis,0,sizeof(vis));</span><br><span class="line">        int f=1;</span><br><span class="line">        for(int i=0;i&lt;n;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            if(vis[s[i]-&apos;a&apos;]&amp;&amp;s[i-1]!=s[i]) f=0;</span><br><span class="line">            vis[s[i]-&apos;a&apos;]=1;</span><br><span class="line">        &#125;</span><br><span class="line">        if(f) puts(&quot;YES&quot;);</span><br><span class="line">        else puts(&quot;NO&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="B-tokitsukaze-and-Hash-Table"><a href="#B-tokitsukaze-and-Hash-Table" class="headerlink" title="B    tokitsukaze and Hash Table"></a><a href="https://ac.nowcoder.com/acm/contest/1080/B" target="_blank" rel="noopener">B    tokitsukaze and Hash Table</a></h4><p>题意：长度为n的哈希表，采用余数定址法，冲突便往右循环查找，求最终排列。<br>并差集做法，将循环相邻位置合并，递归查找更新。<br>STL做法，把允许存放的单元扔进set中，lower_bound存放位置然后删除即可。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include&lt;bits/stdc++.h&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">const int N=1e6+10;</span><br><span class="line">int nex[N],a[N];</span><br><span class="line">int find(int x)</span><br><span class="line">&#123;</span><br><span class="line">    return nex[x]==-1?x:nex[x]=find(nex[x]);</span><br><span class="line">&#125;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    int n;</span><br><span class="line">    while(~scanf(&quot;%d&quot;,&amp;n))</span><br><span class="line">    &#123;</span><br><span class="line">        memset(nex,-1,sizeof(nex));</span><br><span class="line">        memset(a,-1,sizeof(a));</span><br><span class="line">        int x,tc;</span><br><span class="line">        for(int i=0;i&lt;n;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            scanf(&quot;%d&quot;,&amp;x);</span><br><span class="line">            tc=x%n;</span><br><span class="line">            if(a[tc]!=-1) tc=find(tc);</span><br><span class="line">            nex[tc]=(tc+1)%n;</span><br><span class="line">            a[tc]=x;</span><br><span class="line">        &#125;</span><br><span class="line">        for(int i=0;i&lt;n;i++)</span><br><span class="line">            printf(&quot;%d%c&quot;,a[i],i==n-1?&apos;\n&apos;:&apos; &apos;);</span><br><span class="line">    &#125;</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="C-tokitsukaze-and-Soldier"><a href="#C-tokitsukaze-and-Soldier" class="headerlink" title="C    tokitsukaze and Soldier"></a><a href="https://ac.nowcoder.com/acm/contest/1080/C" target="_blank" rel="noopener">C    tokitsukaze and Soldier</a></h4><p>题意：n个士兵，每个士兵都有一个战斗力值和最多能容忍的人数。现让你组一个团使得互相包容且战斗力值最大。<br>优先队列简单应用。在一定的人数下当然是取战斗力最大的这些。先按人数从大到小排序，然后一个个扔进最小优先队列中，如果超出当前能容忍的人数便最小退出，更新答案。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include&lt;bits/stdc++.h&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">const int N=1e5+10;</span><br><span class="line">struct node</span><br><span class="line">&#123;</span><br><span class="line">    int v,s;</span><br><span class="line">    friend bool operator&lt;(node a,node b)</span><br><span class="line">    &#123;</span><br><span class="line">        return a.v&gt;b.v;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;a[N];</span><br><span class="line">int cmp(node a,node b)</span><br><span class="line">&#123;</span><br><span class="line">    if(a.s!=b.s) return a.s&gt;b.s;</span><br><span class="line">    return a.v&gt;b.v;</span><br><span class="line">&#125;</span><br><span class="line">priority_queue&lt;node&gt;q;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    int n;</span><br><span class="line">    while(~scanf(&quot;%d&quot;,&amp;n))</span><br><span class="line">    &#123;</span><br><span class="line">        while(!q.empty()) q.pop();</span><br><span class="line">        long long ans=0,tmp=0;</span><br><span class="line">        for(int i=0;i&lt;n;i++)</span><br><span class="line">            scanf(&quot;%d%d&quot;,&amp;a[i].v,&amp;a[i].s);</span><br><span class="line">        sort(a,a+n,cmp);</span><br><span class="line">        int cnt=0;</span><br><span class="line">        for(int i=0;i&lt;n;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            q.push(a[i]);</span><br><span class="line">            cnt++;</span><br><span class="line">            tmp+=a[i].v;</span><br><span class="line">            while(cnt&gt;a[i].s) // 需要拿出战斗力最小的几个，优先队列按战斗力最小优先</span><br><span class="line">            &#123;</span><br><span class="line">                tmp-=q.top().v;</span><br><span class="line">                q.pop();</span><br><span class="line">                cnt--;</span><br><span class="line">            &#125;</span><br><span class="line">            ans=max(ans,tmp);</span><br><span class="line">        &#125;</span><br><span class="line">        printf(&quot;%lld\n&quot;,ans);</span><br><span class="line">    &#125;</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">/*</span><br><span class="line">6</span><br><span class="line">1 2</span><br><span class="line">3 3</span><br><span class="line">2 2</span><br><span class="line">2 4</span><br><span class="line">1 4</span><br><span class="line">4 5</span><br><span class="line">*/</span><br></pre></td></tr></table></figure></p>
<h4 id="D-tokitsukaze-and-Event"><a href="#D-tokitsukaze-and-Event" class="headerlink" title="D    tokitsukaze and Event"></a><a href="https://ac.nowcoder.com/acm/contest/1080/D" target="_blank" rel="noopener">D    tokitsukaze and Event</a></h4><p>题意：给你一张图，每条边有两个权值，代表两种状态下走的花费，现可以更改一次状态且到达终点后状态必须改变，若限制某些点不可更改状态从s点到t点的最小花费。<br>两边最短路，然后扔进最小优先队列中，只要当前点不可更改直接弹出。或者后缀取min。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include &lt;map&gt;</span><br><span class="line">#include &lt;set&gt;</span><br><span class="line">#include &lt;deque&gt;</span><br><span class="line">#include &lt;queue&gt;</span><br><span class="line">#include &lt;stack&gt;</span><br><span class="line">#include &lt;cmath&gt;</span><br><span class="line">#include &lt;bitset&gt;</span><br><span class="line">#include &lt;vector&gt;</span><br><span class="line">#include &lt;cstdio&gt;</span><br><span class="line">#include &lt;cstring&gt;</span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;algorithm&gt;</span><br><span class="line">#include&lt;bits/stdc++.h&gt;</span><br><span class="line">using  namespace std;</span><br><span class="line">typedef long long ll;</span><br><span class="line">typedef unsigned long long ul;</span><br><span class="line">//#define sc(x) scanf(&quot;%d&quot;,&amp;x)</span><br><span class="line">#define pd(x) printf(&quot;%d\n&quot;,x)</span><br><span class="line">const double eps=1e-8;</span><br><span class="line">const double PI=acos(-1.0);</span><br><span class="line">const ll INF=1e16;</span><br><span class="line">const int mod=1e9+7;</span><br><span class="line">const int N=1e5+10;</span><br><span class="line">ll d[2][N];</span><br><span class="line">int n,m,tot,head[N],vis[N];</span><br><span class="line">struct Edge</span><br><span class="line">&#123;</span><br><span class="line">    int to,next,day,night;</span><br><span class="line">&#125; e[N*2];</span><br><span class="line">struct node</span><br><span class="line">&#123;</span><br><span class="line">    int v;</span><br><span class="line">    ll hurt;</span><br><span class="line">    friend bool operator&lt;(node a,node b)</span><br><span class="line">    &#123;</span><br><span class="line">        return a.hurt&gt;b.hurt;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line">void init()</span><br><span class="line">&#123;</span><br><span class="line">    tot=0;</span><br><span class="line">    memset(head,-1,sizeof(head));</span><br><span class="line">&#125;</span><br><span class="line">void add(int u,int v,int day,int night)</span><br><span class="line">&#123;</span><br><span class="line">    e[tot].to=v,e[tot].next=head[u],e[tot].day=day,e[tot].night=night;</span><br><span class="line">    head[u]=tot++;</span><br><span class="line">    e[tot].to=u,e[tot].next=head[v],e[tot].day=day,e[tot].night=night;</span><br><span class="line">    head[v]=tot++;</span><br><span class="line">&#125;</span><br><span class="line">priority_queue&lt;node&gt;q,q1;</span><br><span class="line">void dijkstra_heap(int s,int flag)</span><br><span class="line">&#123;</span><br><span class="line">    while(!q.empty())</span><br><span class="line">        q.pop();</span><br><span class="line">    for(int i=1; i&lt;=n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        if(flag)</span><br><span class="line">            d[1][i]=(i==s?0:INF);</span><br><span class="line">        else</span><br><span class="line">            d[0][i]=(i==s?0:INF);</span><br><span class="line">        vis[i]=0;</span><br><span class="line">    &#125;</span><br><span class="line">    q.push(node&#123;s,0&#125;);</span><br><span class="line">    node tmp;</span><br><span class="line">    while(!q.empty())</span><br><span class="line">    &#123;</span><br><span class="line">        tmp=q.top();</span><br><span class="line">        q.pop();</span><br><span class="line">        int u=tmp.v;</span><br><span class="line">        if(vis[u]) continue;</span><br><span class="line">        vis[tmp.v]=1;</span><br><span class="line">        for(int i=head[u]; i+1; i=e[i].next)</span><br><span class="line">        &#123;</span><br><span class="line">            int v=e[i].to;</span><br><span class="line">            if(flag)</span><br><span class="line">            &#123;</span><br><span class="line">                if(d[1][v]&gt;d[1][u]+e[i].night)</span><br><span class="line">                &#123;</span><br><span class="line">                    d[1][v]=d[1][u]+e[i].night;</span><br><span class="line">                        q.push(node&#123;v,d[1][v]&#125;);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            else</span><br><span class="line">            &#123;</span><br><span class="line">                if(d[0][v]&gt;d[0][u]+e[i].day)</span><br><span class="line">                &#123;</span><br><span class="line">                    d[0][v]=d[0][u]+e[i].day;</span><br><span class="line">                        q.push(node&#123;v,d[0][v]&#125;);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    while(~scanf(&quot;%d%d&quot;,&amp;n,&amp;m))</span><br><span class="line">    &#123;</span><br><span class="line">        init();</span><br><span class="line">        int s,t,v,u,day,night;</span><br><span class="line">        for(int i=0; i&lt;m; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            scanf(&quot;%d%d%d%d&quot;,&amp;u,&amp;v,&amp;day,&amp;night);</span><br><span class="line">            add(u,v,day,night);</span><br><span class="line">        &#125;</span><br><span class="line">        scanf(&quot;%d%d&quot;,&amp;s,&amp;t);</span><br><span class="line">        dijkstra_heap(s,0);</span><br><span class="line">        dijkstra_heap(t,1);</span><br><span class="line">        while(!q1.empty())</span><br><span class="line">            q1.pop();</span><br><span class="line">        for(int i=1; i&lt;=n; i++)</span><br><span class="line">            q1.push(node&#123;i,d[0][i]+d[1][i]&#125;);</span><br><span class="line">        for(int i=1; i&lt;=n; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            while(q1.top().v&lt;i) q1.pop();</span><br><span class="line">            printf(&quot;%lld\n&quot;,q1.top().hurt);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>后面的不会了。<br>放上题解：<a href="https://ac.nowcoder.com/discuss/231978?type=101&amp;order=0&amp;pos=3&amp;page=1" target="_blank" rel="noopener">【题解】牛客练习赛50</a></p>
]]></content>
      <categories>
        <category>题解</category>
      </categories>
      <tags>
        <tag>题解</tag>
        <tag>牛客网</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习系统设计</title>
    <url>/2019/09/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1051928204&amp;courseId=1004570029" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="确定执行的优先级"><a href="#确定执行的优先级" class="headerlink" title="确定执行的优先级"></a>确定执行的优先级</h4><p>例子：邮件垃圾分类器<br><img src="https://i.loli.net/2019/09/01/RcXdgUJ91BTxPmq.png" alt="2019-09-01 15-58-04 的屏幕截图.png"><br><img src="https://i.loli.net/2019/09/01/SFyqzXmYkfKI2xs.png" alt="2019-09-01 16-00-00 的屏幕截图.png"></p>
<h4 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h4><p>开始设计一个机器学习应用或系统时，如何快速作出理想效果？<br><img src="https://i.loli.net/2019/09/01/b6Ju3eomLwvXCEx.png" alt="2019-09-01 16-07-22 的屏幕截图.png"><br>例如垃圾邮件分类器，手动判别失败的交叉训练样本，查看问题出在哪里，然后对症下药<br><img src="https://i.loli.net/2019/09/01/ztXDevVbC9cQEZy.png" alt="2019-09-01 16-18-07 的屏幕截图.png"></p>
<p> <strong>数值估计</strong>：是否应该区分单复数/大小写等，查看错误率作出选择。</p>
<h4 id="不对称分类的误差评估"><a href="#不对称分类的误差评估" class="headerlink" title="不对称分类的误差评估"></a>不对称分类的误差评估</h4><p>例子：癌症预测<br>偏斜类（skewed class）:  y=1/0的样本特别少，而另一方的样本数特别多，这样预测出来的结果甚至可能还没总是预测为多的一方这样的假算法效果好</p>
<p>如何解决上面这种情况？需要一个不同的误差评估度量值，例如<strong>查准率与召回率</strong></p>
<p>查准率precision：患有癌症的病人有多大比率的病人是真正患有癌症的。查准率越高越好<br>召回率recall：患有癌症的病人有多大几率正确预测出来。越高越好。<br><img src="https://i.loli.net/2019/09/01/yASVDiUFM56CEx2.png" alt="2019-09-01 16-56-03 的屏幕截图.png"></p>
<p>通过查准率与召回率我们可以知道一个模型好与坏</p>
<h4 id="精确度和召回率的权衡"><a href="#精确度和召回率的权衡" class="headerlink" title="精确度和召回率的权衡"></a>精确度和召回率的权衡</h4><p>我们已经有了查准率和召回率的定义，癌症预测的例子中所是哟使用的逻辑回归模型，当h<sub>&theta;</sub>(x)&ge;threshold时，预测为1。通常threshold=0.5，当threshold增大时，预测为1的数量会减小，但准确率查准率增大，而召回率会减小；当threshold减小时，情况相反。如何自动选择阈值threshold？<br><img src="https://i.loli.net/2019/09/01/kGVIscPpmgqx9yA.png" alt="2019-09-01 17-18-56 的屏幕截图.png"></p>
<p>如何自动选择阈值threshold？<br>比较常见的一种：F值或叫F_1值<br><img src="https://i.loli.net/2019/09/01/pwrIAblJgfi5EyX.png" alt="2019-09-01 17-20-57 的屏幕截图.png"></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>2019.8.4 每周总结</title>
    <url>/2019/08/04/2019-8-4-%E6%AF%8F%E5%91%A8%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>就要回家了，来这一个月了，想不到也是过的如此之快。<br>实验室就我一个人，再见大家得开学了。但这周实际没做什么，很放松。</p>
<ul>
<li>周一，学习神经网络反向传播算法理论</li>
<li>周二，发现没有作业，可能是有点难，幕课网上把本章作业删了，于是打算做下一章作业，其实以前学过的正则化线形回归</li>
<li>周三，阅读文献，把重点部分看看，借助翻译，速度要比第一篇快很多</li>
<li>周四，继续看文献，跳过了methdology部分，一篇文章基本就看完了，但方法那里真的看不懂</li>
<li>周五，把偏差和方差理论学习了，实现后半部分作业</li>
<li>周六，上午导师开会，下午卡在一个地方一下午才明白</li>
<li>周日，上午直接没来，下午打算实现可又卡了一个地方，到晚上才实现一小部分</li>
</ul>
<p>这几天的时间实验室人都走的差不多了，这周实在很放松，要回家了大家都没什么压力感。不过我们还是每周都有OKR，这周的作业到现在还没写完。。。<br>这周搭打一场多校，被虐的不行，太惨了。<br>昨晚看了《我不是药神》，还行，引发挺多思考。</p>
]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title>Effective and Efficient Detection of Moving Targets From a UAV’s Camera</title>
    <url>/2019/08/02/Effective-and-Efficient-Detection-of-Moving-Targets-From-a-UAV%E2%80%99s-Camera/</url>
    <content><![CDATA[<h3 id="Effective-and-Efficient-Detection-of-Moving-Targets-From-a-UAV’s-Camera"><a href="#Effective-and-Efficient-Detection-of-Moving-Targets-From-a-UAV’s-Camera" class="headerlink" title="Effective and Efficient Detection of Moving Targets From a UAV’s Camera"></a><center><a href="http://fcv2011.ulsan.ac.kr/files/announcement/625/Effective%20and%20Efficient%20Detection%20of%20Moving.pdf" target="_blank" rel="noopener">Effective and Efficient Detection of Moving Targets From a UAV’s Camera</a></center></h3><h3 id="无人摄像机对移动目标的有效与高效检测"><a href="#无人摄像机对移动目标的有效与高效检测" class="headerlink" title="无人摄像机对移动目标的有效与高效检测"></a><center>无人摄像机对移动目标的有效与高效检测</center></h3><h4 id="作者：Sara-Minaeian-Jian-Liu-and-Young-Jun-Son"><a href="#作者：Sara-Minaeian-Jian-Liu-and-Young-Jun-Son" class="headerlink" title="作者：Sara Minaeian, Jian Liu, and Young-Jun Son"></a><center>作者：Sara Minaeian, Jian Liu, and Young-Jun Son</center></h4><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><ul>
<li>移动中的摄像机对移动目标的精确而快速的检测是一个重要且具有挑战性的问题，尤其是当计算资源有限的时候。本文，我们提出了一个有效高效且健壮的方法来从一段视频序列中精确检测和分离多个独立移动的前向目标，这些视频序列是由单眼移动摄像机所采集的。我们所提出的方法在多种方式上领先现有的方法：1，为了高效起见，摄像机移动是通过在所有检测区间使用<strong>金字塔卢卡斯-卡纳德</strong>追踪背景关键点而建立的；2，将局部运动历史函数与滑动窗上的时空差异相结合，对多个移动目标进行前景分割，同时为了效率将透视同形法应用于图像配准；3，基于经验法则和考虑摄像机配置参数的鲁棒性动态调整检测区间。所提出的方法已经用无人机摄像头和公开的数据集进行过大量的情景测试。基于所报道的结果，通过与现有方法的比较，<strong>成功地证明了该方法在多个运动目标检测中的准确性和实时性。</strong> 实验结果表明，该方法同样适用于地面摄像机的ITS应用。更具体的地说，与文献中定量指标相比，所提出的方法表现出了很好的性能，同时运行时间测量在实时实现方面得到了显著的改善。</li>
</ul>
<h4 id="索引术语"><a href="#索引术语" class="headerlink" title="索引术语"></a>索引术语</h4><ul>
<li>有效性，图像运动分析，目标检测，鲁棒性，无人飞行器。</li>
</ul>
<h4 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h4><ul>
<li>大多数自动系统（例如视觉监控和智能交通）第一功能步骤就是通过传感器（大多数是摄像头）检测事件/目标。故以此可以为制动器做正确的决定。况且，越来越多的无人飞行器、自动驾驶车辆和其他智能代理工具装载摄像头，这些工具需要做出精确和实时的决定。因此，设计和开发高效的计算机视觉算法对于动态场景中的鲁棒操作至关重要。特别地，由于装载的移动摄像机具有不同的方向，通过无人飞行器进行移动目标检测对于人控制的应用来说可能是一个很有挑战的问题。对于未知的监视环境（由于无人机的自由移动）和目标的外观和运动的变化    这个问题被认为是不合适的。尽管文献中有几篇学术著作论述了用手持摄像机捕捉到的视频影像进行运动检测，但现有的方法要么过于复杂，要么无法以稳健的方式准确地从运动背景中分离出独立运动的前景，因此，他们缺乏足够的准确度和速度来应用于在线应用的中的单眼相机。况且，对于用无人飞行器检测移动物体，重点是对独立运动的前景区域进行分割。由于未知的背景先验模型和前景目标的动力学特性，监控目标检测技术不能直接应用于未知运动目标的分割问题。这些问题包括基于流行的深度学习（DL）和卷积神经网络（CNN）的方法，比如基于快速区域的卷积神经网络（R-CNN），超像素级别卷积神经网络（s-CNN），全常规网络（FCN），这些大多被应用于已知目标或区域分割（例如道路检测）的识别。值得注意的是，一般而言，深度学习方法的计算成本较高，主要集中在单个图像的检测和识别上，这对于机载计算资源有限的无人机来说是不可行的。考虑到这些挑战，本文的主要目标就是提出一种有效且高效的前景分割方法，用于通过无人机的移动摄像头检测独立移动目标，从而实现快速可靠的决策（例如用于对人群进行自主监控）。所提出的方法旨在从三种方式上领先现有文献资料：1，背景运动估计（在没有先验模型的情况下）是使用光学流方法的金字塔版本来跟踪在每个&Delta;t帧（检测间隔）提取的背景关键点；2，前景分割是通过将时空差异和局部运动历史技术结合在一个滑动帧窗口上（间隔为&Delta;t），通过透视变化（即同形法）进行注册，以减少图像配准误差，并区分多个移动目标物体；3，基于无人机的高度和速度，提出了一种启发式算法来调整检测区间，实现无人机鲁棒实时性能。</li>
<li>结果表明，该方法能够对独立运动的前景斑点进行近实时的精确分割，同时对视角和运动速度的变化具有鲁棒性。为了测试和证明所提出的方法，已经进行了实验研究，包括使用无人机视频进行人群控制应用的各种场景，以及自主检测和智能交通系统（ITS）等其他领域文献中可用的数据集。其原因是，虽然这项工作主要是为了无人机视觉监控而提出来的，但它很容易应用于ITS上，ITS高度依赖精确的车辆检测和改进的态势感知技术，以提供所需的交通统计、速度监控、存在检测的数据以及车辆分类。还要注意的是，视觉监控应用在这项技术的重点是低密度到中等密度的人群场景，这种场景中同一帧中前景到背景区域的近似比率小于一。文献中提出了一些替代方法，例如基于锚的群检测，用于高度拥挤的情况。</li>
</ul>
<h4 id="CONCLUSIONS-AND-FUTURE-WORK"><a href="#CONCLUSIONS-AND-FUTURE-WORK" class="headerlink" title="CONCLUSIONS AND FUTURE WORK"></a>CONCLUSIONS AND FUTURE WORK</h4><ul>
<li>本文，我们提出了一种有效且高效的方法，以稳健的方式从无人机的单目移动摄像头中检测多个独立移动目标。考虑到一个滑动窗口框架，在每一个时间帧t上，提取关键点并跟踪到下两个间隔为t的帧上。然后通过透视转换将这些帧显示到t帧上。最后，在后处理操作后应用局部运动历史函数，分离出独立运动的目标。</li>
<li>我们使用无人机捕获的视频以及公开可用的数据集测试了我们的方法。在定量和定性评价的基础上，对不同的场景进行了实验，取得了较好的效果。更具体地说，该方法的有效性是通过考虑不同摄像机的设置（根据海拔高度、速度和视角）和不同应用的结果来评估的，通过与基本真实的数据和最先进的方法的比较验证了该方法的有效性，同时根据通用性能指标报告所取得的性能；通过计算时间分析和与现有方法的报告运行时间进行对比证明了该方法的有效性。灵敏度分析研究也为该方法的关键参数的优化设定提供了依据。</li>
<li>至于未来研究工作，我们旨在提出一种稳健的数据关联算法，在考虑各种监视场景的同时，通过一系列视频帧区分和关联多个检测到的目标，以便通过无人机应用目标跟踪。</li>
</ul>
]]></content>
      <categories>
        <category>文献</category>
      </categories>
      <tags>
        <tag>文献</tag>
        <tag>翻译</tag>
      </tags>
  </entry>
  <entry>
    <title>应用机器学习建议</title>
    <url>/2019/08/02/%E5%BA%94%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BB%BA%E8%AE%AE/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1051792965&amp;courseId=1004570029" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="决定下一步做什么"><a href="#决定下一步做什么" class="headerlink" title="决定下一步做什么"></a>决定下一步做什么</h4><p>例子，当实现了正则化线形回归，使用新的数据集测试假设函数，却发现偏差很大？如何解决？</p>
<ul>
<li>获得更多训练样本（耗时耗力）</li>
<li>尝试更少的特征数据集</li>
<li>尝试更多的特征</li>
<li>添加多项式特征</li>
<li>减少或增大lambda<br><img src="https://i.loli.net/2019/09/01/skltPa2UjDBY9OR.png" alt="2019-09-01 15-40-19 的屏幕截图.png"></li>
</ul>
<p>如何事半功倍，时间用在刀刃上？</p>
<p><font color="red" size="6px">机器学习诊断法</font><br>测试算法哪里出现问题，获得有意义的尝试和改进。</p>
<h4 id="评估假设hypothesis"><a href="#评估假设hypothesis" class="headerlink" title="评估假设hypothesis"></a>评估假设hypothesis</h4><ul>
<li>选取少量数据集</li>
<li>随机选择70%作为训练样本，另外30%作为测试样本</li>
<li>用训练样本最小化代价函数得到参数</li>
<li><strong>用测试样本计算误差</strong>，根据所用模型计算相应代价函数</li>
<li>注意：参数是用训练样本得出的，再用参数计算交叉验证误差</li>
</ul>
<h4 id="模型选择和训练、验证、测试集"><a href="#模型选择和训练、验证、测试集" class="headerlink" title="模型选择和训练、验证、测试集"></a>模型选择和训练、验证、测试集</h4><p>模型选择相当于选择一个多项式h<sub>&theta;</sub>(X)=&theta;<sup>T</sup>X，其中如何选择X的次数degree呢？<br>我们选用不同的degree，用训练样本最小化代价函数即误差得到参数，然后用测试样本跑<strong>误差</strong>进行选择，选择<strong>测试样本误差最小</strong>的那个即可。<br>但这样存在一个问题，我们是用的是<strong>测试集</strong>进行误差评估，这样就过于乐观 了。<br><img src="https://i.loli.net/2019/08/02/5d43a1a5493c660376.png" alt="2019-08-02 10-27-36 的屏幕截图.png"></p>
<p>为了评估假设函数，我们在之前的基础上做修改。<br>调整训练样本比例为60%，在用20%作为交叉验证集CV，最后20%作为测试集。<br>在模式选择即选择degree的时候用训练集训练参数，根据交叉验证集选择误差最小的 -&gt; the better choice<br><img src="https://i.loli.net/2019/08/02/5d43a4393124375025.png" alt="2019-08-02 10-46-15 的屏幕截图.png"><br><img src="https://i.loli.net/2019/08/02/5d43a4393e0a665751.png" alt="2019-08-02 10-47-03 的屏幕截图.png"></p>
<h4 id="诊断偏差与方差"><a href="#诊断偏差与方差" class="headerlink" title="诊断偏差与方差"></a>诊断偏差与方差</h4><p>偏差bias= J<sub>Train</sub>(&theta;)<br>方差variance= J<sub>CV</sub>(&theta;)<br>在欠拟合时偏差和方差都很大，在过拟合时偏差很小但方差很大，如何权衡一目了然。<br><img src="https://i.loli.net/2019/08/02/5d43a72c82d6236836.png" alt="2019-08-02 10-56-27 的屏幕截图.png"><br><img src="https://i.loli.net/2019/08/02/5d43a72c8f1ed31573.png" alt="2019-08-02 10-59-46 的屏幕截图.png"></p>
<h4 id="正则化和偏差、方差"><a href="#正则化和偏差、方差" class="headerlink" title="正则化和偏差、方差"></a>正则化和偏差、方差</h4><p>如何选取正则化参数？<br>还是用训练集训练新的代价函数（加入正则化项的代价函数）得到一组参数，再用交叉验证集验证，选取交叉验证集误差最小的。<br><img src="https://i.loli.net/2019/08/02/5d43ab0394abe11340.png" alt="2019-08-02 11-15-15 的屏幕截图.png"></p>
<p>正则化参数对与偏差与方差的影响<br><img src="https://i.loli.net/2019/08/02/5d43b02d25ac752590.png" alt="2019-08-02 11-37-57 的屏幕截图.png"></p>
<h4 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h4><p>用处：判断一个学习算法是否处与偏差、方差问题，或者二者都有<br><img src="https://i.loli.net/2019/08/02/5d43de3a067ec20800.png" alt="2019-08-02 14-49-01 的屏幕截图.png"><br><img src="https://i.loli.net/2019/08/02/5d43de3a149ee60570.png" alt="2019-08-02 14-54-31 的屏幕截图.png"></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>吴恩达机器学习第五次编程作业-正则线性回归和偏差/方差</title>
    <url>/2019/07/30/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%94%E6%AC%A1%E7%BC%96%E7%A8%8B%E4%BD%9C%E7%94%A8-%E6%AD%A3%E5%88%99%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%92%8C%E5%81%8F%E5%B7%AE-%E6%96%B9%E5%B7%AE/</url>
    <content><![CDATA[<h3 id="课时82编程作业：正则线性回归和偏差-方差"><a href="#课时82编程作业：正则线性回归和偏差-方差" class="headerlink" title="课时82编程作业：正则线性回归和偏差/方差"></a><center><a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/text?lessonId=1053424061&amp;courseId=1004570029" target="_blank" rel="noopener">课时82编程作业：正则线性回归和偏差/方差</a></center></h3><p>工具：Pycharm2019.1，Python3.5</p>
<p>参考资料：</p>
<ol>
<li><a href="https://blog.csdn.net/zy1337602899/article/details/85482695" target="_blank" rel="noopener">吴恩达机器学习第五次作业</a>  </li>
<li><a href="https://sfz-lyq.cn/2019/07/15/%E6%AD%A3%E5%88%99%E5%8C%96/#more">正则化</a></li>
<li><a href="https://sfz-lyq.cn/2019/08/02/%E5%BA%94%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BB%BA%E8%AE%AE/#more">应用机器学习建议</a></li>
<li><a href="https://blog.csdn.net/Cowry5/article/details/80421712" target="_blank" rel="noopener">机器学习作业Python实现(五)</a></li>
<li><a href="https://blog.csdn.net/sdu_hao/article/details/84104733#commentsedit" target="_blank" rel="noopener">吴恩达机器学习第六周编程作业</a></li>
<li><a href="https://blog.csdn.net/ycarry2017/article/details/85051740" target="_blank" rel="noopener">scikit-learn的四种特征缩放方式</a></li>
<li><a href="https://blog.csdn.net/zhuangzhao119/article/details/88094572" target="_blank" rel="noopener">Python 进行特征缩放的方法</a></li>
<li><a href="https://blog.csdn.net/cluster1893/article/details/80759738" target="_blank" rel="noopener">特征缩放</a></li>
<li><a href="https://www.jb51.net/article/153648.htm" target="_blank" rel="noopener">numpy计算方差/标准方差/样本标准方差/协方差</a></li>
</ol>
<p><a href="https://paste.ubuntu.com/p/9zNxnFrMzn/" target="_blank" rel="noopener">正则化线性回归完整代码</a><br><a href="https://paste.ubuntu.com/p/qwm4fsDDh9/" target="_blank" rel="noopener">Polynomial regression完整代码</a></p>
<hr>
<h4 id="Regularized-Linear-Regression"><a href="#Regularized-Linear-Regression" class="headerlink" title="Regularized Linear Regression"></a>Regularized Linear Regression</h4><p>目的：根据水库水位预测水流量  </p>
<h5 id="Visualizing-the-dataset"><a href="#Visualizing-the-dataset" class="headerlink" title="Visualizing the dataset"></a>Visualizing the dataset</h5><p>tips: 数据是.mat存储的，所以使用loadmat取出后如何查看里面的数据构造呢？要知道这里的数据都是以字典dict存储的，所以查看键即可，而对应值是我们需要的ndarray数组。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import scipy.io as sio</span><br><span class="line">import scipy.optimize as opt</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">def Get_data():</span><br><span class="line">    data=sio.loadmat(&apos;./ex5/ex5data1.mat&apos;)</span><br><span class="line">    # for key in data: # 查看键</span><br><span class="line">    #     print(key)</span><br><span class="line">    return data[&apos;X&apos;],data[&apos;y&apos;],data[&apos;y&apos;].size</span><br><span class="line"></span><br><span class="line">def Plot_data(X,y):</span><br><span class="line">    plt.plot(X,y,&apos;rx&apos;)</span><br><span class="line">    plt.xlim(-50,40)</span><br><span class="line">    plt.ylim(0,40)</span><br><span class="line">    plt.xlabel(&apos;Change in water level (x)&apos;,fontsize=8)</span><br><span class="line">    plt.ylabel(&apos;Water flowing out of the dam (y)&apos;, fontsize=8)</span><br><span class="line">    plt.title(&apos;Figure 1: Data&apos;,fontsize=12)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">X,y,m=Get_data()</span><br><span class="line">Plot_data(X,y)</span><br></pre></td></tr></table></figure></p>
<p><img src="https://i.loli.net/2019/07/30/5d3fae446d5c881655.png" alt="Figure_12.png"></p>
<h5 id="Regularized-linear-regression-cost-function"><a href="#Regularized-linear-regression-cost-function" class="headerlink" title="Regularized linear regression cost function"></a>Regularized linear regression cost function</h5><p><img src="https://i.loli.net/2019/07/30/5d3fafc3dae5c93561.png" alt="2019-07-30 10-46-34 的屏幕截图.png"><br><img src="https://i.loli.net/2019/07/30/5d3fafc3efc6b60746.png" alt="2019-07-30 10-46-40 的屏幕截图.png"></p>
<font color="red" size="6px">注意不要惩罚&theta;<sub>0</sub>。<br>  加入惩罚项是为了防止过拟合现象。<br></font><br>

<p><font color="blue" size="6px">代价函数与梯度下降 </font><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Cost_func(X,y,theta,m): # 原来的线形回归代价函数</span><br><span class="line">    hypothesis=X.dot(theta)</span><br><span class="line">    return 1/(2*m)*(((hypothesis-y)**2).sum())</span><br><span class="line"></span><br><span class="line">def Cost_reg(X,y,theta,lamda,m): # 加入正则化项的线形回归代价函数</span><br><span class="line">    _theta=theta[1:] # 不惩罚theta0</span><br><span class="line">    return Cost_func(X,y,theta,m)+lamda/(2*m)*((_theta**2).sum())</span><br><span class="line"></span><br><span class="line">#  except output : 303.993</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Gradient_Linear(X,y,theta,m): # 原来的梯度</span><br><span class="line">    hypothesis=X.dot(theta)</span><br><span class="line">    return 1/m*np.dot(X.T,hypothesis-y)</span><br><span class="line"></span><br><span class="line">def Gradient_reg(X,y,theta,lamda,m): # 加入惩罚项的梯度</span><br><span class="line">    _theta=theta.copy() # 复制创建新的对象，改变数据不影响原数据</span><br><span class="line">    _theta[0]=0 # 不惩罚theta0</span><br><span class="line">    return Gradient_Linear(X,y,theta,m)+lamda/m*_theta</span><br><span class="line"></span><br><span class="line"># except output： [ [-15.30301567]  [598.25074417] ]</span><br></pre></td></tr></table></figure>
<h5 id="Fitting-linear-regression"><a href="#Fitting-linear-regression" class="headerlink" title="Fitting linear regression"></a>Fitting linear regression</h5><p>这里使用的是scipy.optimize.minimize()方法，但效果并不好。<br>终于知道如何把梯度作为参数了，使用<code>jac=partial_derivative</code>即可<br>传入参数要和函数参数一一对应<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Train_Linear_Reg(X,y,init_theta,lamda):</span><br><span class="line">    result=opt.minimize(fun=Cost_reg,x0=init_theta,args=(X,y,lamda,m),method=&apos;TNC&apos;,jac=Gradient_reg) # x0是以一维传入的</span><br><span class="line">    return result.x</span><br><span class="line">    </span><br><span class="line">def Fitting_Linear_reg(X,y,theta):</span><br><span class="line">    plt=Plot_data(X.T[1],y)</span><br><span class="line">    plt.plot(X.T[1],X.dot(theta))</span><br><span class="line">    plt.ylim(-5, 40)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://i.loli.net/2019/07/30/5d404cbb57e6074471.png" alt="Figure_12.png"><br>完整代码见顶部。  </p>
<p><font color="red" size="6px">总结：</font></p>
<ol>
<li>要特别注意进行运算的数组或矩阵之间的关系，一维和二维运算等，这里传入的x0=init_theta就是(2,)一维的，特别容易出错，故把y以y.ravel()传入 </li>
<li>高级优化方法参数列表应该和函数参数列表对应</li>
<li>训练的目的得出参数，最终用参数和样本输入进行运算作为输出再拟合</li>
<li>向量与矩阵进行运算分几种情况：如果想实现矩阵乘法，则向量作为行还是作为列进行运算取决于与它进行运算的那一维的情况；如果是普通加减乘除，则会各自用相同的向量扩展成相同的维数，再对应元素进行运算。</li>
</ol>
<h4 id="Bias-variance"><a href="#Bias-variance" class="headerlink" title="Bias-variance"></a>Bias-variance</h4><p>机器学习重要概念：权衡偏差-方差。高偏差会导致拟合不足，高方差会导致过拟合。  </p>
<h5 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h5><p>学习曲线是偏差和方差关于训练样本数m的变化情况<br>大致步骤：</p>
<ol>
<li>理论：什么是学习曲线</li>
<li>什么是训练误差、交叉验证误差与测试误差，实际就是不同模型的代价函数</li>
<li>用训练样本得出参数</li>
<li>用参数和交叉验证数据集算出交叉验证误差</li>
<li>作图</li>
</ol>
<p>这里是验证误差关于训练样本数m的变化情况，故依次增大训练样本：<code>X[:i,] , y[:i]</code>，用高级优化方法得出参数。<br>再用参数与交叉验证数据集进行运算得出第i次即训练样本数为i的交叉验证误差。<br>得出训练误差和交叉验证误差向量作图。<br>这里下标可能稍微不对称，Pycharm运行结果如此<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Learning_Curve(x, y, xval, yval,m, lamda):</span><br><span class="line">    train_error=np.zeros(m)</span><br><span class="line">    val_error=np.zeros(m)</span><br><span class="line">    for i in range(1,m+1):</span><br><span class="line">        train_X=x[:i,] # 训练样本</span><br><span class="line">        theta=Train_linear_reg(train_X,y[:i],m,lamda) # 高级优化得出参数</span><br><span class="line">        train_error[i-1]=Cost_reg(theta,train_X,y[:i],m,lamda) # 训练样本误差</span><br><span class="line">        val_error[i-1]=Cost_reg(theta,xval,yval,yval.size,lamda) # 交叉验证误差</span><br><span class="line">    return train_error,val_error # 返回两个误差向量</span><br><span class="line"></span><br><span class="line">def Plot_learning_curve(train_error,val_error,m):</span><br><span class="line">    plt.xlim(0, 13)</span><br><span class="line">    plt.yticks(np.linspace(0,150,4))</span><br><span class="line">    plt.plot(np.arange(m),train_error,label=&apos;Train&apos;)</span><br><span class="line">    plt.plot(np.arange(m), val_error, label=&apos;Cross Validation&apos;)</span><br><span class="line">    plt.xlabel(&apos;Number of training examples&apos;,fontsize=8)</span><br><span class="line">    plt.ylabel(&apos;Error&apos;,fontsize=8)</span><br><span class="line">    plt.title(&apos;Learning curve for linear regression&apos;,fontsize=10)</span><br><span class="line">    plt.grid(True) # 显示网格</span><br><span class="line">    plt.legend(loc=&apos;best&apos;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://i.loli.net/2019/08/03/HT9RwZ67GCXzhje.png" alt="Figure_13.png"></p>
<h4 id="Polynomial-regression"><a href="#Polynomial-regression" class="headerlink" title="Polynomial regression"></a>Polynomial regression</h4><h5 id="Learning-Polynomial-Regression"><a href="#Learning-Polynomial-Regression" class="headerlink" title="Learning Polynomial Regression"></a>Learning Polynomial Regression</h5><p>绘制多项式回归曲线<br>步骤：</p>
<ol>
<li>利用已有特征[12,1]扩展次数至[12,6]</li>
<li>获取每列均值与<a href="https://blog.csdn.net/qq_16587307/article/details/81328773" target="_blank" rel="noopener">样本标准偏差</a></li>
<li>标准化特征缩放</li>
<li>用训练样本训练参数</li>
<li>绘制原数据曲线</li>
<li>随机生成[-80,60]以内60个数<code>x1=np.linspace(-80,60,60).reshape(-1,1)</code>，这里只需指定一维另一维自动确定，故-1无所谓</li>
<li>同样进行扩展，再用训练样本均值和样本标准偏差进行标准化特征缩放</li>
<li>绘制多项式回归曲线</li>
<li>扩展交叉验证集，用训练集均值和标准偏差进行归一化即特征缩放</li>
<li>计算训练误差与交叉验证误差</li>
<li>绘制多项式学习曲线</li>
</ol>
<p>关于样本标准偏差：</p>
<ol>
<li>第一个疑点在于为什么所有需要归一化即缩放的地方都用训练样本的均值和标准偏差：在实际应用中，我们是无法知道除训练样本意外的样本数据，即预测的数据是模型未曾见过的，故用训练样本所训练出来的模型处理其他的数据</li>
<li>特征缩放为什么除以标准偏差：我的理解是，和标准差不同的是标准偏差是样本方差的算术平方根，这里使用训练样本训练模型，也是样本数据，不能代表总体数据，故设置ddof=1，在方差的基础上底数减一<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Ploy_features(x,p): # x，x^2，...，x^p</span><br><span class="line">    temp_x=np.zeros((x.shape[0],p))</span><br><span class="line">    for i in range(1,p+1):</span><br><span class="line">        temp_x[:,i-1]=x[:,0]**i</span><br><span class="line">    return temp_x</span><br><span class="line"></span><br><span class="line">def Get_mean_std(x): </span><br><span class="line">    means=np.mean(x,axis=0) # 取每列平均值</span><br><span class="line">    std=np.std(x,axis=0,ddof=1) # 取每列标准偏差</span><br><span class="line">    return means,std</span><br><span class="line"></span><br><span class="line">def Feature_Normalize(x,means,std): # 第一列不用特征缩放</span><br><span class="line">    return np.hstack((np.ones((x.shape[0],1)),(x-means)/std))</span><br><span class="line"></span><br><span class="line">def Plot_fit(x,y,theta,mean,std):</span><br><span class="line">    plt.plot(x,y,&apos;rx&apos;)</span><br><span class="line">    x1=np.linspace(-80,60,60).reshape(-1,1)</span><br><span class="line">    x_ploy=Ploy_features(x1,6)</span><br><span class="line">    x_ploy=Feature_Normalize(x_ploy,mean,std)</span><br><span class="line">    plt.plot(x1,x_ploy@theta,&apos;--&apos;)</span><br><span class="line">    plt.title(&apos;Polynomial Regression Fit (lambda = 0.000000)&apos;,fontsize=12)</span><br><span class="line">    plt.xlabel(&apos;Change in water level (x)&apos;,fontsize=8)</span><br><span class="line">    plt.ylabel(&apos;Water flowing out of the dam (y)&apos;,fontsize=8)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">def Learning_Polynomial_Reg(x,y,xval,yval,lamda=0):</span><br><span class="line">    X_ploy = Ploy_features(X, 6)  # 原文档使用octave优化算法不同，这里为了达到同样效果设为6</span><br><span class="line">    means, std = Get_mean_std(X_ploy)</span><br><span class="line">    X_ploy = Feature_Normalize(X_ploy, means, std)</span><br><span class="line">    train_theta = Train_linear_reg(X_ploy, y.ravel(), m, lamda)</span><br><span class="line">    Plot_fit(x,y,train_theta,means,std) # 绘制多项式回归曲线</span><br><span class="line">    xval_ploy=Ploy_features(xval,6)</span><br><span class="line">    xval_ploy=Feature_Normalize(xval_ploy,means,std)</span><br><span class="line">    error_train,error_val=Learning_Curve(X_ploy,y.ravel(),xval_ploy,yval.ravel(),X_ploy.shape[0],lamda)</span><br><span class="line">    plt=Plot_learning_curve(error_train,error_val,X_ploy.shape[0])</span><br><span class="line">    plt.show() # 绘制多项式学习曲线</span><br><span class="line"></span><br><span class="line">Learning_Polynomial_Reg(X,y,Xval,yval)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><img src="https://i.loli.net/2019/08/04/1rSqaWv7t4RH2G3.png" alt="Figure_14.png"><br><img src="https://i.loli.net/2019/08/05/dIH68lnZukK13zG.png" alt="Figure_15.png"> </p>
<h5 id="Adjusting-the-regularization-parameter"><a href="#Adjusting-the-regularization-parameter" class="headerlink" title="Adjusting the regularization parameter"></a>Adjusting the regularization parameter</h5><p>调整&lambda;的值为1，100。<br>在上面代码中传参的时候更改&lambda;的值，然后在绘制图片的标题部分手动设置输出值即可。<br><img src="https://i.loli.net/2019/08/05/fo1xYhL7INGC3Fy.png" alt="Figure_16.png"><br><img src="https://i.loli.net/2019/08/05/v793opPqmnjCsAi.png" alt="Figure_17.png"><br><img src="https://i.loli.net/2019/08/05/zHaZnf6MJ7m4Pwe.png" alt="Figure_18.png"></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络参数的反向传播算法</title>
    <url>/2019/07/29/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1051775907&amp;courseId=1004570029" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h4><p>在之前的前向传播算法中并没于介绍神经网络是如何训练参数的，而作业也是直接给出了&theta;<sup>1</sup>和&theta;<sup>2</sup>。那么这里介绍神经网络的代价函数就是这个目的。<br>定义：<br>      L：神经网络总层数<br>      s<sub>l</sub>：第l层的单元数，不包括bias unit在内</p>
<p>二分类问题输出只有0和1，但多元分类问题输出可能是个向量也可能是个矩阵，这里就先用下图为例，输出是一个4维<strong>向量</strong><br><img src="https://i.loli.net/2019/07/29/5d3e4a733a89d29057.png" alt="2019-07-29 09-21-12 的屏幕截图.png"></p>
<p>神经网络代价函数：<br><img src="https://i.loli.net/2019/07/29/5d3e4fd74dd0e36818.png" alt="2019-07-29 09-45-49 的屏幕截图.png"><br>关于神经网络代价函数解释：</p>
<ul>
<li>在神经网络问题中，应用的是多元分类问题，输出一个多维向量，那么假设函数h<sub>&theta;</sub>(X)&isin;R<sup>K</sup>。(h<sub>&theta;</sub>(X))<sub>i</sub>表示第i个输出的假设函数。我们在多元分类与前向传播算法中就是根据这个值确定属于哪一分类的。</li>
<li>y<sub>k</sub><sup>(i)</sup>表示第i的样本的输出，也是一个binary vector如上图。</li>
<li>最后一项是正则化项，最初(逻辑回归)在接触正则化项的时候只是为了防止过拟合现象，这里本质也还是逻辑回归。有时候正则化项也叫惩罚项，老板是这样叫的。</li>
<li>为什么正则化项是三层循环呢？用上面的神经网络图来解释，就相当于把每条线上的权值求平方和而已，一般不惩罚第0项&theta;，但其实惩罚也没多大差别，只不过不惩罚&theta;<sub>0</sub>更常用些。</li>
</ul>
<h4 id="反向传播算法-Backpropagation"><a href="#反向传播算法-Backpropagation" class="headerlink" title="反向传播算法 Backpropagation"></a><font color="red">反向传播算法 Backpropagation</font></h4><p>视频地址：<a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1051773834&amp;courseId=1004570029" target="_blank" rel="noopener">反向传播算法</a> ，比较复杂<br>目的：最小化代价函数。<br>TODO：代码实现计算代价函数J(&theta;)，推导并实现出偏导项<br><img src="https://i.loli.net/2019/07/29/5d3e56d1ba02418982.png" alt="2019-07-29 10-15-33 的屏幕截图.png"><br>梯度计算方法如下：这里参照前向传播作业可以直接用矩阵实现<br>h<sub>&theta;</sub>(x)叫<a href="https://www.cnblogs.com/missidiot/p/9378079.html" target="_blank" rel="noopener">激活值</a><br><img src="https://i.loli.net/2019/07/29/5d3e57ba630bb65590.png" alt="2019-07-29 10-17-55 的屏幕截图.png"></p>
<p>反向传播算法概念与计算方法:<br><img src="https://i.loli.net/2019/07/29/5d3e62512d6d194144.png" alt="2019-07-29 11-04-33 的屏幕截图.png"><br>解释：</p>
<ul>
<li>&delta;<sub>j</sub><sup style="margin-left:-5px">(l)</sup>表示第i层第j个节点误差，即a<sub>j</sub><sup style="margin-left:-5px">(i)</sup>-y<sub>j</sub>。  </li>
<li>知道了最后一层的误差就可以推出前面所有层的误差，如上计算公式</li>
<li>g&prime;(z<sup>(3)</sup>)表示导数项，z=&theta;<sup>T</sup>X；h<sub>&theta;</sub>(X)=g(z)=a=sigmoid(z)=1/(1+e<sup>-z</sup>)。而由微积分对z求导可以推出g&prime;(z<sup>(3)</sup>)=a<sup>(3)</sup>  (1-a<sup>(3)</sup>)，其余同理。这里<code>.*</code>表示普通元素对应相乘。</li>
</ul>
<p>计算方法：<br><img src="https://i.loli.net/2019/07/29/5d3e693128cbf17961.png" alt="2019-07-29 11-29-06 的屏幕截图.png"></p>
<h4 id="理解反向传播算法"><a href="#理解反向传播算法" class="headerlink" title="理解反向传播算法"></a>理解反向传播算法</h4><p>Cost(i)表示了神经网络预测样本值的准确程度，即网络的输出值与实际y值的偏差程度<br>反向传播算法实际上是在计算&delta;<sub>j</sub><sup style="margin-left:-5px">(l)</sup>，而&delta;<sub>j</sub><sup style="margin-left:-5px">(l)</sup>恰恰等于cost(i)关于z<sub>j</sub><sup style="margin-left:-5px">(l)</sup>的偏导数，其中对于只有一个样本输入来说，cost(i)已经给出如下：<br><img src="https://i.loli.net/2019/07/29/5d3e94b32da1941274.png" alt="2019-07-29 14-39-38 的屏幕截图.png"><br><img src="https://i.loli.net/2019/07/29/5d3ea10938ff424869.png" alt="2019-07-29 15-32-15 的屏幕截图.png"></p>
<h4 id="展开参数"><a href="#展开参数" class="headerlink" title="展开参数"></a>展开参数</h4><p>为了计算方便将所有参数矩阵展开成向量形式，传入函数进行计算<br><img src="https://i.loli.net/2019/07/29/5d3ea6c63625485539.png" alt="2019-07-29 15-56-23 的屏幕截图.png"><br><img src="https://i.loli.net/2019/07/29/5d3ea6c62261433560.png" alt="2019-07-29 15-56-44 的屏幕截图.png"></p>
<h4 id="梯度检测"><a href="#梯度检测" class="headerlink" title="梯度检测"></a>梯度检测</h4><p>为了保证正确实现了梯度，而且每次下降都是正确的，不至于最终得出结果和实际误差存在量级别的差异。<br>梯度检测是计算量非常大，在验证了反向传播算法所计算的梯度和梯度检测相差不多时就可以关闭梯度检测再直接用反向传播算法训练分类器。<br><img src="https://i.loli.net/2019/07/29/5d3eaa259ce1e67757.png" alt="2019-07-29 16-10-49 的屏幕截图.png"><br><img src="https://i.loli.net/2019/07/29/5d3eaa258db9847713.png" alt="2019-07-29 16-09-57 的屏幕截图.png"><br><img src="https://i.loli.net/2019/07/29/5d3eaa7dec8da36393.png" alt="2019-07-29 16-12-37 的屏幕截图.png"><br><img src="https://i.loli.net/2019/07/29/5d3eab6b3767473775.png" alt="2019-07-29 16-16-12 的屏幕截图.png"></p>
<h4 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h4><p>高级优化算法中会默认我们所传入的参数是会提供一些初始值的。<br>问题：如果对参数进行初始化？  </p>
<p><font color="red" size="5px">思想1：全部赋为0</font> ，在神经网络中并不适用   </p>
<p><font color="red" size="5px">思想2：随机初始化</font><br><img src="https://i.loli.net/2019/07/29/5d3eafe8d3d9240264.png" alt="2019-07-29 16-35-45 的屏幕截图.png"></p>
<h4 id="汇总"><a href="#汇总" class="headerlink" title="汇总"></a>汇总</h4><ol>
<li>选择神经网络结构：输入单元个数为单个样本输入X<sup>(i)</sup>的维数；输出单元的个数[1-10]都可以，但一般用向量且对应位为1其余为0表示i.e，[0,0,0,1,0,0]<sup>T</sup>；隐藏单元通常默认只有一层，但也可不止一层，这样每个隐藏层单元数要相同，通常等于特征数，或者是特征数的倍数个。</li>
<li>训练神经网络：随机初始化权重&theta;为接近0的数；实现前向传播算法得到h<sub>&theta;</sub>((x<sup>(i)</sup>))；计算代价函数；实现反向传播算法得到偏导数项；用梯度检查比较得出的偏导数项，确定偏差不大则屏蔽梯度检查部分；使用梯度下降或更高级的优化方法来最小化J(&theta;)并得到参数&theta;<br><img src="https://i.loli.net/2019/07/29/5d3eb41f9e37855213.png" alt="2019-07-29 16-53-29 的屏幕截图.png"><br><img src="https://i.loli.net/2019/07/29/5d3eb6788a0ca86336.png" alt="2019-07-29 17-03-44 的屏幕截图.png"></li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>2019.7.28 每周总结</title>
    <url>/2019/07/28/%E5%8D%97%E8%88%AA%E7%AC%AC%E4%B8%89%E5%91%A8%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>本周，产出较低。。<br>计划是将机器学习神经网络那一章里理论学完并完成作业，然后再开始看第二篇论文，可是这周实际上就勉勉强强写完了神经网络前向传播算法。<br>此刻，心情有点复杂。  </p>
<ul>
<li>周一： 理论学习，完成。实际上每章理论知识并不多，一天的时间足够看完，但要理解还得花点时间。如果没有意外，应该是把算法实现的。但问题出在我的工具上，我用的Python实现，但有些库并不会用，然后去找资料补充所需知识了。</li>
<li>周二：应该是找了个视频简单的学习了下Pandas库，之前读入文件使用过，所以’系统’学习一下。</li>
<li>周三：应该是学了一下Scipy库，简单的接触了一下，找到了一个完整的笔记，我所做的基本算是翻译成简化的中文文档了。到后面有的地方就看不懂了，也每用过，所以学到optimize那里就没往下学了。</li>
<li>周四：听说sklearn库是一个机器学习’外挂’库，之前导师也说过这个库，而且计划中也有关于这个的学习。于是去官网看文档学，也找不到合适的视频资料，干脆上官网自己学。只看了Introduce就看不下去了，它的学习框架和我自己学机器学习的视频知识框架不搭，很多我现阶段可能没法直接开始学。  </li>
<li>周五：原本周三周四是看论文的时间，但用来做其他也不知道有意义还是无意义的事，所以先把作业实现了。前几天是把多元分类做了，这一天做的神经网络前向传播算法。其实有之前的学习实现起来实际并没有用多少时间。很多时候学一个东西实际有效的时间并没有多少，而大部分可能需要找找资料，还有大部分时间被我自己荒废掉了。<del>可能</del>我太依赖氛围这个东西了吧。</li>
<li>周六周日两天以前的老队员过来完，于是陪逛了两天。</li>
</ul>
<p>晚上开完会去找了找师兄，是关于之前平衡工作和技术话题还有论文话题一些琐事，可是我得到的整理有用的大概是无人机是个大方向，没有指明细的方向，需要找关于无人机的各种论文广泛阅读，可是也在理给人建议是一件遭雷劈的事；还有强化学习是未来，强化学习是未来？<br>并没有得到什么有用的东西，感觉317的氛围也很轻松愉快，我在那里没有一点压力感，不过感觉师姐对我有点冷。师兄那得不到什么有用的东西，方向需要自己定。感觉还是自己的问题，没有坚定要做一件事或者说不知道该做什么，现在大家都在学机器学习，导师说了几次要我摸摸无人机，但我并不知道有什么用。。总之  ，心情有点复杂的是不知道未来在哪，不知道会怎样。但，现阶段还是按计划走，其他的开学再说？</p>
<p>下周，OKR已经写了，这里不再重复了，但关于运动我打打球也不知道可不可以；关于其他的入门计划，现在看来好像没法再拿出时间做了。还有太多时间自己太松散了，正常时间看视频听音乐做其他的事这些本不应该的啊。。</p>
]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title>scikit-learn: machine learning in Python</title>
    <url>/2019/07/24/Scikit-learn/</url>
    <content><![CDATA[<h3 id="scikit-learn-machine-learning-in-Python"><a href="#scikit-learn-machine-learning-in-Python" class="headerlink" title="scikit-learn: machine learning in Python"></a><center>scikit-learn: machine learning in Python</center></h3><p>工具：pycharm2019.1，python3.5</p>
<p>参考资料：<a href="http://scipy-lectures.org/packages/scikit-learn/index.html" target="_blank" rel="noopener">scikit-learn: machine learning in Python</a>，<a href="https://scikit-learn.org/stable/tutorial/index.html" target="_blank" rel="noopener">scikit-learn Tutorials</a></p>
<hr>
<p>准备工作：  </p>
<ol>
<li><a href="https://sfz-lyq.cn/2019/07/11/NumPy%E5%BA%93%E5%AD%A6%E4%B9%A0/">numpy</a> </li>
<li><a href="https://sfz-lyq.cn/2019/07/23/SciPy%E5%BA%93%E5%AD%A6%E4%B9%A0/">scipy</a></li>
<li><a href="https://sfz-lyq.cn/2019/07/07/Python%E5%9B%BE%E5%BD%A2%E7%BB%98%E5%88%B6/">matplotlib</a></li>
</ol>
<h4 id="Introduction-problem-settings"><a href="#Introduction-problem-settings" class="headerlink" title="Introduction: problem settings"></a>Introduction: problem settings</h4><h5 id="Data-in-scikit-learn"><a href="#Data-in-scikit-learn" class="headerlink" title="Data in scikit-learn"></a>Data in scikit-learn</h5><ul>
<li>The data matrix，即数据矩阵。机器学习算法在scikit-learn中实现要求数据是二维数组或矩阵存储的。数组的大小要求是 [n_samples, n_features]，即行代表样本数，列代表特征数。  </li>
<li>一个样本可以是一个文件，一张图片，一段音频等。</li>
<li>以线形回归模型为例，特征数即x<sub>1</sub>，x<sub>2</sub>…x<sub>n</sub>，矩阵的列数，每一列代表一种特征。  </li>
<li>注意，特征数量必须提前确定。</li>
</ul>
<h5 id="A-Simple-Example-the-Iris-Dataset"><a href="#A-Simple-Example-the-Iris-Dataset" class="headerlink" title="A Simple Example: the Iris Dataset"></a>A Simple Example: the Iris Dataset</h5><p>预测牵牛花种类。<br>花的特征有4个：sepal length (cm)， sepal width (cm)， petal length (cm) ，petal width (cm)<br>前提工作：导入包，没有则在pycharm解释器中安装sklearn<br>注意：scikit-learn被作为sklearn导入了；scikit-learn中内置了iris信息的CSV文件的复制文件，并单独设置了一个函数加载到numpy数组中。<br>导入数据：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">iris=load_iris()</span><br><span class="line">print(iris.data.shape,type(iris.data))</span><br><span class="line">print(iris.data[0])</span><br><span class="line">- - - - - - - - - - - - - - - - - </span><br><span class="line">(150, 4) &lt;class &apos;numpy.ndarray&apos;&gt;</span><br><span class="line">[5.1 3.5 1.4 0.2]</span><br></pre></td></tr></table></figure></p>
<p>由于这里特征数是4，故只选取前两个特征来绘制数据。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def plot_dataset(iris): # choose fisrt two features</span><br><span class="line">    # this formatter will label the colorbar with the correct target names</span><br><span class="line">    formatter = plt.FuncFormatter(lambda i, *args: iris.target_names[int(i)])</span><br><span class="line">    plt.scatter(iris.data[:,0],iris.data[:,1],c=iris.target)</span><br><span class="line">    plt.colorbar(ticks=[0, 1, 2], format=formatter)</span><br><span class="line">    plt.xlabel(iris.feature_names[0])</span><br><span class="line">    plt.ylabel(iris.feature_names[1])</span><br><span class="line">    plt.tight_layout() # Automatically adjust subplot parameters to give specified padding</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://i.loli.net/2019/07/24/5d3814371f04b81170.png" alt="Figure_10.png"></p>
<h4 id="Basic-principles-of-machine-learning-with-scikit-learn"><a href="#Basic-principles-of-machine-learning-with-scikit-learn" class="headerlink" title="Basic principles of machine learning with scikit-learn"></a>Basic principles of machine learning with scikit-learn</h4><p>每个算法都是通过一个’估计值’的对象暴露在sckit-learn中，或者说与应用在scikit-learn中？<br>例子，，没看懂。。</p>
<h4 id="Supervised-Learning-Classification-and-regression"><a href="#Supervised-Learning-Classification-and-regression" class="headerlink" title="Supervised Learning: Classification and regression"></a>Supervised Learning: Classification and regression</h4><p>监督学习：分类和回归。  </p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>SciPy库学习</title>
    <url>/2019/07/23/SciPy%E5%BA%93%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h3 id="Python库之SciPy"><a href="#Python库之SciPy" class="headerlink" title="Python库之SciPy"></a><center>Python库之SciPy</center></h3><p>工具：Pycharm2019.01，Python3.5  </p>
<p>参考资料：<a href="http://scipy-lectures.org/index.html" target="_blank" rel="noopener">Scipy Lecture Notes</a></p>
<hr>
<h4 id="关于Scipy"><a href="#关于Scipy" class="headerlink" title="关于Scipy"></a>关于Scipy</h4><ul>
<li>Scipy包中有许多工具用来解决科学计算的一些共同问题。不同的子模块负责不同的应用。比如插值、拟合、最优化、图像处理、数值计算以及特殊函数等等。   </li>
<li>Scipy中许多模块都是基于numpy的，scipy命名空间中许多主要的函数实际上就是numpy函数比如scipy.cos就是np.cos，故通常一起导入numpy库和scipy库</li>
</ul>
<h4 id="File-input-output-scipy-io"><a href="#File-input-output-scipy-io" class="headerlink" title="File input/output: scipy.io"></a>File input/output: scipy.io</h4><p><code>import numpy as np</code><br><code>from scipy import io as sio</code>  </p>
<ol>
<li><code>sio.savemat(&#39;file_path&#39;,{&#39;key&#39;,value})</code> : 将一个用命名和数组的字典保存到matlab样式的.mat文件中。  </li>
<li><code>sio.loadmat(&#39;file_path&#39;)</code>：导入.mat格式的文件</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from scipy import io as sio</span><br><span class="line">a = np.ones((3, 3))</span><br><span class="line">sio.savemat(&apos;file.mat&apos;,&#123;&apos;a&apos;:a&#125;) # 将命名为‘a’的字典保存为.mat格式的文件，默认保存到当前目录下，(.mat)后缀可以不加</span><br><span class="line">data = sio.loadmat(&apos;file.mat&apos;)</span><br><span class="line">print(data[&apos;a&apos;])</span><br><span class="line">- - - - - - - - - </span><br><span class="line">[[1. 1. 1.]</span><br><span class="line"> [1. 1. 1.]</span><br><span class="line"> [1. 1. 1.]]</span><br></pre></td></tr></table></figure>
<h4 id="Linear-algebra-operations-scipy-linalg"><a href="#Linear-algebra-operations-scipy-linalg" class="headerlink" title="Linear algebra operations: scipy.linalg"></a>Linear algebra operations: scipy.linalg</h4><p>线形代数运算<br><code>import numpy as np</code><br><code>from scipy import linalg</code><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a=np.array([[1,2],[3,4]])</span><br><span class="line">print(linalg.det(a))  # 求方阵的行列式，要求必须是方阵</span><br><span class="line">print(linalg.inv(a)) # 求逆，必须可逆，否则报错提示奇异(singular不可逆)矩阵</span><br><span class="line">print(np.allclose(np.dot(a,linalg.inv(a)),np.eye(2))) # 判断是否为单位矩阵</span><br><span class="line">- - - - - - - - </span><br><span class="line">-2.0</span><br><span class="line">[[-2.   1. ]</span><br><span class="line"> [ 1.5 -0.5]]</span><br><span class="line">True</span><br></pre></td></tr></table></figure></p>
<p>奇异值分解：<br>关于奇异值分解：<a href="https://www.cnblogs.com/tgycoder/p/6266786.html" target="_blank" rel="noopener">SVD简介</a><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from scipy import linalg</span><br><span class="line">arr = np.arange(9).reshape((3, 3)) + np.diag([1, 0, 1])</span><br><span class="line">print(arr)</span><br><span class="line">uarr, spec, vharr = linalg.svd(arr)  # uarr应该是伪逆</span><br></pre></td></tr></table></figure></p>
<h4 id="Interpolation-scipy-interpolate"><a href="#Interpolation-scipy-interpolate" class="headerlink" title="Interpolation: scipy.interpolate"></a><a href="http://scipy-lectures.org/intro/scipy.html#interpolation-scipy-interpolate" target="_blank" rel="noopener">Interpolation: scipy.interpolate</a></h4><p>插值。<br>没看懂。<br>代码：<a href="http://scipy-lectures.org/intro/scipy/auto_examples/plot_interpolation.html#a-demo-of-1d-interpolation" target="_blank" rel="noopener">A demo of 1D interpolation</a></p>
<h4 id="Optimization-and-fit-scipy-optimize"><a href="#Optimization-and-fit-scipy-optimize" class="headerlink" title="Optimization and fit: scipy.optimize"></a>Optimization and fit: scipy.optimize</h4><p>scipy.optimize提供一种算法来最小化函数值(规模或多维度)，曲线拟合和根查找<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import scipy.optimize as opt</span><br></pre></td></tr></table></figure></p>
<h5 id="Curve-fitting"><a href="#Curve-fitting" class="headerlink" title="Curve fitting"></a>Curve fitting</h5><p>曲线拟合，这里以sin()曲线为例<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def test_func(x, a, b):  # 定义函数</span><br><span class="line">    return a * np.sin(b * x)</span><br><span class="line">x_data = np.linspace(-5, 5, num=50)</span><br><span class="line">y_data = 2.9 * np.sin(1.5 * x_data) + np.random.normal(size=50) # sin函数存在偏差</span><br><span class="line">params, params_covariance =opt.curve_fit(test_func, x_data, y_data, p0=[2, 2]) # 求上面函数参数a，b</span><br><span class="line">plt.scatter(x_data,y_data,label=&apos;Data&apos;)</span><br><span class="line">plt.legend(loc=&apos;upper right&apos;)</span><br><span class="line">plt.plot(x_data,test_func(x_data,params[0],params[1]),label=&apos;fitted function&apos;) # 拟合</span><br><span class="line">plt.legend(loc=&apos;upper right&apos;)</span><br><span class="line">plt.show() # 显示效果如下</span><br></pre></td></tr></table></figure></p>
<p><img src="https://i.loli.net/2019/07/23/5d370585e4b6652582.png" alt="Figure_7.png"></p>
<h5 id="Finding-the-minimum-of-a-scalar-function"><a href="#Finding-the-minimum-of-a-scalar-function" class="headerlink" title="Finding the minimum of a scalar function"></a>Finding the minimum of a scalar function</h5><p>寻找标量函数最小值。<br>例如：<br>下面函数全局最优(小)值为-1.3，而局部最优值大概3.8。<br>用<code>opt.minimize()</code>函数，需要定义函数f()，以及传入起点坐标x0，返回找到的最小值信息（包括函数值，坐标等）。<br>为什么说是局部最优呢，当x0=5时，返回的函数值是8.3，位于x=3.83处。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def f(x):</span><br><span class="line">    return x**2 + 10*np.sin(x）</span><br><span class="line">x = np.arange(-10, 10, 0.1)</span><br><span class="line">plt.plot(x, f(x)) </span><br><span class="line">plt.show() </span><br><span class="line">result=opt.minimize(f,x0=0) # 寻找局部最小值的x坐标</span><br><span class="line">print(result) # 打印返回结果信息，</span><br><span class="line">- - - - - - - - - - -  </span><br><span class="line">   fun: -7.945823375615215  # 局部最小值</span><br><span class="line"> hess_inv: array([[0.08589237]])</span><br><span class="line">      jac: array([-1.1920929e-06])</span><br><span class="line">  message: &apos;Optimization terminated successfully.&apos;</span><br><span class="line">     nfev: 18</span><br><span class="line">      nit: 5</span><br><span class="line">     njev: 6</span><br><span class="line">   status: 0</span><br><span class="line">  success: True</span><br><span class="line">        x: array([-1.30644012])  # 局部最小值的坐标</span><br></pre></td></tr></table></figure></p>
<p><img src="https://i.loli.net/2019/07/24/5d37b4eac692090537.png" alt="Figure_8.png"></p>
<p>如果我们不知道全局最优点附近信息也就无法合理选择起点X0了，而<code>opt.basinhopping()</code>函数可以找到全局最优解。将上面<code>opt.minimize(f,x0=0)</code>替换成<code>opt.basinhopping(f,x0)</code>即可，x0还是需要赋初值，经测试当x0=7时找到的是全局最优(-1.3,-7.9)，而当x0=5时找到的是局部最优值(3.8,8.3)，故为了找到全局最优保险的方法还是需要多试几个参数。</p>
<p>试试<code>opt.minimize_scalar()</code>方法：能找到全局最优解<br><code>opt.minimize_scalar()</code>是只有一个变量时的最小函数值，而<code>opt.minimize()</code>是变量作为一个参数向量传进去的。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def f(x):</span><br><span class="line">    return x**2 + 10*np.sin(x)</span><br><span class="line">x = np.arange(-10, 10, 0.1)</span><br><span class="line">result=opt.minimize_scalar(f)</span><br><span class="line">print(result.x, result.fun)</span><br><span class="line">- - - - - - - - - - - - - - - -  - - - - - </span><br><span class="line">-1.3064400120612139 -7.945823375615284</span><br></pre></td></tr></table></figure>
<h5 id="Finding-the-roots-of-a-scalar-function"><a href="#Finding-the-roots-of-a-scalar-function" class="headerlink" title="Finding the roots of a scalar function"></a>Finding the roots of a scalar function</h5><p>需求：想找到x=？时，f(x)=0。<br>方法：opt.root(f,x0)，x0需要赋初值<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def f(x):</span><br><span class="line">    return x**2 + 10*np.sin(x)</span><br><span class="line">x = np.arange(-10, 10, 0.1)</span><br><span class="line">root=opt.root(f,x0=-3) # 设置起点，只能找到一个，要找到另外一个需要调参使得x0=1</span><br><span class="line">print(root.x,root.fun)</span><br><span class="line">root=opt.root(f,x0=1)</span><br><span class="line">print(root.x,root.fun)</span><br><span class="line">- - - - - - - - - - - - - - - - -</span><br><span class="line">[-2.47948183] [-1.77635684e-15]</span><br><span class="line">[0.] [0.]</span><br></pre></td></tr></table></figure></p>
<p>将以上三个函数绘制到一张图中：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def f(x): # 定义函数</span><br><span class="line">    return x**2 + 10*np.sin(x)</span><br><span class="line">x = np.arange(-10, 10, 0.1)</span><br><span class="line"></span><br><span class="line"># Global optimization</span><br><span class="line">grid = (-10, 10, 0.1)</span><br><span class="line">xmin_global = opt.brute(f, (grid, )) # 暴力找出参数列表中所得解的最优值</span><br><span class="line">print(&quot;Global minima found %s&quot; % xmin_global)</span><br><span class="line"></span><br><span class="line"># Constrain optimization</span><br><span class="line">xmin_local = opt.fminbound(f, 0, 10) # 在取区间(0,10)找局部最优解</span><br><span class="line">print(&quot;Local minimum found %s&quot; % xmin_local)</span><br><span class="line"></span><br><span class="line">root = opt.root(f, 1)  # our initial guess is 1</span><br><span class="line">print(&quot;First root found %s&quot; % root.x)</span><br><span class="line">root2 = opt.root(f, -2.5)</span><br><span class="line">print(&quot;Second root found %s&quot; % root2.x)</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(6, 4)) # 新建画布对象指定size</span><br><span class="line">ax = fig.add_subplot(111) # 将画布分成一行一列指定第一块区域作为对象赋给ax</span><br><span class="line"></span><br><span class="line"># Plot the function</span><br><span class="line">ax.plot(x, f(x), &apos;b-&apos;, label=&quot;f(x)&quot;)</span><br><span class="line"></span><br><span class="line"># Plot the minima</span><br><span class="line">xmins = np.array([xmin_global[0], xmin_local])</span><br><span class="line">ax.plot(xmins, f(xmins), &apos;go&apos;, label=&quot;Minima&quot;)</span><br><span class="line"></span><br><span class="line"># Plot the roots</span><br><span class="line">roots = np.array([root.x, root2.x])</span><br><span class="line">ax.plot(roots, f(roots), &apos;kv&apos;, label=&quot;Roots&quot;)</span><br><span class="line"></span><br><span class="line"># Decorate the figure</span><br><span class="line">ax.legend(loc=&apos;best&apos;) # 自适应选择位置</span><br><span class="line">ax.set_xlabel(&apos;x&apos;)</span><br><span class="line">ax.set_ylabel(&apos;f(x)&apos;)</span><br><span class="line">ax.axhline(0, color=&apos;gray&apos;) # 设置y=0画横线</span><br><span class="line">plt.show()</span><br><span class="line">- - - - - - - - - - - - - - - - - - </span><br><span class="line">Global minima found [-1.30641113]</span><br><span class="line">Local minimum found 3.8374671194983834</span><br><span class="line">First root found [0.]</span><br><span class="line">Second root found [-2.47948183]</span><br></pre></td></tr></table></figure></p>
<p><img src="https://i.loli.net/2019/07/24/5d37c4a47fb5b32171.png" alt="Figure_9.png">  </p>
<p>未完待续…..</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas库学习</title>
    <url>/2019/07/22/Pandas%E5%BA%93%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h3 id="Python库之pandas"><a href="#Python库之pandas" class="headerlink" title="Python库之pandas"></a><center><a href="https://www.bilibili.com/video/av38356551" target="_blank" rel="noopener">Python库之pandas</a></center></h3><p>环境：Pycharm2019.01，python3.5</p>
<hr>
<h4 id="Series与DataFrame"><a href="#Series与DataFrame" class="headerlink" title="Series与DataFrame"></a>Series与DataFrame</h4><p>Series与DataFrame分别对应于一维序列与二维表结构</p>
<h5 id="Series"><a href="#Series" class="headerlink" title="Series"></a>Series</h5><p>创建Series，索引默认从0开始：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">s1=pd.Series([1,2,-3,-5])</span><br><span class="line">print(s1)</span><br><span class="line">print(s1.values,type(s1)) # 查看s1的元素</span><br><span class="line">print(s1.index) # 查看s1的索引，显示[0,4)，步长为1</span><br><span class="line">- - - - - - - - - </span><br><span class="line">0    1</span><br><span class="line">1    2</span><br><span class="line">2   -3</span><br><span class="line">3   -5</span><br><span class="line">dtype: int64</span><br><span class="line">[ 1  2 -3 -5]</span><br><span class="line">RangeIndex(start=0, stop=4, step=1)</span><br></pre></td></tr></table></figure></p>
<p>pandas是基于numpy来构建的，所以这个一维序列所使用的结构也是array，但在Pycharm下type是Series类型的。  </p>
<p>创建索引，不使用默认索引：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">s2=pd.Series([1,2.0,-3,-5],index=[&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;])</span><br><span class="line">print(s2)</span><br><span class="line">print(s2[&apos;c&apos;]) # 根据索引取值，设置的索引必须对应值</span><br><span class="line">- - - - - - - - -</span><br><span class="line">a    1.0</span><br><span class="line">b    2.0</span><br><span class="line">c   -3.0</span><br><span class="line">d   -5.0</span><br><span class="line">dtype: float64</span><br><span class="line">-3.0</span><br></pre></td></tr></table></figure></p>
<p>Series可以看成一个定长的有序字典。字典没有顺序的概念，但Series一旦赋值就固定了。</p>
<h5 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">data=&#123;</span><br><span class="line">    &apos;year&apos;:[2014,2015,2016,2017],</span><br><span class="line">    &apos;income&apos;:[10,20,30,40],</span><br><span class="line">    &apos;outcome&apos;:[10,20,30,40]</span><br><span class="line">&#125;</span><br><span class="line">df1=pd.DataFrame(data) </span><br><span class="line">print(df1)</span><br><span class="line">- - - - - - - - - - - - -</span><br><span class="line">   income  outcome  year</span><br><span class="line">0      10       10  2014</span><br><span class="line">1      20       20  2015</span><br><span class="line">2      30       30  2016</span><br><span class="line">3      40       40  2017</span><br></pre></td></tr></table></figure>
<p>使用其他方法创建DataFrame:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">df2=pd.DataFrame(np.arange(10).reshape(2,5))</span><br><span class="line">df3=pd.DataFrame(np.arange(12).reshape(3,4),index=[&apos;a&apos;,&apos;b&apos;,&apos;c&apos;],columns=[1,5,2,6])</span><br><span class="line">print(df2)</span><br><span class="line">print(df3)</span><br><span class="line">print(df2.index） # 行</span><br><span class="line">print(df2.columns) # 列</span><br><span class="line">print(df2.values) # 输出值</span><br><span class="line">a=df2.values </span><br><span class="line">print(type(a))  # a是array类型</span><br><span class="line">print(df2.describe()) # 描述df2属性 </span><br><span class="line">- - - - - - - - - - - - </span><br><span class="line">   0  1  2  3  4</span><br><span class="line">0  0  1  2  3  4</span><br><span class="line">1  5  6  7  8  9</span><br><span class="line">   1  5   2   6</span><br><span class="line">a  0  1   2   3</span><br><span class="line">b  4  5   6   7</span><br><span class="line">c  8  9  10  11</span><br><span class="line">RangeIndex(start=0, stop=2, step=1)</span><br><span class="line">RangeIndex(start=0, stop=5, step=1)</span><br><span class="line">[[0 1 2 3 4]</span><br><span class="line"> [5 6 7 8 9]]</span><br><span class="line">&lt;class &apos;numpy.ndarray&apos;&gt;</span><br><span class="line">0         1         2         3         4</span><br><span class="line">count  2.000000  2.000000  2.000000  2.000000  2.000000 # 行数</span><br><span class="line">mean   2.500000  3.500000  4.500000  5.500000  6.500000 # 平均值</span><br><span class="line">std    3.535534  3.535534  3.535534  3.535534  3.535534 # 标准差</span><br><span class="line">min    0.000000  1.000000  2.000000  3.000000  4.000000 # 最小值</span><br><span class="line">25%    1.250000  2.250000  3.250000  4.250000  5.250000 # 列和的1/4</span><br><span class="line">50%    2.500000  3.500000  4.500000  5.500000  6.500000</span><br><span class="line">75%    3.750000  4.750000  5.750000  6.750000  7.750000</span><br><span class="line">max    5.000000  6.000000  7.000000  8.000000  9.000000</span><br></pre></td></tr></table></figure></p>
<p>转置与排序：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">df4=pd.DataFrame(np.arange(10).reshape(2,5),index=[1,2],columns=[2,1,5,6,0])</span><br><span class="line">print(df4)</span><br><span class="line">print(df4.T)</span><br><span class="line">print(df4.sort_index(axis=1)) # 按列名大小排序</span><br><span class="line">print(df4.sort_values(by=5)) # 按第5列值大小排序</span><br><span class="line">- - - - - - - - - - - - -</span><br><span class="line">   2  1  5  6  0</span><br><span class="line">1  0  1  2  3  4</span><br><span class="line">2  5  6  7  8  9</span><br><span class="line">   1  2</span><br><span class="line">2  0  5</span><br><span class="line">1  1  6</span><br><span class="line">5  2  7</span><br><span class="line">6  3  8</span><br><span class="line">0  4  9</span><br><span class="line">   0  1  2  5  6</span><br><span class="line">1  4  1  0  2  3</span><br><span class="line">2  9  6  5  7  8</span><br><span class="line">   2  1  5  6  0</span><br><span class="line">1  0  1  2  3  4</span><br><span class="line">2  5  6  7  8  9</span><br></pre></td></tr></table></figure></p>
<h4 id="pandas选择数据"><a href="#pandas选择数据" class="headerlink" title="pandas选择数据"></a>pandas选择数据</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">indexs=pd.date_range(&apos;20190105&apos;,periods=3)</span><br><span class="line">df5=pd.DataFrame(np.arange(12).reshape(3,4),index=indexs,columns=[&apos;A&apos;,&apos;B&apos;,&apos;C&apos;,&apos;D&apos;])</span><br><span class="line">print(df5)</span><br><span class="line">print(df5[&apos;B&apos;]) # 将DataFrame的列获取为一个Series</span><br><span class="line"># print(df5.B) # 作用同上</span><br><span class="line">print(df5[0:2]) # 获取表的前两行</span><br><span class="line">print(df5.loc[&apos;20190107&apos;,[&apos;B&apos;,&apos;D&apos;]]) # 通过标签选择数据再获取其中的列</span><br><span class="line">print(df5.iloc[0:3,2:4]) # 通过位置选择数据，行列从0开始和numpy类似</span><br><span class="line">print(df5.iloc[[0,2],[1,3]]) # 选择不连续的行列值</span><br><span class="line">- - - - - - - - - - - - - - -</span><br><span class="line">            A  B   C   D</span><br><span class="line">2019-01-05  0  1   2   3</span><br><span class="line">2019-01-06  4  5   6   7</span><br><span class="line">2019-01-07  8  9  10  11</span><br><span class="line">2019-01-05    1</span><br><span class="line">2019-01-06    5</span><br><span class="line">2019-01-07    9</span><br><span class="line">Freq: D, Name: B, dtype: int64</span><br><span class="line">            A  B  C  D</span><br><span class="line">2019-01-05  0  1  2  3</span><br><span class="line">2019-01-06  4  5  6  7</span><br><span class="line">B     9</span><br><span class="line">D    11</span><br><span class="line">Name: 2019-01-07 00:00:00, dtype: int64</span><br><span class="line">             C   D</span><br><span class="line">2019-01-05   2   3</span><br><span class="line">2019-01-06   6   7</span><br><span class="line">2019-01-07  10  11</span><br><span class="line">            B   D</span><br><span class="line">2019-01-05  1   3</span><br><span class="line">2019-01-07  9  11</span><br></pre></td></tr></table></figure>
<p>通过混合标签选择数据：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">indexs=pd.date_range(&apos;20190105&apos;,periods=3)</span><br><span class="line">df6=pd.DataFrame(np.arange(12).reshape(3,4),index=indexs,columns=[&apos;A&apos;,&apos;B&apos;,&apos;C&apos;,&apos;D&apos;])</span><br><span class="line">print(df6)</span><br><span class="line">print(df6.ix[1:3,[&apos;B&apos;,&apos;D&apos;]]) # 和numpy类似</span><br><span class="line">print(df6.C&gt;=6) # 判断某列与值的关系，这里就可以将逻辑回归数据分开data1=data[y=1]</span><br><span class="line">- - - - - - - - - - - - - - </span><br><span class="line">            A  B   C   D</span><br><span class="line">2019-01-05  0  1   2   3</span><br><span class="line">2019-01-06  4  5   6   7</span><br><span class="line">2019-01-07  8  9  10  11</span><br><span class="line">            B   D</span><br><span class="line">2019-01-06  5   7</span><br><span class="line">2019-01-07  9  11</span><br><span class="line">2019-01-05    False</span><br><span class="line">2019-01-06     True</span><br><span class="line">2019-01-07     True</span><br><span class="line">Freq: D, Name: C, dtype: bool</span><br></pre></td></tr></table></figure></p>
<h4 id="pandas赋值及操作"><a href="#pandas赋值及操作" class="headerlink" title="pandas赋值及操作"></a>pandas赋值及操作</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">indexs=np.arange(20190101,20190103)</span><br><span class="line">df7=pd.DataFrame(np.arange(10).reshape(2,5),index=indexs,columns=[&apos;A&apos;,&quot;B&quot;,&apos;C&apos;,&apos;D&apos;,&apos;E&apos;])</span><br><span class="line">print(df7)</span><br><span class="line">print(df7.iloc[1,3]) # 输出第1行第3列的数</span><br><span class="line">df7[df7.iloc[0:1]&lt;3]=3 # 将第0行小与3的数替换成3</span><br><span class="line">print(df7)</span><br><span class="line">df7[&apos;F&apos;]=10  # 插入一列并赋值为10，df7.loc[&apos;F&apos;]=10则表示插入一行赋值为10</span><br><span class="line">print(df7)</span><br><span class="line">- - - - - - - - - - - - - - -</span><br><span class="line">          A  B  C  D  E</span><br><span class="line">20190101  0  1  2  3  4</span><br><span class="line">20190102  5  6  7  8  9</span><br><span class="line">8</span><br><span class="line">          A  B  C  D  E</span><br><span class="line">20190101  3  3  3  3  4</span><br><span class="line">20190102  5  6  7  8  9</span><br><span class="line">          A  B  C  D  E   F</span><br><span class="line">20190101  0  1  2  3  4  10</span><br><span class="line">20190102  5  6  7  8  9  10</span><br></pre></td></tr></table></figure>
<p>插入行或列：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">indexs=np.arange(20190101,20190103)</span><br><span class="line">df7=pd.DataFrame(np.arange(10).reshape(2,5),index=indexs,columns=[&apos;A&apos;,&quot;B&quot;,&apos;C&apos;,&apos;D&apos;,&apos;E&apos;])</span><br><span class="line">print(df7)</span><br><span class="line">s1=pd.Series([1,2,3,4,5],index=[&apos;A&apos;,&apos;B&apos;,&apos;C&apos;,&apos;D&apos;,&apos;E&apos;])</span><br><span class="line">s1.name=&apos;S1&apos;</span><br><span class="line">df7=df7.append(s1) # 在行尾添加</span><br><span class="line">print(df7)</span><br><span class="line">df7.insert(df7.shape[1],&apos;F&apos;,10) # 在列尾增添一列，全部赋为0，df7.pop(&apos;F&apos;)或del df7[&apos;F&apos;] 则表示删除这列</span><br><span class="line">print(df7)</span><br><span class="line">df7=df7.drop([&apos;E&apos;,&apos;F&apos;],axis=1) # 删除列</span><br><span class="line">print(df7)</span><br><span class="line">- - - - - - - - - - - - </span><br><span class="line">          A  B  C  D  E</span><br><span class="line">20190101  0  1  2  3  4</span><br><span class="line">20190102  5  6  7  8  9</span><br><span class="line">          A  B  C  D  E</span><br><span class="line">20190101  0  1  2  3  4</span><br><span class="line">20190102  5  6  7  8  9</span><br><span class="line">S1        1  2  3  4  5</span><br><span class="line">          A  B  C  D  E   F</span><br><span class="line">20190101  0  1  2  3  4  10</span><br><span class="line">20190102  5  6  7  8  9  10</span><br><span class="line">S1        1  2  3  4  5  10</span><br><span class="line">          A  B  C  D</span><br><span class="line">20190101  0  1  2  3</span><br><span class="line">20190102  5  6  7  8</span><br><span class="line">S1        1  2  3  4</span><br></pre></td></tr></table></figure></p>
<h4 id="pandas处理丢失数据"><a href="#pandas处理丢失数据" class="headerlink" title="pandas处理丢失数据"></a>pandas处理丢失数据</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">indexs=np.arange(1,4)</span><br><span class="line">df1=pd.DataFrame(np.arange(12).reshape(3,4),index=indexs,columns=[&apos;A&apos;,&apos;B&apos;,&apos;C&apos;,&apos;D&apos;])</span><br><span class="line">df2=pd.DataFrame(df1,index=indexs,columns=[&apos;A&apos;,&apos;B&apos;,&apos;C&apos;,&apos;D&apos;,&apos;E&apos;]) # 没有指定默认为空Nan</span><br><span class="line">print(df1)</span><br><span class="line">print(df2) </span><br><span class="line">print(df2.dropna(axis=1,how=&apos;any&apos;)) # 指定列只要有空值就删除这一行，how=[&apos;any&apos;,&apos;all&apos;]，any表示任意，all表示全部满足空则删除</span><br><span class="line">print(df2.fillna(value=1)) # 用1填充表格中所有空值</span><br><span class="line">print(np.any(df2.isnull()))  # 是否存在空值，np.all()需要都为空值则True</span><br><span class="line">- - - - - - - - - - - -</span><br><span class="line">   A  B   C   D</span><br><span class="line">1  0  1   2   3</span><br><span class="line">2  4  5   6   7</span><br><span class="line">3  8  9  10  11</span><br><span class="line">   A  B   C   D   E</span><br><span class="line">1  0  1   2   3 NaN</span><br><span class="line">2  4  5   6   7 NaN</span><br><span class="line">3  8  9  10  11 NaN</span><br><span class="line">   A  B   C   D    E</span><br><span class="line">1  0  1   2   3  1.0</span><br><span class="line">2  4  5   6   7  1.0</span><br><span class="line">3  8  9  10  11  1.0</span><br><span class="line">True</span><br></pre></td></tr></table></figure>
<h4 id="pandas读取及写入文件"><a href="#pandas读取及写入文件" class="headerlink" title="pandas读取及写入文件"></a>pandas读取及写入文件</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">data=pd.read_csv(&apos;file_path&apos;) # 读取文件，可选项参数：delimiter=&apos;,&apos;表示以&apos;,&apos;为分隔符，dtype=int表示生成int类型数据</span><br><span class="line">data.to_csv(&apos;file_path&apos;）# 默认保存为csv格式文件，也可以选择其他格式</span><br></pre></td></tr></table></figure>
<h4 id="pandas数据合并"><a href="#pandas数据合并" class="headerlink" title="pandas数据合并"></a>pandas数据合并</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">df1=pd.DataFrame(np.arange(12).reshape(3,4),columns=[&apos;A&apos;,&apos;B&apos;,&apos;C&apos;,&apos;D&apos;])</span><br><span class="line">df2=pd.DataFrame(np.arange(12,24).reshape(3,4),columns=[&apos;A&apos;,&apos;B&apos;,&apos;C&apos;,&apos;D&apos;])</span><br><span class="line">df3=pd.DataFrame(np.arange(24,36).reshape(3,4),columns=[&apos;A&apos;,&apos;B&apos;,&apos;C&apos;,&apos;D&apos;])</span><br><span class="line">df4=pd.concat([df1,df2,df3],axis=0)</span><br><span class="line">print(df4) # 纵向合并，注意列名应该对应，不然出现空值</span><br><span class="line">print(pd.concat([df1,df2,df3],axis=0,ignore_index=True))</span><br><span class="line"># 纵向合并，不考虑原来的index，重新编排index</span><br><span class="line">print(pd.concat([df1,df2,df3],axis=1)) # 横向合并，不指定axis，默认为0</span><br><span class="line">df5=pd.DataFrame(np.arange(24,36).reshape(3,4),columns=[&apos;A&apos;,&apos;B&apos;,&apos;c&apos;,&apos;d&apos;])</span><br><span class="line">print(pd.concat([df1,df2,df5],join=&apos;inner&apos;,ignore_index=True)) # axis默认为0，合并表时inner表示舍弃列名不对应的列，outer表示缺少的部分用Nan填充</span><br><span class="line">df2=pd.DataFrame(np.arange(12,24).reshape(4,3),columns=[&apos;A&apos;,&apos;C&apos;,&apos;D&apos;])</span><br><span class="line">print(pd.concat([df1,df2],axis=1,join_axes=[df1.index])) # 横向合并，index使用df1的index，不指定则会出现空值</span><br><span class="line">- - - - - - - - - - - - - - - - - - - - - </span><br><span class="line">    A   B   C   D</span><br><span class="line">0   0   1   2   3</span><br><span class="line">1   4   5   6   7</span><br><span class="line">2   8   9  10  11</span><br><span class="line">0  12  13  14  15</span><br><span class="line">1  16  17  18  19</span><br><span class="line">2  20  21  22  23</span><br><span class="line">0  24  25  26  27</span><br><span class="line">1  28  29  30  31</span><br><span class="line">2  32  33  34  35</span><br><span class="line">    A   B   C   D</span><br><span class="line">0   0   1   2   3</span><br><span class="line">1   4   5   6   7</span><br><span class="line">2   8   9  10  11</span><br><span class="line">3  12  13  14  15</span><br><span class="line">4  16  17  18  19</span><br><span class="line">5  20  21  22  23</span><br><span class="line">6  24  25  26  27</span><br><span class="line">7  28  29  30  31</span><br><span class="line">8  32  33  34  35</span><br><span class="line">   A  B   C   D   A   B   C   D   A   B   C   D</span><br><span class="line">0  0  1   2   3  12  13  14  15  24  25  26  27</span><br><span class="line">1  4  5   6   7  16  17  18  19  28  29  30  31</span><br><span class="line">2  8  9  10  11  20  21  22  23  32  33  34  35</span><br><span class="line">    A   B</span><br><span class="line">0   0   1</span><br><span class="line">1   4   5</span><br><span class="line">2   8   9</span><br><span class="line">3  12  13</span><br><span class="line">4  16  17</span><br><span class="line">5  20  21</span><br><span class="line">6  24  25</span><br><span class="line">7  28  29</span><br><span class="line">8  32  33</span><br><span class="line">   A  B   C   D   A   C   D</span><br><span class="line">0  0  1   2   3  12  13  14</span><br><span class="line">1  4  5   6   7  15  16  17</span><br><span class="line">2  8  9  10  11  18  19  20</span><br></pre></td></tr></table></figure>
<h4 id="pandas合并merge"><a href="#pandas合并merge" class="headerlink" title="pandas合并merge"></a>pandas合并merge</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">df1=pd.DataFrame(&#123;&apos;key&apos;:[1,2,3,4],</span><br><span class="line">                  &apos;1&apos;:[11,12,13,14],</span><br><span class="line">                  &apos;2&apos;:[21,22,23,24]&#125;)</span><br><span class="line">df2=pd.DataFrame(&#123;&apos;key&apos;:[1,2,3,4],</span><br><span class="line">                  &apos;3&apos;:[31,32,33,34],</span><br><span class="line">                  &apos;4&apos;:[41,42,43,44]&#125;)</span><br><span class="line">print(df1)</span><br><span class="line">print(df2)</span><br><span class="line">print(pd.merge(df1,df2,on=&apos;key&apos;))  # 根据key相同的行进行横向合并，还可以加参数how=[&apos;left&apos;,&apos;right&apos;,&apos;inner&apos;,&apos;outer&apos;]，分别表示只考虑左边右边，默认为inner，outer为用空值补全；可以用参数indicator=Ture显示merge偏向信息</span><br><span class="line">- - - - - - - - - - - - - - - - - - - - - </span><br><span class="line">    1   2  key</span><br><span class="line">0  11  21    1</span><br><span class="line">1  12  22    2</span><br><span class="line">2  13  23    3</span><br><span class="line">3  14  24    4</span><br><span class="line">    3   4  key</span><br><span class="line">0  31  41    1</span><br><span class="line">1  32  42    2</span><br><span class="line">2  33  43    3</span><br><span class="line">3  34  44    4</span><br><span class="line">    1   2  key   3   4</span><br><span class="line">0  11  21    1  31  41</span><br><span class="line">1  12  22    2  32  42</span><br><span class="line">2  13  23    3  33  43</span><br><span class="line">3  14  24    4  34  44</span><br></pre></td></tr></table></figure>
<h4 id="pandas-plot"><a href="#pandas-plot" class="headerlink" title="pandas plot"></a>pandas plot</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x=pd.DataFrame(np.random.randn(1000),index=np.arange(1000)) # 随机1000个数</span><br><span class="line">x=x.cumsum() # 累加求和</span><br><span class="line">plt.plot(x) # 绘制</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/07/23/5d3675f598f3680394.png" alt="Figure_5.png"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">x=pd.DataFrame(np.random.randn(1000,4),index=np.arange(1000),columns=[&apos;A&apos;,&apos;B&apos;,&apos;C&apos;,&apos;D&apos;]) # 随机生成4组随机数，行和列都已经指定</span><br><span class="line">x=x.cumsum() # 累加求和</span><br><span class="line">print(x.head()) # 打印前5行</span><br><span class="line">plt.plot(x)</span><br><span class="line">plt.show()</span><br><span class="line">- - - - - - - - - - - - - - - - - - - - - </span><br><span class="line">          A         B         C         D</span><br><span class="line">0  1.659744  0.107002 -0.370865 -0.339613</span><br><span class="line">1  0.735340 -0.106536  0.469888 -1.700528</span><br><span class="line">2  2.015053 -0.821541 -0.234350 -1.447786</span><br><span class="line">3  3.367802 -0.139346 -2.070282 -1.300126</span><br><span class="line">4  2.802626 -0.284305 -3.187980 -1.908843</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/07/23/5d3675f5991d230861.png" alt="Figure_6.png"></p>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>吴恩达机器学习第三次编程作业-多元分类与神经网络</title>
    <url>/2019/07/22/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%89%E6%AC%A1%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<h3 id="课时67编程作业：多元分类与神经网络"><a href="#课时67编程作业：多元分类与神经网络" class="headerlink" title="课时67编程作业：多元分类与神经网络"></a><center><a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/text?lessonId=1053425062&amp;courseId=1004570029" target="_blank" rel="noopener">课时67编程作业：多元分类与神经网络</a></center></h3><p>工具：Pycharm2019.01，Python3.5  </p>
<p>参考资料：<a href="https://sfz-lyq.cn/2019/07/23/SciPy%E5%BA%93%E5%AD%A6%E4%B9%A0/#more">Python库之SciPy</a>，<a href="https://blog.csdn.net/weixin_44750583/article/details/88603512" target="_blank" rel="noopener">逻辑回归解决多元分类</a>，<a href="https://blog.csdn.net/weixin_39223665/article/details/79675927" target="_blank" rel="noopener"><br>Python3 求最大/小值及索引值 Numpy</a>，<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_ncg.html#scipy.optimize.fmin_ncg" target="_blank" rel="noopener">scipy.optimize.fmin_ncg</a>，<a href="https://docs.scipy.org/doc/scipy/reference/optimize.html" target="_blank" rel="noopener">scipy.optimize</a>，<a href="https://blog.csdn.net/u011851421/article/details/83783826" target="_blank" rel="noopener">numpy矩阵乘法运算</a></p>
<p><a href="https://paste.ubuntu.com/p/N2m5CXTNCc/" target="_blank" rel="noopener">多元分类完整代码: fmin_ncg</a><br><a href="https://paste.ubuntu.com/p/fTC64VRr8P/" target="_blank" rel="noopener">多元分类完整代码: fmin_cg</a><br><a href="https://paste.ubuntu.com/p/dwMMj7RYNZ/" target="_blank" rel="noopener">神经网络前向传播算法完整代码</a></p>
<p>注：本章所用参考资料均已给出</p>
<hr>
<p>本次练习实现一对多逻辑回归与神经网络来识别手写数字。</p>
<h4 id="Multi-class-Classification"><a href="#Multi-class-Classification" class="headerlink" title="Multi-class Classification"></a>Multi-class Classification</h4><h5 id="数据集格式"><a href="#数据集格式" class="headerlink" title="数据集格式"></a>数据集格式</h5><p>本次练习数据集格式为.mat类型，故采用Scipy.io.loadmat方式读入数据，以字典dictionary形式存在，其中有两个键’X’和’y’分别代表训练样本输入和输出，对应值是以ndarray形式存在。<br>其中，输入是一个(5000,400)的数组，400表示数字是20*20像素的，被拉伸为400维一行。<br>获取数据：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Get_Data():</span><br><span class="line">    data=sio.loadmat(&apos;./ex3/ex3data1.mat&apos;) </span><br><span class="line">    # print(type(data)) # dict</span><br><span class="line">    # for key in data: # 如果不清楚文件内容，可以打印出来看看</span><br><span class="line">    #     print(key,&apos;:&apos;,data[key])</span><br><span class="line">    return data[&apos;X&apos;],data[&apos;y&apos;],data[&apos;y&apos;].shape[0] # 输入，输出，以及样本数</span><br></pre></td></tr></table></figure></p>
<h5 id="Visualizing-the-data"><a href="#Visualizing-the-data" class="headerlink" title="Visualizing the data"></a>Visualizing the data</h5><p>由于数据集过大，故随机选取100个样本输入打印出来。<br>每次从<code>[0,5000)</code>中随机取一个数：<code>rand_choose=np.random.random_integers(0,m)</code>    </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Plot_Data(X,y,m): # random choose 100 samples to plot it</span><br><span class="line">    fig=plt.figure(figsize=(6,6)) # 新建画布并指定size</span><br><span class="line">    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05) # 指定显示比例</span><br><span class="line">    for i in range(1,101): # 区域编号从1开始</span><br><span class="line">        ax = fig.add_subplot(10, 10, i, xticks=[], yticks=[])</span><br><span class="line">        rand_choose=np.random.random_integers(0,m) </span><br><span class="line">     ax.imshow(X[rand_choose].reshape(20,20).T,cmap=plt.cm.binary,interpolation=&apos;nearest&apos;)</span><br><span class="line">        # label the image with the target value</span><br><span class="line">        ax.text(0,20, str(y[rand_choose])) # 显示真实数字</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/07/25/5d399e37023c323968.png" alt="Figure_11.png"></p>
<h5 id="Vectorizing-Logistic-Regression"><a href="#Vectorizing-Logistic-Regression" class="headerlink" title="Vectorizing Logistic Regression"></a>Vectorizing Logistic Regression</h5><p>主要是定义代价函数与梯度下降，然后调用scipy.optimize库方法跑出参数&theta;，最后利用参数进行预测。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Cost_Func(theta,X,Y,m): # 原来的代价函数</span><br><span class="line">    hypothesis=1/(1+np.exp(-X@theta)) # 假设函数</span><br><span class="line">    Cost_J=-1/m*((Y@np.log(hypothesis)+(1-Y)@np.log(1-hypothesis)))</span><br><span class="line">    return Cost_J</span><br><span class="line"></span><br><span class="line">def Cost_reg(theta,X,Y,m,lamda): # 正则化代价函数，加上正则项</span><br><span class="line">    Cost_J=Cost_Func(theta,X,Y,m)</span><br><span class="line">    theta_tmp=theta[1:] # 不惩罚theta[0]</span><br><span class="line">    Cost_J=Cost_J+lamda/(2*m)*np.sum(theta_tmp*theta_tmp)</span><br><span class="line">    return Cost_J</span><br><span class="line"></span><br><span class="line">def Gredient_Dec(theta,X,Y,m): # 原来的梯度函数</span><br><span class="line">    hypothesis=1/(1+np.exp(-X.dot(theta))) </span><br><span class="line">    partial_derivatives=1/m*np.dot(X.T,hypothesis-Y) # 偏导数</span><br><span class="line">    return partial_derivatives</span><br><span class="line"></span><br><span class="line">def Gredient_reg(theta,X,Y,m,lamda): # 正则化梯度函数</span><br><span class="line">    partial_derivations=Gredient_Dec(theta,X,Y,m) </span><br><span class="line">    theta_tmp=lamda/m*theta</span><br><span class="line">    theta_tmp[0]=0 # 不惩罚theta[0]</span><br><span class="line">    return partial_derivations+theta_tmp</span><br></pre></td></tr></table></figure></p>
<h5 id="One-vs-all-Classification"><a href="#One-vs-all-Classification" class="headerlink" title="One-vs-all Classification"></a>One-vs-all Classification</h5><ul>
<li>将复杂的分类问题简化为若干个二分类任务，最经典的拆分策略有三种：一对一（OvO）、一对其余（OvR）和多对多（MvM）。  </li>
<li>这里的分类问题需要10个分类器，故最终目标需要’训练’出10个&theta;参数向量。而在训练时，使用逻辑回归方法，将待分类的类别当作1，其余为0。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def One_vs_All_classes(X,Y,theta,labda,m,K):</span><br><span class="line">    y=Y.ravel()</span><br><span class="line">    classes_y = np.zeros(m).astype(int)</span><br><span class="line">    for i in range(K):</span><br><span class="line">        classes_y[y!=i]=0 # 不属于此类设为0，否则为1</span><br><span class="line">        classes_y[y==i]=1 # 变成了简单的逻辑回归问题</span><br><span class="line">        # print(i,sum(classes_y==1),sum(y==i))</span><br><span class="line">        result=opt.fmin_ncg(f=Cost_reg,fprime=Gredient_reg,x0=theta[:,i:i+1],args=(X,classes_y,m,lamda),maxiter=400) # 使用牛顿迭代的方法</span><br><span class="line">        theta[:,i:i+1]=np.mat(result).T # 返回的result是一个向量(401,)</span><br><span class="line">    return theta</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="One-vs-all-Prediction"><a href="#One-vs-all-Prediction" class="headerlink" title="One-vs-all Prediction"></a>One-vs-all Prediction</h5><p>预测，使用参数&theta;向量评测每个样本，取H<sub>&theta;</sub>(X)最大的那个作为预测值。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def One_vs_All_prediction(X,Y,fin_theta,m):</span><br><span class="line">    # print(X.shape,fin_theta.shape)</span><br><span class="line">    predict_matrix=1/(1+np.exp(-X.dot(fin_theta)))</span><br><span class="line">    cnt=0</span><br><span class="line">    for i in range(m):</span><br><span class="line">        if np.argmax(predict_matrix[i])==y[i][0]:</span><br><span class="line">            cnt=cnt+1</span><br><span class="line">    print(&apos;%.2f%%&apos; %(cnt/m*100))</span><br></pre></td></tr></table></figure></p>
<p>在本机上跑了大概10分钟？<br>预测结果: 96.46%  </p>
<h5 id="不同的优化函数对比情况"><a href="#不同的优化函数对比情况" class="headerlink" title="不同的优化函数对比情况"></a>不同的优化函数对比情况</h5><p>上述所使用的fmin_ncg()牛顿法原理是利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。但在运行过程中一直打印警告信息提示hassian矩阵不是正数，而且运行时间太长。<br>在scipy.optimize库中使用其他优化方法试试？<br>参考资料见顶部开头。<br>由于一下三种方法的参数列表都相同<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fmin_cg(f, x0[, fprime, args, gtol, norm, …]) Minimize a function using a nonlinear conjugate gradient algorithm.</span><br><span class="line">fmin_bfgs(f, x0[, fprime, args, gtol, norm, …]) Minimize a function using the BFGS algorithm.</span><br><span class="line">fmin_ncg(f, x0, fprime[, fhess_p, fhess, …]) Unconstrained minimization of a function using the Newton-CG method.</span><br></pre></td></tr></table></figure></p>
<ol>
<li><p>故先使用fmin_cg方法，原理上面已经给出：非线性动态梯度算法  .<br>结果运行时长1min左右就出来了。<br>预测结果：96.46%<br>代码见开头</p>
</li>
<li><p>再把方法改为fmin_bfgs()会显示超过最大迭代次数警告，不过最终预测结果准确率竟然达到: 96.48%</p>
</li>
<li><p>用opt.minimize(method=method=’L-BFGS-B’)预测结果为：95.44%。<br>完整代码为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">result=opt.minimize(fun=Cost_reg,x0=theta[:,i:i+1],args=(X,classes_y,m),method=&apos;L-BFGS-B&apos;)</span><br><span class="line">        theta[:,i:i+1]=np.mat(result.x).T</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>梯度下降不知道怎么嵌入。。。<br>路过大神望告知！  </p>
<h4 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h4><p>目标：实现神经网络识别手写数字。<br>核心：实现前向传播算法，前向传播算法即神经网络从输入层到隐藏层到输出层的过程。  </p>
<h5 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h5><p>数字矩阵还是使用之前多元分类器的数据，这里再给出了第一层到第二层的&theta;<sup>1</sup>和第二层到第三层的&theta;<sup>2</sup>。其中，&theta;<sup>1</sup>是25 x 401的，即第二层有a<sup>(2)</sup>有25个单元；&theta;<sup>2</sup>是10 x 26，26是在第二层多加一个bias unit。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Get_Data():</span><br><span class="line">    data=sio.loadmat(&apos;./ex3/ex3data1.mat&apos;)</span><br><span class="line">    theta=sio.loadmat(&apos;./ex3/ex3weights.mat&apos;)</span><br><span class="line">    return data[&apos;X&apos;],data[&apos;y&apos;],theta[&apos;Theta1&apos;],theta[&apos;Theta2&apos;],data[&apos;y&apos;].size</span><br><span class="line"></span><br><span class="line">X,y,theta1,theta2,m=Get_Data()</span><br><span class="line">X=np.hstack((np.ones((m,1)),X))</span><br></pre></td></tr></table></figure></p>
<h5 id="Model-representation"><a href="#Model-representation" class="headerlink" title="Model representation"></a>Model representation</h5><p><img src="https://i.loli.net/2019/07/28/5d3d842ee0e4b88574.png" alt="2019-07-28 19-16-38 的屏幕截图.png"><br>大致步骤：<br>      不同的样本添加X0=1表示bias unit，分别通过第一层计算得到第二层<br>      第二层得到的数据再添加一个bias unit，通过Theta2计算得到第三层，取第三层概率值H<sub>&theta;</sub>(X)最大的就是最终的分类预测了。 </p>
<h5 id="Feedforward-Propagation-and-Prediction"><a href="#Feedforward-Propagation-and-Prediction" class="headerlink" title="Feedforward Propagation and Prediction"></a>Feedforward Propagation and Prediction</h5><p>前向传播与预测。<br>前向传播就是信息从第一层到最后一层最终的出结果的过程。<br>预测则是类似多元分类，用的出的参数&theta;与样本或者其他数据进行运算，看计算结果属于哪一类。<br>这里可以直接用矩阵运算快速得出结果。<br><code>np.argmax()</code>可得到ndarray类型最大值的索引，同理argmin则是取最小值的索引，可指定参数axis，默认为0表示列，1则表示行即每行最大值的索引<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def prediction(X,y,theta1,theta2,m):</span><br><span class="line">    # print(theta1.shape,X.shape)</span><br><span class="line">    a_2=1/(1+np.exp(-np.dot(X,theta1.T))) # 直接计算第二层结果</span><br><span class="line">    a_2=np.hstack((np.ones((m,1)),a_2)) # 添加一个bias unit</span><br><span class="line">    a_3=1/(1+np.exp(-a_2.dot(theta2.T)))  # 用样的方法计算第三层</span><br><span class="line">    print(&apos;%.2f%%&apos; %(sum(np.argmax(a_3,axis=1)+1==y.ravel())/m*100)) # 加一是y的范围是[1,10]</span><br></pre></td></tr></table></figure></p>
<p>预测结果：97.52%<br>完整代码见开头</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络</title>
    <url>/2019/07/22/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1051722792&amp;courseId=1004570029" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="非线形假设"><a href="#非线形假设" class="headerlink" title="非线形假设"></a>非线形假设</h4><ul>
<li>计算机中一辆车或者一个车把手，是用一个数字矩阵表示的，或者说像素强度值的网格。告诉我们图像中每个像素的亮度值。计算机视觉问题就是根据这个像素点亮度矩阵来告诉我们这些数值。</li>
<li>如何区分一辆汽车，找到一些样本通过学习算法分辨哪些是汽车哪些不是。假如选取两个像素点代表两个坐标轴特征，那么可以用非线形回归将01分开。但两个像素比如50X50就有2500个像素点，特征矩阵就有2500个值，如果采用RGB格式每个像素点包含红绿蓝三种颜色，那么就有7500个值。</li>
<li>特征过多逻辑回归就不是一个好的分类算法了。<br><img src="https://i.loli.net/2019/07/22/5d3521c624dcf10545.png" alt="2019-07-22 10-38-07 的屏幕截图.png"></li>
</ul>
<h4 id="神经元与大脑"><a href="#神经元与大脑" class="headerlink" title="神经元与大脑"></a>神经元与大脑</h4><p>起源：人们想尝试制造出能模仿人类大脑的机器。<br>大脑：大脑有许多不同的感知皮层，通过学习，触觉也能学会’听’，听觉也能学会’看’。<br>目的：找到一种近似大脑判断的学习算法，而不用同时运行几千个算法程序模拟大脑。  </p>
<h4 id="如何表示神经网络"><a href="#如何表示神经网络" class="headerlink" title="如何表示神经网络"></a>如何表示神经网络</h4><p>下面是一个神经元：神经元树突Dendrite相当于输入，Axon相当于输出；那么神经元相当于几个计算单元，接收信号计算并相其他传递信息。<br><img src="https://i.loli.net/2019/07/22/5d35259b0548537705.png" alt="2019-07-22 10-55-14 的屏幕截图.png"><br>神经元之间通信通过轴突，产生微弱电流发送给其他神经元。  </p>
<p>将神经元模拟成一个逻辑单元logistic unit，如下一个简单的模型。<br><img src="https://i.loli.net/2019/07/22/5d35285cdc5f571147.png" alt="2019-07-22 11-06-58 的屏幕截图.png"><br>神经网络术语：激活函数g(z)=1/(1+e<sup>-z</sup>)<br>在其他的学习资料中参数&theta;也被成为权重，实质是一样的。  </p>
<p>有了上面神经元定义和模型，那么神经网络就是一组神经元链接在一起的集合。<br><img src="https://i.loli.net/2019/07/22/5d3529370ea6124285.png" alt="2019-07-22 11-10-22 的屏幕截图.png"><br>从左到右依次是输入层，隐藏层，输出层。隐藏层即不是x也不是y，在训练集中无法看到所以叫隐藏层，即非输入输出层就叫隐藏层。<br>下图模拟了一个神经网络到学习算法的一个简单模型：从数学上定义神经网络的一个简单假设<br><img src="https://i.loli.net/2019/07/22/5d352b662903871842.png" alt="2019-07-22 11-18-37 的屏幕截图.png"></p>
<p>前向传播：从输入层到隐藏层到输出层的过程<br>下面是向量化实现：<br><img src="https://i.loli.net/2019/07/22/5d35320818e5619989.png" alt="2019-07-22 11-47-30 的屏幕截图.png"></p>
<p>如果将第一层输入特征X1..Xn去除，那么得到的后面的模型就想当与是逻辑回归模型，神经网络中并没有用输入特征x训练逻辑回归，而是自己训练逻辑回归的输入a1…a3。  </p>
<h4 id="例子与直觉理解"><a href="#例子与直觉理解" class="headerlink" title="例子与直觉理解"></a>例子与直觉理解</h4><p>非线形分类例子：XOR/XNOR/AND/OR<br> x&isin;{0,1}<br> <img src="https://i.loli.net/2019/07/22/5d3559a4acb6533142.png" alt="2019-07-22 14-36-43 的屏幕截图.png"><br>组合得到 x<sub>1</sub> XNOR x<sub>2</sub><br><img src="https://i.loli.net/2019/07/22/5d355b4b72fa834307.png" alt="2019-07-22 14-42-38 的屏幕截图.png"><br><img src="https://i.loli.net/2019/07/22/5d355d066be3331629.png" alt="2019-07-22 14-51-28 的屏幕截图.png"> </p>
<p>第一层输入通过第一个隐藏层，计算出一系列特征，再通过下一个隐藏层计算出更复杂的特征，再往下更复杂，最后这些特征被用于逻辑回归分类器的最后一层。</p>
<h4 id="多元分类"><a href="#多元分类" class="headerlink" title="多元分类"></a>多元分类</h4><p>利用神经网络解决多类别分类问题。<br><img src="https://i.loli.net/2019/07/22/5d3565e4b34c928246.png" alt="2019-07-22 15-28-37 的屏幕截图.png"></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>2019.7.21 每周总结</title>
    <url>/2019/07/21/%E5%8D%97%E8%88%AA%E7%AC%AC%E4%BA%8C%E5%91%A8%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>本周感觉过的比上周快，而且好像就做了两件事： 逻辑回归和文献。<br>本周目标（O）：学习逻辑回归模型，完成一篇文献阅读。  </p>
<ul>
<li>周一：学习了一天的逻辑回归，但还没看完。  </li>
<li>周二：上午把残留的一点点看完了，然后开始实现算法。到晚上把不带正则化的逻辑回归写完，并写下笔记。  </li>
<li>周三：把上一周没读完的文献继续阅读，感觉说了很多废话，但不知道他的实验怎么做的具体怎么实施的，一大堆定义搞的晕头转向。下午无聊不想看论文就刷了几个题。  </li>
<li>周四：继续看文献，但根据所学方法非重点部分不去纠结，最后把conclusion with future work翻译完。第一篇文献阅读完毕，然而，毫无感觉。又刷了几个题，利用看文献的两天时间把刷题训练增加进来也不错。</li>
<li>周五：开始实现正则逻辑回归，存在一点点问题就是文档给的求参数的方法是调库，但我不会scipy库和pandas库。难道这就是传说中的调库侠嘛？我用的还是梯度下降的方法跑出的参数实验效果不是很好，与文档有一定的差别。</li>
<li>周六：上午10点半起的，吃完午饭才去的实验室。原本导师定的周六下午开组会，中午蹭了蹭牛客多校联赛，发现自己的思维与区域赛水平差很远了，被学弟爆虐。找了其他学校的一场校赛补补水题维持生活。  </li>
<li>周日：今天天气热的一批，上午很晚去中山陵，然后下来去南京农业蹭饭，都已经一点了。下午去博物院，历史馆毫无意思，后面去数字馆还行，到了民国馆才是最好玩的。后面右去了艺术馆。出门旅游简直就是花钱买罪受。   </li>
</ul>
<p>关键成果（KR）：</p>
<ul>
<li>逻辑回归完成度算个90%吧。</li>
<li>文献完成度页算个90%吧。</li>
<li>另外还做了两套校赛题，没有AK，没有AK。</li>
</ul>
<p>下周目标（O）：</p>
<ul>
<li>神经网络学习</li>
<li>开启另一篇文献</li>
<li>pandas库和scipy库！</li>
</ul>
]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title>北京信息科技大学第十一届程序设计竞赛 部分题解</title>
    <url>/2019/07/21/%E5%8C%97%E4%BA%AC%E4%BF%A1%E6%81%AF%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E7%AC%AC%E5%8D%81%E4%B8%80%E5%B1%8A%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%AB%9E%E8%B5%9B-%E9%83%A8%E5%88%86%E9%A2%98%E8%A7%A3/</url>
    <content><![CDATA[<h3 id="北京信息科技大学第十一届程序设计竞赛"><a href="#北京信息科技大学第十一届程序设计竞赛" class="headerlink" title="北京信息科技大学第十一届程序设计竞赛"></a><center><a href="https://ac.nowcoder.com/acm/contest/940#question" target="_blank" rel="noopener">北京信息科技大学第十一届程序设计竞赛</a></center></h3><p>感觉没有吉首大学那场好玩。<br>老年选手智商尽失，勉勉强强刷刷水题维持生活。  </p>
<h4 id="A-kotori和糖果"><a href="#A-kotori和糖果" class="headerlink" title="A    kotori和糖果"></a>A    kotori和糖果</h4><p>卡了一天，made竟然是个煞笔题，用map取重剪去搜索节点就可以了，但竟然还会爆INT？？？？<br>题意：n堆石头，每堆一个，合并的代价是两堆数量差值。求合并成一堆最小代价。<br>分析：哈夫曼合并树模型，容易推出公式f(n)=f(n/2)+f((n+1)/2)+(n&amp;1)，但对于n=1e18运算节点太多重复太多，所以用map处理。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include&lt;bits/stdc++.h&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">const int INF=0x3f3f3f;</span><br><span class="line">const int N=1e3+10;</span><br><span class="line">map&lt;long long ,long long &gt;q;</span><br><span class="line">long long fun(long long n)</span><br><span class="line">&#123;</span><br><span class="line">    if(n&lt;=2) return q[n]=0;</span><br><span class="line">    if(q[n]) return q[n];</span><br><span class="line">    return q[n]=fun(n/2)+fun((n+1)/2)+(n&amp;1);</span><br><span class="line">&#125;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    int t;</span><br><span class="line">    long long n;</span><br><span class="line">    scanf(&quot;%d&quot;,&amp;t);</span><br><span class="line">    q.clear();</span><br><span class="line">    while(t--)</span><br><span class="line">    &#123;</span><br><span class="line">        scanf(&quot;%lld&quot;,&amp;n);</span><br><span class="line">        printf(&quot;%lld\n&quot;,fun(n));</span><br><span class="line">    &#125;</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="B-kotori和气球"><a href="#B-kotori和气球" class="headerlink" title="B    kotori和气球"></a>B    kotori和气球</h4><p>题意：n中颜色数量不限的气球，选m个摆成一排，相同颜色不能相邻，方案数对109取余。<br>分析：<code>ans=n*(n-1)*(n-1)* *** *(n-1)</code><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include&lt;bits/stdc++.h&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">const int MOD=109;</span><br><span class="line">const int N=1e5+10;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    int n,m;</span><br><span class="line">    while(~scanf(&quot;%d%d&quot;,&amp;n,&amp;m))</span><br><span class="line">    &#123;</span><br><span class="line">        int ans=n;</span><br><span class="line">        for(int i=1;i&lt;m;i++)</span><br><span class="line">            ans=ans*(n-1)%MOD;</span><br><span class="line">        printf(&quot;%d\n&quot;,ans);</span><br><span class="line">    &#125;</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="C-kotori和出道"><a href="#C-kotori和出道" class="headerlink" title="C    kotori和出道"></a>C    kotori和出道</h4><p>题意：n个人始终成一个环，依次报数，偶数出列，求最后剩下的人是谁。<br>分析：经典约瑟夫环问题。但这题打个表规律就出来了。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import math</span><br><span class="line">while True:</span><br><span class="line">    try:</span><br><span class="line">        t=int(input())</span><br><span class="line">        for i in range(t):</span><br><span class="line">            n=int(input())</span><br><span class="line">            x=int(math.log2(n))</span><br><span class="line">            n=n-2**x</span><br><span class="line">            print(2*n+1)</span><br><span class="line">    except:break</span><br></pre></td></tr></table></figure></p>
<h4 id="D-kotori和迷宫"><a href="#D-kotori和迷宫" class="headerlink" title="D    kotori和迷宫"></a>D    kotori和迷宫</h4><p>题意：意思就是从起点开始搜，到了出口’e’就直接出去，求能到达的出口数量和最近出口的距离。<br>分析：在广搜的基础上加一句即可，判断当前点是否为出口’e’，是直接<code>continue</code>。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include&lt;bits/stdc++.h&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">const int MOD=109;</span><br><span class="line">const int N=1e5+10;</span><br><span class="line">int n,m,ans1,ans2,vis[55][55];</span><br><span class="line">char s[55][55];</span><br><span class="line">struct node</span><br><span class="line">&#123;</span><br><span class="line">    int x,y,step;</span><br><span class="line">&#125;;</span><br><span class="line">queue&lt;node&gt;q;</span><br><span class="line">void bfs(int x,int y,int step)</span><br><span class="line">&#123;</span><br><span class="line">    ans1=0,ans2=90;</span><br><span class="line">    node tmp;</span><br><span class="line">    tmp.x=x,tmp.y=y,tmp.step=0;</span><br><span class="line">    memset(vis,0,sizeof(vis));</span><br><span class="line">    while(!q.empty()) q.pop();</span><br><span class="line">    q.push(tmp);</span><br><span class="line">    while(!q.empty())</span><br><span class="line">    &#123;</span><br><span class="line">        tmp=q.front();</span><br><span class="line">        q.pop();</span><br><span class="line">        if(vis[tmp.x][tmp.y]) continue;</span><br><span class="line">        vis[tmp.x][tmp.y]=1;</span><br><span class="line">        if(s[tmp.x][tmp.y]==&apos;e&apos;)</span><br><span class="line">        &#123;</span><br><span class="line">            ans1++;</span><br><span class="line">            ans2=min(ans2,tmp.step);</span><br><span class="line">            continue;</span><br><span class="line">        &#125;</span><br><span class="line">        if(tmp.x-1&gt;0&amp;&amp;s[tmp.x-1][tmp.y]!=&apos;*&apos;&amp;&amp;!vis[tmp.x-1][tmp.y])</span><br><span class="line">        q.push(node&#123;tmp.x-1,tmp.y,tmp.step+1&#125;);</span><br><span class="line">        if(tmp.x+1&lt;=n&amp;&amp;s[tmp.x+1][tmp.y]!=&apos;*&apos;&amp;&amp;!vis[tmp.x+1][tmp.y])</span><br><span class="line">        q.push(node&#123;tmp.x+1,tmp.y,tmp.step+1&#125;);</span><br><span class="line">        if(tmp.y-1&gt;0&amp;&amp;s[tmp.x][tmp.y-1]!=&apos;*&apos;&amp;&amp;!vis[tmp.x][tmp.y-1])</span><br><span class="line">        q.push(node&#123;tmp.x,tmp.y-1,tmp.step+1&#125;);</span><br><span class="line">        if(tmp.y+1&lt;=m&amp;&amp;s[tmp.x][tmp.y+1]!=&apos;*&apos;&amp;&amp;!vis[tmp.x][tmp.y+1])</span><br><span class="line">        q.push(node&#123;tmp.x,tmp.y+1,tmp.step+1&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    if(ans1==0) puts(&quot;-1&quot;);</span><br><span class="line">    else printf(&quot;%d %d\n&quot;,ans1,ans2);</span><br><span class="line">&#125;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    while(~scanf(&quot;%d%d&quot;,&amp;n,&amp;m))</span><br><span class="line">    &#123;</span><br><span class="line">        int sx,sy;</span><br><span class="line">        for(int i=1; i&lt;=n; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            scanf(&quot;%s&quot;,s[i]+1);</span><br><span class="line">            for(int j=1; j&lt;=m; j++)</span><br><span class="line">                if(s[i][j]==&apos;k&apos;)</span><br><span class="line">                    sx=i,sy=j;</span><br><span class="line">        &#125;</span><br><span class="line">        bfs(sx,sy,0);</span><br><span class="line">    &#125;</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="E-kotori和素因子"><a href="#E-kotori和素因子" class="headerlink" title="E    kotori和素因子"></a>E    kotori和素因子</h4><ul>
<li>题意：n&le;10个数，每个数&le;1000。从每个数中选出一个素因子，要求所选出的所有素因子不同。求这些素因子的和的最小值。  </li>
<li>分析：正解是暴搜即可。1000以内的数每个数由最多5个素因子之积组成，那么暴力的复杂度是5<sup>10</sup>=9765625。  </li>
<li>题解2：费用流模型。源点0到[1-n]这n个数分别建边容量为1，费用为0；n个数每个数分别与它的素因子建边，容量为1，费用为0（或1，这样素因子到汇点的费用为0即可）；素因子到汇点T建边，容量为1，费用为素因子大小。</li>
<li>想想看这样为什么费用流模型可以？费用流模型在最大流的基础上求最小费用，流量自然是我们这n个数每个数都要选一个，我们给每个数i都创建流量为1的通路，那么最终最大流量就是n，不到n输出-1；费用是素因子对答案的贡献，当一个数有多个素因子时就有多个选择，那么费用流模型会自动找寻增广轨寻求费用最小的那条通道。费用流模型能解决更大的数据范围，具体没算。被自己的机智折服。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include&lt;bits/stdc++.h&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">const int INF=0x3f3f3f;</span><br><span class="line">const int N=1e3+10;</span><br><span class="line">struct Edge</span><br><span class="line">&#123;</span><br><span class="line">    int to,next,cap,flow,cost;</span><br><span class="line">&#125; e[N*10];</span><br><span class="line">int head[N],tot,tn;</span><br><span class="line">int pre[N],dis[N];</span><br><span class="line">int vis[N];</span><br><span class="line">void init(int num)</span><br><span class="line">&#123;</span><br><span class="line">    tot=0;//边的数量</span><br><span class="line">    tn=num;</span><br><span class="line">    memset(head,-1,sizeof(head));</span><br><span class="line">&#125;</span><br><span class="line">void add(int u,int v,int cap,int cost)</span><br><span class="line">&#123;</span><br><span class="line">    e[tot].to=v,e[tot].cap=cap,e[tot].cost=cost,e[tot].flow=0;</span><br><span class="line">    e[tot].next=head[u];</span><br><span class="line">    head[u]=tot++;</span><br><span class="line">    e[tot].to=u,e[tot].cap=0,e[tot].cost=-cost,e[tot].flow=0;</span><br><span class="line">    e[tot].next=head[v];</span><br><span class="line">    head[v]=tot++;</span><br><span class="line">&#125;</span><br><span class="line">bool spfa(int s,int t)</span><br><span class="line">&#123;</span><br><span class="line">    queue&lt;int&gt;q;</span><br><span class="line">    for(int i=0; i&lt;=tn; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        dis[i]=INF;</span><br><span class="line">        vis[i]=0;</span><br><span class="line">        pre[i]=-1;</span><br><span class="line">    &#125;</span><br><span class="line">    dis[s]=0;</span><br><span class="line">    vis[s]=1;</span><br><span class="line">    q.push(s);</span><br><span class="line">    while(!q.empty())</span><br><span class="line">    &#123;</span><br><span class="line">        int u=q.front();</span><br><span class="line">        q.pop();</span><br><span class="line">        vis[u]=0;</span><br><span class="line">        for(int i=head[u]; i+1; i=e[i].next)</span><br><span class="line">        &#123;</span><br><span class="line">            int v=e[i].to;</span><br><span class="line">            if(e[i].cap&gt;e[i].flow&amp;&amp;dis[v]&gt;dis[u]+e[i].cost)</span><br><span class="line">            &#123;</span><br><span class="line">                dis[v]=dis[u]+e[i].cost;</span><br><span class="line">                pre[v]=i;</span><br><span class="line">                if(!vis[v])</span><br><span class="line">                &#123;</span><br><span class="line">                    vis[v]=1;</span><br><span class="line">                    q.push(v);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    if(pre[t]==-1)  return false;</span><br><span class="line">    return true;</span><br><span class="line">&#125;</span><br><span class="line">void mincost_maxflow(int s,int t,int n)</span><br><span class="line">&#123;</span><br><span class="line">    int cost=0;</span><br><span class="line">    int flow=0;</span><br><span class="line">    while(spfa(s,t))</span><br><span class="line">    &#123;</span><br><span class="line">        int Min=INF;</span><br><span class="line">        for(int i=pre[t]; i!=-1; i=pre[e[i^1].to])</span><br><span class="line">            if(Min&gt;e[i].cap-e[i].flow)</span><br><span class="line">                Min=e[i].cap-e[i].flow;</span><br><span class="line">        for(int i=pre[t]; i!=-1; i=pre[e[i^1].to])</span><br><span class="line">        &#123;</span><br><span class="line">            e[i].flow+=Min;</span><br><span class="line">            e[i^1].flow-=Min;</span><br><span class="line">            cost+=e[i].cost*Min;</span><br><span class="line">        &#125;</span><br><span class="line">        flow+=Min;</span><br><span class="line">    &#125;</span><br><span class="line">    if(flow&lt;n||flow&gt;=INF) puts(&quot;-1&quot;);</span><br><span class="line">    else printf(&quot;%d\n&quot;,cost);</span><br><span class="line">&#125;</span><br><span class="line">int prime[1003],Vis[1003];</span><br><span class="line">int a[50];</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    int n,cnt=0;</span><br><span class="line">    memset(prime,-1,sizeof(prime));</span><br><span class="line">    prime[0]=prime[1]=0;</span><br><span class="line">    for(int i=2; i&lt;=1000; i++)</span><br><span class="line">        if(prime[i])</span><br><span class="line">        &#123;</span><br><span class="line">            for(int j=i*i; j&lt;=1000; j+=i)</span><br><span class="line">                prime[j]=0;</span><br><span class="line">            prime[cnt++]=i;</span><br><span class="line">        &#125;</span><br><span class="line">    while(~scanf(&quot;%d&quot;,&amp;n))</span><br><span class="line">    &#123;</span><br><span class="line">        memset(Vis,0,sizeof(Vis));</span><br><span class="line">        int Cnt=0;</span><br><span class="line">        for(int i=1; i&lt;=n; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            scanf(&quot;%d&quot;,&amp;a[i]);</span><br><span class="line">            for(int j=0; j&lt;cnt; j++)</span><br><span class="line">                if(a[i]%prime[j]==0&amp;&amp;Vis[prime[j]]==0)</span><br><span class="line">                    Vis[prime[j]]=++Cnt;</span><br><span class="line">        &#125;</span><br><span class="line">        init(1+n+Cnt+2);</span><br><span class="line">        for(int i=1; i&lt;=n; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            add(0,i,1,0);  //源点到i</span><br><span class="line">            for(int j=0; j&lt;cnt; j++)</span><br><span class="line">                if(a[i]%prime[j]==0)</span><br><span class="line">                    add(i,n+Vis[prime[j]],1,prime[j]);  //i到素因子</span><br><span class="line">        &#125;</span><br><span class="line">        for(int j=0; j&lt;cnt; j++)</span><br><span class="line">            if(Vis[prime[j]])</span><br><span class="line">                add(n+Vis[prime[j]],n+Cnt+2,1,0);  //素因子到汇点</span><br><span class="line">        mincost_maxflow(0,n+Cnt+2,n);</span><br><span class="line">    &#125;</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="H-andy和购物"><a href="#H-andy和购物" class="headerlink" title="H    andy和购物"></a>H    andy和购物</h4><p>题意：水题，排序计算内积即可。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">while True:</span><br><span class="line">    try:</span><br><span class="line">        t=int(input())</span><br><span class="line">        for i in range(t):</span><br><span class="line">            n=int(input())</span><br><span class="line">            a=list(map(int,input().split()))</span><br><span class="line">            b=list(map(float,input().split()))</span><br><span class="line">            a.sort()</span><br><span class="line">            b.sort(reverse=True)</span><br><span class="line">            ans=0</span><br><span class="line">            for j in range(n):</span><br><span class="line">                ans=ans+a[j]*b[j]</span><br><span class="line">            print(&apos;%.3f&apos;%ans)</span><br><span class="line">    except:break</span><br></pre></td></tr></table></figure></p>
<h4 id="I-andy种树"><a href="#I-andy种树" class="headerlink" title="I    andy种树"></a>I    andy种树</h4><p>题意：每次选一个区间加一，然后最后想知道1-n每个坐标的数是多少。<br>分析：会线段树用线段树区间更新单点查询裸题。不会线段树用左家右减法即可，区间左端点加1，区间右端点再往右一个的位置减一，最后累加。烂大街的技巧。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include&lt;bits/stdc++.h&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">const int MOD=1e9+7;</span><br><span class="line">const int N=1e6+10;</span><br><span class="line">int a[N];</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    int n,m;</span><br><span class="line">    while(~scanf(&quot;%d%d&quot;,&amp;n,&amp;m))</span><br><span class="line">    &#123;</span><br><span class="line">        memset(a,0,sizeof(a));</span><br><span class="line">        int l,r;</span><br><span class="line">        for(int i=1;i&lt;=m;i++)</span><br><span class="line">        &#123;</span><br><span class="line">         scanf(&quot;%d%d&quot;,&amp;l,&amp;r);</span><br><span class="line">         a[l]++;</span><br><span class="line">         a[r+1]--;</span><br><span class="line">        &#125;</span><br><span class="line">        for(int i=1;i&lt;=n;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            a[i]+=a[i-1];</span><br><span class="line">            printf(&quot;%d%c&quot;,a[i],i==n?&apos;\n&apos;:&apos; &apos;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="J-andy的树被砍了"><a href="#J-andy的树被砍了" class="headerlink" title="J    andy的树被砍了"></a>J    andy的树被砍了</h4><p>题意：煞笔题还想好好伪装。<br>分析：伤害直接累加，然后求某棵树先加上对应前一个的累加伤害值，再在伤害数组中lower_bound()二分查找。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include&lt;bits/stdc++.h&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">const int MOD=1e9+7;</span><br><span class="line">const int N=1e6+10;</span><br><span class="line">long long a[N],b[N];</span><br><span class="line">int n;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    while(~scanf(&quot;%d&quot;,&amp;n))</span><br><span class="line">    &#123;</span><br><span class="line">        a[0]=b[0]=0;</span><br><span class="line">        for(int i=1; i&lt;=n; i++)</span><br><span class="line">            scanf(&quot;%lld&quot;,&amp;a[i]);</span><br><span class="line">        for(int i=1; i&lt;=n; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            scanf(&quot;%lld&quot;,&amp;b[i]);</span><br><span class="line">            b[i]+=b[i-1];</span><br><span class="line">        &#125;</span><br><span class="line">        for(int i=1; i&lt;=n; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            int pos=lower_bound(b+1,b+n+1,a[i]+b[i-1])-b;</span><br><span class="line">            printf(&quot;%d%c&quot;,pos,i==n?&apos;\n&apos;:&apos; &apos;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>题解</category>
      </categories>
      <tags>
        <tag>题解</tag>
        <tag>牛客网</tag>
      </tags>
  </entry>
  <entry>
    <title>吉首大学2019年程序设计竞赛 部分题解</title>
    <url>/2019/07/18/%E5%90%89%E9%A6%96%E5%A4%A7%E5%AD%A62019%E5%B9%B4%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%AB%9E%E8%B5%9B-%E9%83%A8%E5%88%86%E9%A2%98%E8%A7%A3/</url>
    <content><![CDATA[<h3 id="吉首大学2019年程序设计竞赛"><a href="#吉首大学2019年程序设计竞赛" class="headerlink" title="吉首大学2019年程序设计竞赛"></a><center><a href="https://ac.nowcoder.com/acm/contest/992#question" target="_blank" rel="noopener">吉首大学2019年程序设计竞赛</a></center></h3><h4 id="B-干物妹小埋"><a href="#B-干物妹小埋" class="headerlink" title="B    干物妹小埋"></a>B    干物妹小埋</h4><p>题意：求一个单调不减序列的最大权值和。<br>分析： 数据范围2e5，用数据结构。按高度排序去重放进线段树中，区间查询最值单点更新即可，注意数据范围。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include&lt;bits/stdc++.h&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">const int MOD=1e9+7;</span><br><span class="line">const int N=2e5+10;</span><br><span class="line">int n,b[N],bb[N],vis[N];</span><br><span class="line">long long c[N];</span><br><span class="line">struct node</span><br><span class="line">&#123;</span><br><span class="line">    int l,r;</span><br><span class="line">    long long happy;</span><br><span class="line">&#125; a[N&lt;&lt;2];</span><br><span class="line">void build(int l,int r,int k)</span><br><span class="line">&#123;</span><br><span class="line">    a[k].l=l,a[k].r=r,a[k].happy=0;</span><br><span class="line">    if(l==r) return ;</span><br><span class="line">    int mid=(l+r)&gt;&gt;1;</span><br><span class="line">    build(l,mid,2*k);</span><br><span class="line">    build(mid+1,r,2*k+1);</span><br><span class="line">&#125;</span><br><span class="line">void update(int id,int  num,int k)</span><br><span class="line">&#123;</span><br><span class="line">    if(id==a[k].l&amp;&amp;a[k].r==id)</span><br><span class="line">    &#123;</span><br><span class="line">        a[k].happy=num;</span><br><span class="line">        return ;</span><br><span class="line">    &#125;</span><br><span class="line">    int mid=(a[k].l+a[k].r)&gt;&gt;1;</span><br><span class="line">    if(id&lt;=mid) update(id,num,2*k);</span><br><span class="line">    else update(id,num,2*k+1);</span><br><span class="line">    a[k].happy=max(a[k*2].happy,a[k*2+1].happy);</span><br><span class="line">&#125;</span><br><span class="line">long long query(int l,int r,int k)</span><br><span class="line">&#123;</span><br><span class="line">    if(l&lt;=a[k].l&amp;&amp;r&gt;=a[k].r) return a[k].happy;</span><br><span class="line">    int mid=(a[k].l+a[k].r)&gt;&gt;1;</span><br><span class="line">    if(r&lt;=mid) return query(l,r,2*k);</span><br><span class="line">    else if(l&gt;mid) return query(l,r,2*k+1);</span><br><span class="line">    else return max(query(l,mid,2*k),query(mid+1,r,2*k+1));</span><br><span class="line">&#125;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    while(~scanf(&quot;%d&quot;,&amp;n))</span><br><span class="line">    &#123;</span><br><span class="line">        for(int i=1; i&lt;=n; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            scanf(&quot;%d&quot;,&amp;b[i]);</span><br><span class="line">            bb[i]=b[i];</span><br><span class="line">        &#125;</span><br><span class="line">        for(int i=1; i&lt;=n; i++) scanf(&quot;%lld&quot;,&amp;c[i]);</span><br><span class="line">        sort(bb+1,bb+n+1);</span><br><span class="line">        int pos,k=unique(bb+1,bb+n+1)-bb-1;</span><br><span class="line">        build(1,k,1);</span><br><span class="line">        for(int i=1; i&lt;=n; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            pos=lower_bound(bb+1,bb+k+1,b[i])-bb;</span><br><span class="line">            long long cnt=query(1,pos,1);</span><br><span class="line">            update(pos,cnt+c[i],1);</span><br><span class="line">        &#125;</span><br><span class="line">        printf(&quot;%lld\n&quot;,a[1].happy);</span><br><span class="line">    &#125;</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="E-多喝嘤料"><a href="#E-多喝嘤料" class="headerlink" title="E    多喝嘤料"></a>E    多喝嘤料</h4><p>题意：喝饮料，三个瓶子或4个瓶盖换一瓶，起初n瓶，求最多能喝多少瓶。<br>分析：判断边界即可，无法再喝是因为无法兑换了。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">while True:</span><br><span class="line">    try:</span><br><span class="line">        t=int(input())</span><br><span class="line">        for i in range(t):</span><br><span class="line">            n=int(input())</span><br><span class="line">            ans=n</span><br><span class="line">            kong=gai=n</span><br><span class="line">            while kong//3+gai//4&gt;0:</span><br><span class="line">                n=(kong//3)+(gai//4)</span><br><span class="line">                kong=kong%3+n</span><br><span class="line">                gai=gai%4+n</span><br><span class="line">                ans=ans+n</span><br><span class="line">                # print(n,kong,gai)</span><br><span class="line">            print(ans)</span><br><span class="line">    except:break</span><br></pre></td></tr></table></figure></p>
<h4 id="F-天花乱坠"><a href="#F-天花乱坠" class="headerlink" title="F 天花乱坠"></a>F 天花乱坠</h4><p>题意：一个正n边形，初始每条边都为100。现沿着中点再作正n边形，一直重复，求所构成图案所有边边长和。<br>分析：这是一个收敛的数列求和取极限问题。根据余弦公式cosb=(a<sup>2</sup>+b<sup>2</sup>-c<sup>2</sup>)/(2ab)，很容易求得新的边长c，由于答案保留两位有效数字，故将精度调为1e-8，高了错误，低了超时。注意精度问题，尽量少作除法，能预处理就先预处理，就这样卡过去了。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import  math</span><br><span class="line">from math import pi</span><br><span class="line">while True:</span><br><span class="line">    try:</span><br><span class="line">        n=int(input())</span><br><span class="line">        ans=0</span><br><span class="line">        a=100.0</span><br><span class="line">        angle=1-math.cos((n-2)*pi/n)</span><br><span class="line">        while True:</span><br><span class="line">            ans=ans+a</span><br><span class="line">            a=math.sqrt(a*a*angle/2)</span><br><span class="line">            if a&lt;1e-8:break;</span><br><span class="line">        print(&quot;%.2f&quot; %(ans*n))</span><br><span class="line">    except:break</span><br></pre></td></tr></table></figure></p>
<h4 id="G-说能过那是假的"><a href="#G-说能过那是假的" class="headerlink" title="G    说能过那是假的"></a>G    说能过那是假的</h4><p>题意：给你一个由’O’、’R’、’Z’组成的字符串，求正序组成’ORZ’的情况有多少种。<br>分析： 三层暴力可能超时，考虑预处理。以’R’为中间点，每个’R’对答案的贡献就是左边的’O’的个数乘以右边的’Z’的个数。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">while True:</span><br><span class="line">    try:</span><br><span class="line">        s=input()</span><br><span class="line">        leng=len(s)</span><br><span class="line">        a=[[0]*(leng+2) for i in range(2)] # 申请二维数组</span><br><span class="line">        for id in range(leng): # 预处理O</span><br><span class="line">            a[0][id+1]=a[0][id]</span><br><span class="line">            if s[id]==&apos;O&apos;:a[0][id+1]=a[0][id+1]+1</span><br><span class="line">        for id in range(leng,0,-1): # 预处理Z</span><br><span class="line">            a[1][id]=a[1][id+1]</span><br><span class="line">            if s[id-1]==&apos;Z&apos;:a[1][id]=a[1][id]+1</span><br><span class="line">        ans=0</span><br><span class="line">        for id in range(leng):</span><br><span class="line">            if s[id]==&apos;R&apos;:ans=ans+a[1][id+1]*a[0][id+1]</span><br><span class="line">        print(ans)</span><br><span class="line">    except:break</span><br></pre></td></tr></table></figure></p>
<h4 id="H-蛇皮走位"><a href="#H-蛇皮走位" class="headerlink" title="H    蛇皮走位"></a>H    蛇皮走位</h4><p>题意：用26个小写英文字母按反’S’形填充矩阵<br>分析：分奇偶行直接输出即可。<br>Python实现：ord(c)表示字符c的ascii值，chr(x)表示数字x对应的ascii字符。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">while True:</span><br><span class="line">    try:</span><br><span class="line">        n=int(input())</span><br><span class="line">        x=n*2+1</span><br><span class="line">        id=0</span><br><span class="line">        for i in range(x):</span><br><span class="line">            if i&amp;1==0:</span><br><span class="line">                for j in range(x):</span><br><span class="line">                    print(chr(id+97),end=&quot;&quot;)</span><br><span class="line">                    id=(id+1)%26</span><br><span class="line">            else:</span><br><span class="line">                id=(id+x-1)%26</span><br><span class="line">                for j in range(x):</span><br><span class="line">                    print(chr(id+97),end=&quot;&quot;)</span><br><span class="line">                    id=(id-1+26)%26</span><br><span class="line">                id=(id+x+1)%26</span><br><span class="line">            print(&quot;&quot;)</span><br><span class="line">    except:break</span><br></pre></td></tr></table></figure></p>
<h4 id="J-滑稽树下你和我"><a href="#J-滑稽树下你和我" class="headerlink" title="J    滑稽树下你和我"></a>J    滑稽树下你和我</h4><p>题意：n个点的树，每条边有一个距离。求树上任意两点的距离之和。<br>分析：突破点在于求每条边对答案贡献。也就是算每条边经过多少次，就是这条边左边的所有点乘以右边的所有点（树上每条边都是桥）。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include&lt;bits/stdc++.h&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">const int MOD=1e9+7;</span><br><span class="line">const int N=1e5+10;</span><br><span class="line">int n,tot,head[N];</span><br><span class="line">long long cnt[N];</span><br><span class="line">long long ans;</span><br><span class="line">struct Edge</span><br><span class="line">&#123;</span><br><span class="line">    int to,next,w;</span><br><span class="line">&#125;e[N*2];</span><br><span class="line">void init()</span><br><span class="line">&#123;</span><br><span class="line">    tot=0;</span><br><span class="line">    memset(head,-1,sizeof(head));</span><br><span class="line">    memset(cnt,0,sizeof(cnt));</span><br><span class="line">&#125;</span><br><span class="line">void add(int u,int v,int w)</span><br><span class="line">&#123;</span><br><span class="line">    e[tot].to=v,e[tot].w=w,e[tot].next=head[u];</span><br><span class="line">    head[u]=tot++;</span><br><span class="line">    e[tot].to=u,e[tot].w=w,e[tot].next=head[v];</span><br><span class="line">    head[v]=tot++;</span><br><span class="line">&#125;</span><br><span class="line">void dfs1(int u,int fa)</span><br><span class="line">&#123;</span><br><span class="line">    for(int i=head[u];i+1;i=e[i].next)</span><br><span class="line">    &#123;</span><br><span class="line">        int v=e[i].to;</span><br><span class="line">        if(v==fa) continue;</span><br><span class="line">        dfs1(v,u);</span><br><span class="line">        cnt[u]+=cnt[v];</span><br><span class="line">    &#125;</span><br><span class="line">    cnt[u]++;  // 加上自己</span><br><span class="line">&#125;</span><br><span class="line">void dfs2(int u,int fa)</span><br><span class="line">&#123;</span><br><span class="line">    for(int i=head[u];i+1;i=e[i].next)</span><br><span class="line">    &#123;</span><br><span class="line">        int v=e[i].to;</span><br><span class="line">        if(v==fa) continue;</span><br><span class="line">        ans=(ans+cnt[v]*(n-cnt[v])*e[i].w%MOD)%MOD;</span><br><span class="line">        dfs2(v,u);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    while(~scanf(&quot;%d&quot;,&amp;n))</span><br><span class="line">    &#123;</span><br><span class="line">         init();</span><br><span class="line">         int u,v,w;</span><br><span class="line">         for(int i=1;i&lt;n;i++)</span><br><span class="line">         &#123;</span><br><span class="line">             scanf(&quot;%d%d%d&quot;,&amp;u,&amp;v,&amp;w);</span><br><span class="line">             add(u,v,w);</span><br><span class="line">         &#125;</span><br><span class="line">         dfs1(1,1); # 预处理每个点子树所有点数量</span><br><span class="line">         ans=0;</span><br><span class="line">         dfs2(1,1);</span><br><span class="line">         printf(&quot;%lld\n&quot;,ans);</span><br><span class="line">    &#125;</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>题解</category>
      </categories>
      <tags>
        <tag>题解</tag>
        <tag>牛客网</tag>
      </tags>
  </entry>
  <entry>
    <title>吴恩达机器学习第二次编程作业-逻辑回归</title>
    <url>/2019/07/16/%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E6%AC%A1%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h3 id="课时60编程作业：逻辑回归"><a href="#课时60编程作业：逻辑回归" class="headerlink" title="课时60编程作业：逻辑回归"></a><center><a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/text?lessonId=1053422066&amp;courseId=1004570029" target="_blank" rel="noopener">课时60编程作业：逻辑回归</a></center></h3><p>工具：pycharm2019.1，Python3.5  </p>
<p>参考资料：<a href="https://blog.csdn.net/xiaoxiangzi222/article/details/55097570" target="_blank" rel="noopener"> Logistic回归总结</a>、<a href="https://blog.csdn.net/huahuaxiaoshao/article/details/85174312" target="_blank" rel="noopener">梯度下降求解逻辑回归
</a>、<a href="https://blog.csdn.net/CoderPai/article/details/83384499" target="_blank" rel="noopener">数据提取</a>、<a href="https://www.jianshu.com/p/b479ec2da7c2" target="_blank" rel="noopener">Python 不同颜色可视化数据集</a>、<a href="https://jingyan.baidu.com/article/0eb457e508b6d303f0a90572.html" target="_blank" rel="noopener">Dataframe筛选数据</a>、<a href="https://blog.csdn.net/qq_30163461/article/details/80080529" target="_blank" rel="noopener">DataFrame类型转换成array类型</a>  、<a href="https://www.cnblogs.com/dan-baishucaizi/p/9398801.html" target="_blank" rel="noopener">pandas数据读写</a></p>
<p><a href="https://paste.ubuntu.com/p/w992jdTZNw/" target="_blank" rel="noopener">逻辑回归完整代码</a><br><a href="https://paste.ubuntu.com/p/8F6KP3xk3M/" target="_blank" rel="noopener">正则化完整代码</a></p>
<hr>
<h4 id="1-逻辑回归"><a href="#1-逻辑回归" class="headerlink" title="1. 逻辑回归"></a>1. 逻辑回归</h4><p>任务：建立一个逻辑回归分类模型来预测一个学生是否能被大学录取</p>
<h5 id="1-1-数据可视化"><a href="#1-1-数据可视化" class="headerlink" title="1.1 数据可视化"></a>1.1 数据可视化</h5><p>先利用pandas库提取数据，然后分类，绘制。<br>pandas需要手动添加一行表示列名，不然以第一行数据作为列名，数据类型自动识别。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def pandas_getData():</span><br><span class="line">    data = pd.read_csv(&apos;./ex2/ex2data1.txt&apos;)</span><br><span class="line">    y = data[&apos;y&apos;]   # 按列名提取</span><br><span class="line">    data1 = data[y == 0]  # 数据分类</span><br><span class="line">    data2 = data[y == 1]  # 已录取</span><br><span class="line">    return data,data1,data2,data.shape[0]</span><br><span class="line"></span><br><span class="line">def plot_data(data1,data2):</span><br><span class="line">    plt.plot(data1[&apos;x1&apos;],data1[&apos;x2&apos;],&apos;yo&apos;,label=&apos;Not admitted&apos;)</span><br><span class="line">    plt.plot(data2[&apos;x1&apos;],data2[&apos;x2&apos;],&apos;k+&apos;,label=&apos;Admitted&apos;)</span><br><span class="line">    plt.xlim(30,101)</span><br><span class="line">    plt.ylim(30,100)</span><br><span class="line">    plt.xlabel(&apos;Exam 1 score&apos;,fontsize=10)</span><br><span class="line">    plt.ylabel(&apos;Exam 2 score&apos;, fontsize=10)</span><br><span class="line">    plt.legend(loc=1,fontsize=8)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://i.loli.net/2019/07/16/5d2d6e90a260a72174.png" alt="Figure_1.png"></p>
<h5 id="1-2-Cost-function-and-gradient"><a href="#1-2-Cost-function-and-gradient" class="headerlink" title="1.2 Cost function and gradient"></a>1.2 Cost function and gradient</h5><p><img src="https://i.loli.net/2019/07/16/5d2d7269b6c2f65646.png" alt="2019-07-16 14-44-13 的屏幕截图.png">  </p>
<p>关于为什么学习率&alpha;这里不用除以m均值化在参考资料中有回答说&alpha;是个常数，可以省掉m，本人不是很懂。<br>代价函数与梯度下降只要有线形回归的基础就很好计算：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Cost_Function(x,y,theta,m):</span><br><span class="line">    hypothesis=1/(1+np.exp(-np.dot(x,theta)))   # 假设函数</span><br><span class="line">    Cost_J =-1/m*(y*np.log(hypothesis)+(1-y)*np.log(1-hypothesis)).sum()</span><br><span class="line">    return Cost_J</span><br><span class="line"></span><br><span class="line">def Gradient_descent(x,y,theta,m,alpha,iterations):</span><br><span class="line">    for iteration in range(iterations):</span><br><span class="line">        hypothesis=1/(1+np.exp(-np.dot(x,theta)))</span><br><span class="line">        theta=theta-alpha*np.dot(x.T,hypothesis-y)</span><br><span class="line">    return theta   # 我们只需要最终的参数theta来进行预测</span><br></pre></td></tr></table></figure></p>
<p>用梯度下降的方法来计算参数&theta;还需要初始化参数init_theta、&alpha;以及迭代次数MaxIter，也别忘了X需要增加一列1表示x0。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data,data1,data2,m=pandas_getData()  # 获取数据</span><br><span class="line"># plot_data(data1,data2)  # 绘制数据</span><br><span class="line">x=np.array(data[[&apos;x1&apos;,&apos;x2&apos;]])  # 将DataFrame类型转换成array类型</span><br><span class="line">y=np.array(data[&apos;y&apos;]).reshape(m,1)</span><br><span class="line">x=np.hstack((np.ones((m,1)),x))   # 增加一列 X0=1</span><br><span class="line">init_theta=np.zeros((x.shape[1],1)) # 初始化theta</span><br><span class="line">MaxIter=2000000  # 迭代次数</span><br><span class="line">alpha=0.00001 # 学习率</span><br><span class="line">print(&apos;初始代价函数为：&apos;,Cost_Function(x,y,init_theta,m)) # 这里如果代价函数没写错计算出来的值应该是0.6931471805599453</span><br></pre></td></tr></table></figure></p>
<ul>
<li>在源文档中给出的求参数的方法是用Octave调库直接得出，网上找到的很多博客也是用scipy库函数。毫无疑问库中封装了最简介高效的方法。但如果我就想使用梯度下降呢？  </li>
<li>本人已经试过了，需要不停的调参（&alpha;和MaxIter），这对于机器性能是个考验。用库函数得出的最优参数是：[-25.16131867,   0.20623159,   0.20147149]，用这个参数向量求出的代价函数值为：0.2034977015894744。</li>
<li>本人在不停的调参过程中发现当我把学习率&alpha;设置为0.00001时，再一直增大迭代次数发现实验结果越来越接近文档给出的最优值，考虑到机器性能我只增加到了200000次，梯度下降得出的参数&theta;向量为：[-19.08468795   0.15767916   0.15228738]，用此参数求出的代价函数值为：0.21041324594328958</li>
<li>有了参数&theta;就可以用来进行预测和绘制决策边界线了</li>
</ul>
<h5 id="预测和决策边界线"><a href="#预测和决策边界线" class="headerlink" title="预测和决策边界线"></a>预测和决策边界线</h5><ol>
<li>还记得之前说过h<sub>&theta;</sub>(x)=1/(1+e<sup>-&theta;<sup>T</sup>X</sup>))，这个函数用来预测分类y=1概率。</li>
<li>我们可以设定阈值为0.5，只要h&gt;=0.5，那么就判定这是属于1，否则属于0。</li>
<li>上面是具体计算方法，我们可以有两种形式来预测：一种是用给定的样本X看看其h函数概率；还有一种是直接用训练样本跑，看有多少个正确分类了，这个百分比越高说明预测越准确。</li>
<li>以上是预测方法，那么关于决策边界线，这个在吴恩达机器学习视频中已经给出。没有实践很容易忽略这点，我们要观察h函数，它是一个sigma函数，只要&theta;<sup>T</sup>X的值大于等于0，那么h函数值就大于等于0.5，想想看是不是。而X向量是样本点，这条分界线就是若干个样本点，这些样本点所计算出来的&theta;<sup>T</sup>X=&theta;<sub>0</sub>+&theta;<sub>1</sub>X<sub>1</sub>+… 值应该等于0，两侧的点要么大于0计算出来的h值就大于0.5，反之同理。</li>
<li>用文档给出的样本[1 45 85]预测的概率为0.7221802，文档给出的概率为0.776。看来梯度下降算法确实不如其他算法，唯一的优点可能就是简单了。</li>
<li><p>先用训练样本来看看实验效果如何：  89.00%</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def text_sample(x,y,theta,m):</span><br><span class="line">    hypothesis=1/(1+np.exp(-np.dot(x,theta)))</span><br><span class="line">    cnt_correct=0</span><br><span class="line">    for i in range(m):</span><br><span class="line">        if hypothesis[i][0]&gt;=0.5 and y[i][0]==1:cnt_correct=cnt_correct+1 # 正确预测1</span><br><span class="line">        if hypothesis[i][0]&lt;0.5 and y[i][0]==0:cnt_correct=cnt_correct+1 # 正确预测0</span><br><span class="line">    correct_percent=cnt_correct/m  # 计算准确率</span><br><span class="line">    print(&apos;%.2f%%&apos; %(correct_percent*100))</span><br></pre></td></tr></table></figure>
</li>
<li><p>绘制决策边界，我们已经知道决策边界线的&theta;<sup>T</sup>X=&theta;<sub>0</sub>+&theta;<sub>1</sub>X<sub>1</sub>+…&theta;<sub>n</sub>X<sub>n</sub>=0，数据图的横轴是X1，纵轴是X2，那么随便造几百个X1，根据此公式计算X2再绘制直线即可。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def plotDecisionBoundary(data1,data2,x,y,theta):</span><br><span class="line">    plt.plot(data1[&apos;x1&apos;], data1[&apos;x2&apos;], &apos;yo&apos;, label=&apos;Not admitted&apos;)</span><br><span class="line">    plt.plot(data2[&apos;x1&apos;], data2[&apos;x2&apos;], &apos;k+&apos;, label=&apos;Admitted&apos;)</span><br><span class="line">    plt.xlim(30, 101)</span><br><span class="line">    plt.ylim(30, 100)</span><br><span class="line">    plt.xlabel(&apos;Exam 1 score&apos;, fontsize=10)</span><br><span class="line">    plt.ylabel(&apos;Exam 2 score&apos;, fontsize=10)</span><br><span class="line">    plt.legend(loc=1, fontsize=8)</span><br><span class="line"></span><br><span class="line">    x1 = np.arange(130, step=0.1)</span><br><span class="line">    x2 = -(final_theta[0] + x1 * final_theta[1]) / final_theta[2]</span><br><span class="line">    plt.plot(x1, x2)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><img src="https://i.loli.net/2019/07/17/5d2e7705009fa32467.png" alt="Figure_2.png"></p>
<p>至此，简单逻辑回归分类模型已经完成，下面是第二部分-正则化。</p>
<h4 id="2-正则化"><a href="#2-正则化" class="headerlink" title="2. 正则化"></a>2. 正则化</h4><p>芯片质检问题，两轮检测，一个结果，建立逻辑回归模型。</p>
<h5 id="2-1-绘制数据"><a href="#2-1-绘制数据" class="headerlink" title="2.1 绘制数据"></a>2.1 绘制数据</h5><p>和1.1一样，使用Pandas库提取数据，然后按y=0/1分类，传参绘制<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import matplotlib</span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">def Get_data():</span><br><span class="line">    data=pd.read_csv(&apos;./ex2/ex2data2.txt&apos;,names=[&apos;x1&apos;,&apos;x2&apos;,&apos;y&apos;]) # setting header or default setting header=None</span><br><span class="line">    data1=data[data[&apos;y&apos;]==1] # accepted</span><br><span class="line">    data2=data[data[&apos;y&apos;]==0] # rejected</span><br><span class="line">    print(data1.shape,data2.shape)</span><br><span class="line">    return data,data1,data2</span><br><span class="line"></span><br><span class="line">def Plot_Data(data1,data2):</span><br><span class="line">    plt.plot(data1[&apos;x1&apos;],data1[&apos;x2&apos;],&apos;k+&apos;,label=&apos;y=1&apos;)</span><br><span class="line">    plt.plot(data2[&apos;x1&apos;],data2[&apos;x2&apos;],&apos;yo&apos;,label=&apos;y=0&apos;)</span><br><span class="line">    plt.xlim(-1,1.5)</span><br><span class="line">    plt.ylim(-0.8,1.2)</span><br><span class="line">    plt.xlabel(&apos;Microchip Test 1&apos;,fontsize=10)</span><br><span class="line">    plt.ylabel(&apos;Microchip Test 2&apos;,fontsize=10)</span><br><span class="line">    plt.legend(loc=&apos;upper right&apos;,fontsize=8)</span><br><span class="line">    plt.show()</span><br><span class="line">data,data1,data2=Get_data()</span><br><span class="line">Plot_Data(data1,data2)</span><br></pre></td></tr></table></figure></p>
<p><img src="https://i.loli.net/2019/07/19/5d3168bc6cb0581582.png" alt="Figure_3.png"></p>
<h5 id="2-2-Feature-mapping-特征扩展"><a href="#2-2-Feature-mapping-特征扩展" class="headerlink" title="2.2  Feature mapping 特征扩展"></a>2.2  Feature mapping 特征扩展</h5><p>特征越多，维数越高，所作出的曲线拟合得也更好（第8章课程总结第8页）。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def mapFeature(x1, x2,m):</span><br><span class="line">    out=np.ones((m,1))</span><br><span class="line">    degree = 6</span><br><span class="line">    for i in range(1,degree+1):</span><br><span class="line">        for j in range(0,i+1):</span><br><span class="line">            out=np.hstack((out,((x1**(i-j))*(x2**j)).reshape(m,1)))</span><br><span class="line">    return out</span><br></pre></td></tr></table></figure></p>
<h5 id="2-3-Cost-function-and-gradient"><a href="#2-3-Cost-function-and-gradient" class="headerlink" title="2.3 Cost function and gradient"></a>2.3 Cost function and gradient</h5><p><img src="https://i.loli.net/2019/07/19/5d317815857a477982.png" alt="2019-07-19 15-57-31 的屏幕截图.png"><br><img src="https://i.loli.net/2019/07/19/5d31781572b6617121.png" alt="2019-07-19 15-57-35 的屏幕截图.png"><br>根据上面的公式就可以计算了。<br>先初始化：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data,data1,data2,m=Get_data()</span><br><span class="line"># Plot_Data(data1,data2)</span><br><span class="line">X=np.array(data[[&apos;x1&apos;,&apos;x2&apos;]])</span><br><span class="line">X=mapFeature(X.T[0],X.T[1],m)</span><br><span class="line">Y=np.array(data[&apos;y&apos;]).reshape(m,1)</span><br><span class="line">init_theta=np.zeros((X.shape[1],1))</span><br><span class="line">lamda=1</span><br><span class="line">alpha=0.0001</span><br><span class="line">MaxIter=2000000</span><br></pre></td></tr></table></figure></p>
<p>代价函数：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Cost_Function(X,Y,init_theta,m,lamda):</span><br><span class="line">    hypothesis=1/(1+np.exp(-X.dot(init_theta)))</span><br><span class="line">    Cost_J=-1/m*(Y*np.log(hypothesis)+(1-Y)*np.log(1-hypothesis)).sum()</span><br><span class="line">    Cost_J=Cost_J+lamda/(2*m)*(init_theta**2).sum()</span><br><span class="line">    return Cost_J</span><br></pre></td></tr></table></figure></p>
<p>梯度下降：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Gradient_descent(X,Y,theta,m,alpha,iterations,lamda):</span><br><span class="line">    for i in range(iterations):</span><br><span class="line">        tmp=theta[0][0] # theta0单独处理</span><br><span class="line">        theta=theta-alpha/m*(np.dot(X.T,(1/(1+np.exp(-X.dot(theta)))-Y))+lamda*theta)</span><br><span class="line">        theta[0][0]=tmp+alpha*lamda/m</span><br><span class="line">    return theta</span><br></pre></td></tr></table></figure></p>
<h5 id="2-4-绘制决策边界线"><a href="#2-4-绘制决策边界线" class="headerlink" title="2.4 绘制决策边界线"></a>2.4 绘制决策边界线</h5><p>这里是用等高线的方法作决策边界线，随机生成x1和x2，然后再用特征扩展方法扩展维数与&theta;向量相乘。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def plot_Decision_boundary(data1,data2,theta):</span><br><span class="line">    plt.plot(data1[&apos;x1&apos;], data1[&apos;x2&apos;], &apos;k+&apos;, label=&apos;y=1&apos;)</span><br><span class="line">    plt.plot(data2[&apos;x1&apos;], data2[&apos;x2&apos;], &apos;yo&apos;, label=&apos;y=0&apos;)</span><br><span class="line">    plt.xlim(-1, 1.5)</span><br><span class="line">    plt.ylim(-0.8, 1.2)</span><br><span class="line">    plt.xlabel(&apos;Microchip Test 1&apos;, fontsize=10)</span><br><span class="line">    plt.ylabel(&apos;Microchip Test 2&apos;, fontsize=10)</span><br><span class="line">    plt.title(&apos;lambda = 1&apos;,fontsize=15)</span><br><span class="line">    point_num=50</span><br><span class="line">    x1=np.linspace(-1,1.5,point_num)</span><br><span class="line">    x2=np.linspace(-1,1.5,point_num)</span><br><span class="line">    z=np.zeros((point_num,point_num))</span><br><span class="line">    for i in range(point_num):</span><br><span class="line">        for j in range(point_num):</span><br><span class="line">            z[i][j]=mapFeature(x1[i],x2[j],1).dot(theta)</span><br><span class="line">    plt.contour(x1,x2,z)</span><br><span class="line">    plt.legend(loc=&apos;upper right&apos;, fontsize=8)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://i.loli.net/2019/07/19/5d31ad261d6e455524.png" alt="Figure_4.png"></p>
<p>由于这里的参数&theta;以及学习率分别用梯度下降跑出来的和自己设置的，与用库函数跑出来的最优值不同，所以作出的效果与文档也有差别。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>正则化</title>
    <url>/2019/07/15/%E6%AD%A3%E5%88%99%E5%8C%96/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1051270933&amp;courseId=1004570029" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="过度拟合"><a href="#过度拟合" class="headerlink" title="过度拟合"></a>过度拟合</h4><ul>
<li>虽然拟合出来的曲线通过了所有的样本点，但弯弯曲曲不稳定，具有高方差。训练过度使得拟合曲线千方百计把样本分开，但应用到新的样本数据时效果很差。如果特征很多，但样本数很少就容易出现这种情况。</li>
<li>解决办法：<ul>
<li>减少特征数：人工选择特征/模式选择算法</li>
<li>正则化：减少数据规模或者说参数&theta;的大小    </li>
</ul>
</li>
</ul>
<h4 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h4><p>在之前的代价函数基础上在增加penalty，即加上关于&theta;的多项式: &lambda;/(2m)*m&Sigma;(&theta;<sub>j</sub>)<sup>2</sup>，叫做正则化项。而这个正则化项所乘的参数&lambda;叫做正则化参数。<br>得到了新的代价函数J，  这是正则化线形回归的优化目标<br>为什么要加上正则化项呢？我们的目标是尽量最小化代价函数J，这样会将&theta;一起减小，但拟合出来的曲线更符合要求。</p>
<h4 id="线形回归的正则化"><a href="#线形回归的正则化" class="headerlink" title="线形回归的正则化"></a>线形回归的正则化</h4><p>对于正则化梯度下降的&theta;更新问题：<br><img src="https://i.loli.net/2019/07/15/5d2c6ea06c51924638.png" alt="2019-07-15 20-12-27 的屏幕截图.png"><br>对于正则化的正规方程问题：<br><img src="https://i.loli.net/2019/07/15/5d2c6fa462f4c49815.png" alt="2019-07-15 20-20-42 的屏幕截图.png"><br>对于不可逆问题，在正则化中只要&lambda;&gt;0，那么括号里面的一定是可逆的。</p>
<h4 id="逻辑回归的正则化"><a href="#逻辑回归的正则化" class="headerlink" title="逻辑回归的正则化"></a>逻辑回归的正则化</h4><p>防止过拟合现象，代价函数加上正则化项。<br><img src="https://i.loli.net/2019/07/19/5d31282ca072b95835.png" alt="2019-07-19 10-16-53 的屏幕截图.png"><br>梯度下降：<br><img src="https://i.loli.net/2019/07/19/5d3128ff913fa70328.png" alt="2019-07-19 10-19-33 的屏幕截图.png"></p>
<p><img src="https://i.loli.net/2019/07/15/5d2c7a1746ed613920.png" alt="2019-07-15 21-05-17 的屏幕截图.png"></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Logistic 回归</title>
    <url>/2019/07/15/Logistic-%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1051024622&amp;courseId=1004570029" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="分类classification"><a href="#分类classification" class="headerlink" title="分类classification"></a>分类classification</h4><p>例子   </p>
<ul>
<li>垃圾邮件分类</li>
<li>肿瘤良性还是恶性</li>
</ul>
<p>在这类预测问题中y是一个离散的值，只有{0,1}两种取值。<br>如果用线形回归模型计算分类，得出的h<sub>&theta;</sub>(x)可能远大于1或远小于0，而我们希望得到的是0或者1。所以，提出逻辑回归模型，让0&lt;= h<sub>&theta;</sub>(x) &lt;=1。逻辑回归实际是一个分类算法。  </p>
<h4 id="假设陈述"><a href="#假设陈述" class="headerlink" title="假设陈述"></a>假设陈述</h4><p>目的：当有一个分类问题时，用哪种方程表示假设函数。<br>逻辑回归模型要使得0&lt;= h<sub>&theta;</sub>(x) &lt;=1。<br>那么，令：h<sub>&theta;</sub>(x)=g(&theta;<sup>T</sup>x)<br>而，g(z)=1/(1+e<sup>-z</sup>)<br>则，h<sub>&theta;</sub>(x)=1/(1+e<sup>-&theta;<sup>T</sup>x</sup>)    </p>
<p>假设函数hypothesis意义：估算对于输入的x，y=1的概率是多少    </p>
<h4 id="决策界线"><a href="#决策界线" class="headerlink" title="决策界线"></a>决策界线</h4><ul>
<li>所谓决策界线就是讲将数据集划分为几个部分，当&theta;参数确定时很容易计算出边界线。   </li>
<li>边界线可能是直线也可能是曲线甚至是圆</li>
<li>关于边界线的问题，我们知道边界线是将数据样本点分为几部分，比如一条直线分两部分，直线上方代表1，下方代表0，那么在G(&theta;X)中，只要&theta;X&gt;=0，这个sigma函数就接近1。故只需边界线上点为0即可绘制出边界线了。如下图<br><img src="https://i.loli.net/2019/07/16/5d2ddb835203245861.png" alt="2019-07-16 22-12-36 的屏幕截图.png"></li>
</ul>
<h4 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h4><ol>
<li>先来拟合logistic回归模型的参数&theta;，那么就需要定义用来拟合参数的优化目标或者叫代价函数。  </li>
<li>如果选用线形回归模型中的代价函数会出现什么问题，在线形回归模型中假设函数h<sub>&theta;</sub>(x)是线形函数，而逻辑归回归模型中假设函数是非线形的，带入线形回归模型中所做出来的J(&theta;)是非凸函数，有很多局部最优解，而非线回归模型中碗状的凸函数。  </li>
<li>有了上面理论，我们将线形回归模型中(h<sub>&theta;</sub>(x)-y)<sup>2</sup>记为Cost  </li>
<li><p>应用于逻辑回归模型的代价函数Cost(h<sub>&theta;</sub>(x),y)=  </p>
<ul>
<li>-log( h<sub>&theta;</sub>(x) ) ， if y=1</li>
<li>-log( 1-h<sub>&theta;</sub>(x) ) ，if y=0  </li>
</ul>
</li>
<li><p>如何理解代价函数：代价函数描述了预测结果与样本实际的偏离程度。当y=1，我们的hypothesis也为1时，Cost为0，这是对的（无偏差）这就说明预测对了；而如果预测结果趋于0，那么Cost趋向正无穷，说明与实际偏差很大。对于y=0的情况也是同理。</p>
</li>
</ol>
<h4 id="简化代价函数与梯度下降"><a href="#简化代价函数与梯度下降" class="headerlink" title="简化代价函数与梯度下降"></a>简化代价函数与梯度下降</h4><p>逻辑回归代价函数J(&theta;)：注意y只有两个值故容易合并<br>关于梯度下降推导：<a href="https://www.jianshu.com/p/610859e27f66" target="_blank" rel="noopener">公式推导</a><br><img src="https://i.loli.net/2019/07/15/5d2bed8559bf636025.png" alt="2019-07-15 11-05-31 的屏幕截图.png"><br><img src="https://i.loli.net/2019/07/15/5d2bef64f0ea844193.png" alt="2019-07-15 11-13-25 的屏幕截图.png">  </p>
<ol>
<li>为什么这样选择代价函数，事实上我们知道代价函数有很多，这个是根据统计学的极大似然估计法得出来的，这个函数有一个特点是它是<strong>凸</strong>的。  </li>
<li>将J(&theta;)简化之后我们就可以尝试利用<strong>梯度下降</strong>来最小化它并得到最小化时的参数&theta;，再用参数来做预测。  </li>
<li>梯度下降如下：<br><img src="https://i.loli.net/2019/07/15/5d2c1dee422d076645.png" alt="2019-07-15 14-31-50 的屏幕截图.png"></li>
<li>线形回归中的梯度下降同样可以应用到逻辑回归模型中，使得梯度下降收敛更快 </li>
</ol>
<h4 id="高级优化"><a href="#高级优化" class="headerlink" title="高级优化"></a>高级优化</h4><p>计算代价函数并不只有梯度下降算法。还有：</p>
<ul>
<li>L-BFGS </li>
<li>BFGS</li>
<li>Conjugate gradient <a href="https://baike.baidu.com/item/%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6%E6%B3%95/7139204?fromtitle=Conjugate%20Gradient&amp;fromid=11232557&amp;fr=aladdin" target="_blank" rel="noopener">共轭梯度法</a><br>这些算法都不需要手动选择学习率&alpha;，也都比梯度下降算法快。但要比梯度下降更复杂。  </li>
</ul>
<p>这些高级算法一般封装在某个库中，可以直接调用。</p>
<h4 id="多元分类：一对多"><a href="#多元分类：一对多" class="headerlink" title="多元分类：一对多"></a>多元分类：一对多</h4><p>即： 待分类的对象 对 多个类别<br>比如：邮件，类别有家人、工作、朋友等</p>
<p>大致是对不同的类别设置不同的分类器：选择其中一种作为正，其余都为0，那么就可以得出不同的分类器，我们将数据输入到不同的分类器中选择hypothesis最大的最可信的那个。<br><img src="https://i.loli.net/2019/07/15/5d2c269d5004814353.png" alt="2019-07-15 15-09-03 的屏幕截图.png"><br><img src="https://i.loli.net/2019/07/15/5d2c26ea8ca8062647.png" alt="2019-07-15 15-10-02 的屏幕截图.png"></p>
<hr>
<hr>
<center>补充</center>

<hr>
<ol>
<li><a href="https://blog.csdn.net/sinat_34328764/article/details/80246139" target="_blank" rel="noopener">Python Matplotlib 改变坐标轴的默认位置</a></li>
<li><a href="http://file.sh.peixun.net/file/201812/05/2018120522460ukuzqukq7.pdf" target="_blank" rel="noopener">课件</a></li>
</ol>
<h3 id="sklearn中的逻辑回归"><a href="#sklearn中的逻辑回归" class="headerlink" title="sklearn中的逻辑回归"></a><font size="5px" color="red">sklearn中的逻辑回归</font></h3><h4 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a><font size="3px" color="red">Sigmoid函数</font></h4><p><img src="https://img-blog.csdnimg.cn/20191223145742591.jpg?aHR0cHM6Ly9hdmF0YXIuY3Nkbi5uZXQvNy83L0IvMV9yYWxmX2h4MTYzY29tLmpwZw =250x150" alt="Sigmoid函数"></p>
<ul>
<li>绘制函数<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">x=np.linspace(-10,10,10000)</span><br><span class="line">ax = plt.gca()  # a表示axis</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;) # 隐藏右边的线</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;) # 隐藏上面的线</span><br><span class="line">ax.spines[&apos;bottom&apos;].set_position((&apos;data&apos;, 0)) # 设置纵轴起点</span><br><span class="line">ax.spines[&apos;left&apos;].set_position((&apos;data&apos;, 0)) # 设置横轴中点</span><br><span class="line">plt.plot(x,1/(1+np.power(np.e,-x)))</span><br><span class="line">plt.grid(True)</span><br><span class="line">plt.title(&apos;Figmoid Function&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20191223145855618.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="sigmoid 函数"><br><code>Sigmoid</code>函数是一个S型的函数，当自变量z趋近正无穷时，因变量g(z)趋近于1，而当z趋近负无穷时，g(z)趋近于0，<strong>它能够将任何实数映射到(0,1)区间</strong>，使其可用于将任意值函数转换为更适合二分类的函数。因为这个性质，<strong>Sigmoid函数也被当作是归一化的一种方法</strong>，与我们之前学过的MinMaxSclaer同理，是属于数据预处理中的“<strong>缩放</strong>”功能，可以将数据压缩到[0,1]之内。区别在于，MinMaxScaler归一化之后，是可以取到0和1的（最大值归一化后就是1，最小值归一化后就是0），但<strong>Sigmoid函数只是无限趋近于0和1</strong>。</p>
<h4 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a><font size="3px" color="red">逻辑回归</font></h4><ol>
<li>逻辑回归对<strong>线性关系</strong>的拟合效果好到丧心病狂，相对的，逻辑回归在<strong>非线性数据</strong>的效果很多时候比瞎猜还不如，所以如果你已经知道数据之间的联系是非线性的，千万不要迷信逻辑回归</li>
<li><strong>逻辑回归计算快</strong></li>
<li><strong>逻辑回归返回的分类结果不是固定的0，1，而是以小数形式呈现的类概率数字</strong></li>
</ol>
<h4 id="sklearn中的逻辑回归-1"><a href="#sklearn中的逻辑回归-1" class="headerlink" title="sklearn中的逻辑回归"></a><font size="3px" color="red">sklearn中的逻辑回归</font></h4><table>
<thead>
<tr>
<th>逻辑回归相关的类</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>linear_model.LogisticRegression</td>
<td>逻辑回归回归分类器（又叫logit回归，最大熵分类器）</td>
</tr>
<tr>
<td>linear_model.LogisticRegressionCV</td>
<td>带交叉验证的逻辑回归分类器</td>
</tr>
<tr>
<td>linear_model.logistic_regression_path</td>
<td>计算Logistic回归模型以获得正则化参数的列表</td>
</tr>
<tr>
<td>linear_model.SGDClassifier</td>
<td>利用梯度下降求解的线性分类器（SVM，逻辑回归等等）</td>
</tr>
<tr>
<td>linear_model.SGDRegressor</td>
<td>利用梯度下降最小化正则化后的损失函数的线性回归模型</td>
</tr>
<tr>
<td>metrics.log_loss</td>
<td>对数损失，又称逻辑损失或交叉熵损失</td>
</tr>
<tr>
<td><strong>其他会涉及的类</strong></td>
<td><strong>说明</strong></td>
</tr>
<tr>
<td>metrics.confusion_matrix</td>
<td>混淆矩阵，模型评估指标之一</td>
</tr>
<tr>
<td>metrics.roc_auc_score</td>
<td>ROC曲线，模型评估指标之一</td>
</tr>
<tr>
<td><strong>metrics.accuracy_score</strong></td>
<td>精确性，模型评估指标之一</td>
</tr>
</tbody>
</table>
<h4 id="linear-model-LogisticRegression"><a href="#linear-model-LogisticRegression" class="headerlink" title="linear_model.LogisticRegression"></a><font size="3px" color="red">linear_model.LogisticRegression</font></h4><ul>
<li><code>sklearn.linear_model.LogisticRegression (penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1,   class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None)</code></li>
</ul>
<h4 id="重要参数-penalty-amp-C"><a href="#重要参数-penalty-amp-C" class="headerlink" title="重要参数 penalty &amp; C"></a><font size="3px" color="red">重要参数 penalty &amp; C</font></h4><ul>
<li><code>penalty</code>和<code>C</code>都是正则化参数，<strong>正则化</strong>是用来防止模型过拟合的过程。常用的有L1正则化和L2正则化两种选项，分别通过在损失函数后加上参数向量&theta;的L1范式和L2范式的倍数来实现。  </li>
<li>这个增加的范式，被称为“<strong>正则项</strong>”，也被称为”惩罚项”。其中L1范数表现为参数向量中的每个参数的绝对值之和，L2范数表现为参数向量中的每个参数的平方和的开方值。</li>
<li>损失函数<br><img src="https://img-blog.csdnimg.cn/20191223152842631.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="逻辑回归损失函数"><br>C是用来控制正则化程度的超参数，n是方程中特征的总数，也是方程中参数的总数，j代表每个参数。在这里，J要大于等于1，是因为我们的参数向量&theta;中，第一个参数是&theta;<sub>0</sub>是我们的截距，它通常是不参与正则化的<br><img src="https://img-blog.csdnimg.cn/20191223153110591.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="参数说明"></li>
</ul>
<h4 id="学习曲线选择C"><a href="#学习曲线选择C" class="headerlink" title="学习曲线选择C"></a><font size="3px" color="red">学习曲线选择C</font></h4><ul>
<li>逻辑回归的重要属性<code>coef_</code>，查看每个特征所对应的参数<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.linear_model import LogisticRegression as RL</span><br><span class="line">from sklearn.datasets import load_breast_cancer</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.metrics import accuracy_score</span><br><span class="line"></span><br><span class="line">data=load_breast_cancer()</span><br><span class="line">X=data.data</span><br><span class="line">Y=data.target</span><br><span class="line">print(X.shape,Y.shape) # (569, 30) (569,)</span><br><span class="line"></span><br><span class="line">RL_L1=RL(penalty=&apos;l1&apos;,C=0.5,max_iter=1000) # 实例化并初始化参数</span><br><span class="line">RL_L2=RL(penalty=&apos;l2&apos;,C=0.5,max_iter=1000) # 实例化并初始化参数</span><br><span class="line"></span><br><span class="line">RL_L1=RL_L1.fit(X,Y) # 训练模型</span><br><span class="line">RL_L2=RL_L2.fit(X,Y)</span><br><span class="line"></span><br><span class="line">print(RL_L1.coef_)</span><br><span class="line">print(RL_L2.coef_.shape) # (1, 30)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>可以看见，当选择L1正则化的时候，许多特征的参数都被设置为了0，这些特征在真正建模的时候，就不会出现在我们的模型当中了，而L2正则化则是对所有的特征都给出了参数。</p>
<ul>
<li>分别实例化L1和L2两种正则化方法并进行绘制学习曲线选择最佳C值<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.linear_model import LogisticRegression as RL</span><br><span class="line">from sklearn.datasets import load_breast_cancer</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.metrics import accuracy_score</span><br><span class="line"></span><br><span class="line">data=load_breast_cancer()</span><br><span class="line">X_train,X_test,y_train,y_test=train_test_split(data.data,data.target,test_size=0.3,random_state=10) # 30%数据作为测试集，其余作为训练集</span><br><span class="line"></span><br><span class="line">l1=[]</span><br><span class="line">l2=[]</span><br><span class="line">l1_test=[]</span><br><span class="line">l2_test=[]</span><br><span class="line">for i in np.linspace(0.05,1,19):  # C的可选值</span><br><span class="line">    RL_l1=RL(penalty=&apos;l1&apos;,C=i,max_iter=1000)</span><br><span class="line">    RL_l2=RL(penalty=&apos;l2&apos;,C=i,max_iter=1000)</span><br><span class="line"></span><br><span class="line">    RL_l1=RL_l1.fit(X_train,y_train)</span><br><span class="line">    l1.append(accuracy_score(RL_l1.predict(X_train),y_train))</span><br><span class="line">    l1_test.append(accuracy_score(RL_l1.predict(X_test),y_test))</span><br><span class="line"></span><br><span class="line">    RL_l2 = RL_l2.fit(X_train, y_train)</span><br><span class="line">    l2.append(accuracy_score(RL_l2.predict(X_train), y_train))</span><br><span class="line">    l2_test.append(accuracy_score(RL_l2.predict(X_test), y_test))</span><br><span class="line"></span><br><span class="line">graph=[l1,l2,l1_test,l2_test]</span><br><span class="line">color = [&quot;green&quot;,&quot;black&quot;,&quot;lightgreen&quot;,&quot;gray&quot;]</span><br><span class="line">label = [&quot;l1&quot;,&quot;l2&quot;,&quot;l1test&quot;,&quot;l2test&quot;]</span><br><span class="line">for i in range(len(graph)):</span><br><span class="line">    plt.plot(np.linspace(0.05,1,19),graph[i],color[i],label=label[i])</span><br><span class="line"></span><br><span class="line">plt.legend(loc=&apos;best&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20191223160326919.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05ZSVNUX1RDX0xZUQ==,size_16,color_FFFFFF,t_70" alt="学习曲线"><br>可见，至少在我们的乳腺癌数据集下，两种正则化的结果区别不大。但随着C的逐渐变大，正则化的强度越来越小，模型在训练集和测试集上的表现都呈上升趋势，直到C=0.8左右，训练集上的表现依然在走高，但模型在未知数据集上的表现开始下跌，这时候就是出现了过拟合。我们可以认为，C设定为0.9会比较好。在实际使用时，基本就默认使用l2正则化，如果感觉到模型的效果不好，那就换L1试试看。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>2019.7.14 每周总结</title>
    <url>/2019/07/14/%E5%8D%97%E8%88%AA%E7%AC%AC%E4%B8%80%E5%91%A8%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;来南航整整一周了，周二开了第一次组会，每个人讲了自己的暑期计划，两个学长讲了论文，虽然啥也没听懂。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这周，算是从周一到周六，今天就不算入其中了，做了哪些事呢？</p>
<ol>
<li>周一学了Numpy库</li>
<li>周二上午开组会，开完我们几个预研一的去办了临时卡，终于可以不用借卡吃饭了。然后导师新买了几张办公桌也组装好了，吃完午饭我们去实验室商量好了位置，然后打扫好了下午就开始看<a href="https://sfz-lyq.cn/2019/07/11/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E6%AC%A1%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">吴恩达机器学习第一次作业线性回归</a>。</li>
<li>周三上午还是过来写单变量线形回归，还是有点欠缺，中午吃饭完过来调了几个bug好像就好了，然后赶紧写了笔记。</li>
<li>周四过来开始看学长给的文献，先看了<a href="https://www.zhihu.com/question/27375221" target="_blank" rel="noopener">知乎-如何看懂英文文献</a>，了解到摘要引言和conclusion比较重要，然后一上午只翻译了摘要；下午也是进展缓慢，看文献还是比较难受，有许多单词词组不认识，所以要查要理解。</li>
<li>打算用两天的时间看文献，所以过来还是看文献，但进展十分缓慢，比昨天好一点点，不太能安心看下去，看到实验室有人开小差自己也想开会小差。下午看的实在难受就刷了一个中矿大的校赛，比较水吧，做了几个算是练练代码了。</li>
<li>看了两天论文了，打算再用两天把多变量线形回归写完，结果看文档发现和单变量差别不大，很快就写出了代码。另外发现下载的资料文档真的很好用，晚上不想做什么就整理了一下博客，然后9点多去打球了，实验楼10点关门关门前几分钟回来然后回宿舍。</li>
<li>今天江宁校区停水停电，所以我们出去玩了，结果奔波了一天啥也没怎么玩着，体验感极差，难受的一批。</li>
<li>关于其他的，Emmm，也刷了CF，很简单的题自己竟然做不出来就想不到，感觉自己智商弱爆了。</li>
<li>锻炼的话，打了两次球，打算等会再去。每次从实验楼出来都10点多了就不想去跑步了。。。</li>
</ol>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下周再把逻辑回归学学，看看实现逻辑回归。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;还有那篇论文，务必看完哪怕再用两天时间，刚开始第一篇论文所以我对自己要求不是很高，但至少也不能太低。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 关于三维图和等高线我还不会做，所以把matplotlib作图巩固。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;刷题继续刷，学长都在刷力扣，凭什么你要落下。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;锻炼继续炼，该跑跑，该打球打球。</p>
]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title>文献阅读一</title>
    <url>/2019/07/11/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%E4%B8%80/</url>
    <content><![CDATA[<h3 id="Visual-Object-Tracking-for-Unmanned-Aerial-Vehicles"><a href="#Visual-Object-Tracking-for-Unmanned-Aerial-Vehicles" class="headerlink" title="Visual Object Tracking for Unmanned Aerial Vehicles:"></a><center>Visual Object Tracking for Unmanned Aerial Vehicles:</center></h3><h3 id="A-Benchmark-and-New-Motion-Models"><a href="#A-Benchmark-and-New-Motion-Models" class="headerlink" title="A Benchmark and New Motion Models"></a><center>A Benchmark and New Motion Models</center></h3><center><a href="http://lisiyi.me/paper/AAAI17_UAV.pdf" target="_blank" rel="noopener">原文地址</a></center><br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMTEvNWQyNjkwYmUxYTgyMTk3MTQwLnBuZw" alt="14338-66776-1-PB (1)-0.png"><br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMTEvNWQyNjkwYmZjN2RjOTI5MDc2LnBuZw" alt="14338-66776-1-PB (1)-1.png"><br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMTEvNWQyNjkwYmUxYjM2MzYzMjg3LnBuZw" alt="14338-66776-1-PB (1)-2.png"><br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMTEvNWQyNjkwYmU0YmNhYzE2NjA5LnBuZw" alt="14338-66776-1-PB (1)-3.png"><br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMTEvNWQyNjkwYmU2NGRkNzI4NjgzLnBuZw" alt="14338-66776-1-PB (1)-4.png"><br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMTEvNWQyNjkwYzA0ZGNjZTgzNjkxLnBuZw" alt="14338-66776-1-PB (1)-5.png"><br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMTEvNWQyNjkwYmU0YzRlZDI0OTk4LnBuZw" alt="14338-66776-1-PB (1)-6.png"><br><br>### <center>无人机视觉目标追踪：</center><br>###  <center>一个基准和新的运动模型</center><br>##### <center>作者：Siyi Li, Dit-Yan Yeung</center><br>##### <center>单位：香港科技大学计算机科学与工程学系</center><br>##### <center>邮箱：<a href="mailto:sliay@cse.ust.hk" target="_blank" rel="noopener">sliay@cse.ust.hk</a>,<a href="mailto:dyyeung@cse.ust.hk" target="_blank" rel="noopener">dyyeung@cse.ust.hk</a></center>

<h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>尽管最近视觉追踪<strong>领域</strong>有许多新的进展，但到目前为止大多数研究都聚焦在观测模型上。对于追踪系统的另一个重要的组成部分运动模型尤其是一些极端情况却很少有人涉足。本文，我们用组装在无人机或飞行器上的摄像头来考虑这样一种极端情景。我们建立了一个高度多样化的基准数据集，包含了由无人机摄像头捕捉到的70段视频影像。为了解决摄像头剧烈运动这样一个具有挑战性的问题，我们通过基于背景特征点的几何变换设计了一个简单的基线来将摄像头的运动模型化。最近最先进的追踪器为的广泛对比与这些追踪器在我们的无人机追踪数据集上的运动模型变化同时证实了数据集的必要性和所提出方法的有效性。我们做这个工作的目的是为在无人机追踪领域进一步的研究筑好基础。</p>
<h4 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h4><ul>
<li>视觉追踪是与许多现实应用<strong>有关</strong>的一个基础问题，这些应用包括视频监控，自动驾驶，人机交互以及更多。给出视频帧中目标物体的初始状态（例如位置和尺寸），追踪的目标是自动评估移动物体在后续帧中的状态。虽然视觉追踪已经被研究了几十年了，但由于大量的因素例如<strong>部分闭塞</strong>，物体快而突然的移动，照明变化，还有视角和姿势的大<strong>差异</strong>等等而遗留了一个具有挑战性的问题。   </li>
<li>最近几年我们见证了新型机器人，<strong>无人飞行器</strong>或无人机的进展。尽管在过去无人机大多被运用在军事应用上，但最近商业无人机革命也见证了越来越多的致力于研发小型、可购买的人性化的无人机的研究室。商业无人机的迅速发展可能会民间应用上有重大的影响，包括运输、通信。同时，在这个平台上一些可预见的应用将需要视觉追踪作为一个可行的核心科技。<strong>举几个例子</strong>，视觉追踪会对追踪动物、找人和监控实时交通状况等有用。</li>
<li>本文，我们在无人机平台上研究视觉追踪。除了一般的研究视觉追踪的共同问题，我们还要面对一个新的挑战即当使用无人机捕捉视频时频繁遇到摄像头突然移动。特别地，一个小的干扰例如摄像头的轻微旋转经常导致目标位置在图像场景中大的移位。同时，当无人机起飞时，它的运动通常会比许多传统的追踪应用有更高的<strong>自由度</strong>。因此，需要一个更复杂的运动模型。结果就是传统的运动模型对于带有固定的低速摄像机的追踪应用就不再适用了。本文的一个关注点是进行一个基准评估并且提出基线算法来明确估算<strong>自主运动</strong>。</li>
<li>本文的目标有三点：1. 用统计学的详细分析构造一个统一的无人机追踪基准数据集；2. 设计通用的基线算法来估算摄像机运动并将它们整合到不同的追踪系统中；3. 为了在视觉追踪领域开放一个新的研究方向这个目的，进行大量的实验对比并为视觉追踪模型提供基本见解。</li>
</ul>
<h4 id="Camera-Model"><a href="#Camera-Model" class="headerlink" title="Camera Model"></a>Camera Model</h4><ul>
<li>一款摄像机型描述了3D世界与2D平面图的一个映射。大量关于多视图几何学的摄像机型号已经被仔细地研究过了（Hartley and Zisserman 2003)。最广泛使用的一款是通用针孔摄像机型。Hoiem, Efros, and Hebert 2008 提出了一种简化的摄像机模型，假设所关注到的所有物体都停留在地面上。这个简化的摄像机模型已经被用来追踪地面物体例如汽车和行人（(Choi,Pantofaru, and Savarese 2013）。然而，所有的摄像机模型需要摄像机的初始化信息来推断物体的3维定位和校准摄像头。不幸的是，这些信息在许多物体追踪应用中并不容易获得。</li>
<li>在这里我们通过在2D图像平面中直接参数化摄像机来采用一种不同方法。我们注意到由于无人机上的摄像头通常离目标很远，我们可能会简单地忽略掉任何目标和背景线索之间深度的不同，并因此假设所捕捉到的框架可以被看成不同的平面目标。然后，从双视图几何的角度来看，这些平面通过射影变换联系在一起，射影变换也被称为二维同形。在数学上，让g<sub>t</sub>和g<sub>t-1</sub>分别代表t和t-1帧中静态特征点的齐次坐标。接下来我们就可以通过变化矩阵H来将相机模型参数化了。请注意因为我们介绍相机模型的主要目的是大概地为追踪指导一下搜索区域而不是决定精确的位置，上面的同形近似值在实际中效果不错。因此，我们只需要估算变换矩阵H。由于相机的初始信息不再需要，这个方法在其适用性上更为普遍。</li>
</ul>
<h4 id="Baseline-Method"><a href="#Baseline-Method" class="headerlink" title="Baseline Method"></a>Baseline Method</h4><ul>
<li>在传统的追踪方法上，只有目标移动被模型化。让z<sub>t</sub>和z<sub>t-1</sub>分别代表t和t-1帧中的目标坐标。运动模型就被简单地表示成：   <center> z<sub>t</sub>=z<sub>t-1</sub>+&Delta;z<sub>t</sub> </center><br>基于粒子过滤的方法采用高斯分布&Delta;z<sub>t</sub> 模型，而基于滑动窗的方法基于局部均匀分布的&Delta;z<sub>t</sub>模型。这个简单的移动模型在一般情况下效果不错。然而，在极端情况下例如无人机追踪，仅仅&Delta;z<sub>t</sub>模型是不够的。具体来说，假定一个小的&Delta;z<sub>t</sub>将会丢失下一个帧中的目标，而假定一个大的&Delta;z<sub>t</sub>将会增加漂移的风险。  </li>
<li>基于上面的相机假设，我们可以将新的运动模型表示成相机投影和目标运动的结合：<br><center> z<sub>t</sub>=H<sub>t</sub>z<sub>t-1</sub>+&Delta;z<sub>t</sub> </center><br>H<sub>t</sub>代表相机移动，&Delta;z<sub>t</sub>是由于目标移动而引起的位置替换。一旦我们对相机移动H<sub>t</sub>有了一个合理的评估，那么目标移动替换在局部地区也可以更精确地估算。</li>
<li>注意这个基线算法可以更轻易地被纳入到已有地追踪方法中。具体来说，我们首先通过特征点匹配来估算同形图H<sub>t</sub>（Fischler and Bolles 1981）。然后，之前的目标位置估算通过H<sub>t</sub>投影到当前地图像平面。对于基于滑动窗地追踪器，以转换后的目标坐标为中心地局部区域将会被搜索。对于基于粒子过滤器的追踪器，所有被维护的候选样本将会被转换到当前的图像平面中。除了这些修改，每一个追踪器还是以同样的方式工作。</li>
</ul>
<h4 id="Conclusion-and-Future-Work"><a href="#Conclusion-and-Future-Work" class="headerlink" title="Conclusion and Future Work"></a>Conclusion and Future Work</h4><ul>
<li>本文探讨了在无人机平台上进行视觉追踪的潜力。我们提出了一个统一的无人机追踪基准，它包含了由无人机摄像头捕捉到了一些影像。为了解决摄像头突然移动这个有挑战性的问题，我们通过基于背景特征线索的投影变换设计了简单基线来将相机移动模型化。我们将最先进的追踪器以及它们在无人机追踪基准上的移动模型变化进行了大量的对比。结果表明，通过明确地模型化相机移动，追踪器可以在所提出地移动模型下实现巨大的性能提升。</li>
<li>虽然我们提出的基线方法是有效的，但也确实存在一些失败的例子。例如，相机估算是基于传统的低级特征点检测，而低级特征点检测在某些情况下是嘈杂的甚至是错误的。如何设计卷积神经网络在视频数据上来学习更多的相机模型是一个有趣的问题。目前的基线方法中，相机估算是以脱机方式工作的。将相机估算和目标追踪结合在一个连贯的学习框架中希望可以有用。在未来工作中我们将致力于研究这些方面。</li>
</ul>
<p>重点部分完结，撒花～～～</p>
]]></content>
      <categories>
        <category>文献</category>
      </categories>
      <tags>
        <tag>文献</tag>
        <tag>翻译</tag>
      </tags>
  </entry>
  <entry>
    <title>吴恩达机器学习第一次编程作业-线性回归</title>
    <url>/2019/07/11/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E6%AC%A1%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h3 id="课时45-编程作业：线性回归"><a href="#课时45-编程作业：线性回归" class="headerlink" title="课时45 编程作业：线性回归"></a><center><a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/text?lessonId=1053430047&amp;courseId=1004570029" target="_blank" rel="noopener">课时45 编程作业：线性回归</a></center></h3><p>原文档使用Octave/MATLAB作为官方教程，这里使用python实现线性回归</p>
<hr>
<p>资料：<a href="https://sfz-lyq.cn/2019/06/05/%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E5%BD%A2%E5%9B%9E%E5%BD%92/">单变量线形回归-笔记</a> 、<a href="https://sfz-lyq.cn/2019/06/24/%E5%A4%9A%E5%8F%98%E9%87%8F%E7%BA%BF%E5%BD%A2%E5%9B%9E%E5%BD%92/#more">多变量线性回归-笔记</a></p>
<hr>
<p>工具：pycharm2018.3.3</p>
<hr>
<h4 id="第一节-引言"><a href="#第一节-引言" class="headerlink" title="第一节 引言"></a>第一节 引言</h4><h4 id="第二节-单变量线性回归"><a href="#第二节-单变量线性回归" class="headerlink" title="第二节 单变量线性回归"></a>第二节 单变量线性回归</h4><p><a href="https://paste.ubuntu.com/p/DbxXWKHGMg/" target="_blank" rel="noopener">完整代码1</a><br><a href="https://paste.ubuntu.com/p/B7fHVxSbVh/" target="_blank" rel="noopener">完整代码2</a><br><a href="https://blog.csdn.net/weixin_42415485/article/details/81285615" target="_blank" rel="noopener">参考1</a></p>
<h5 id="2-0-导入包"><a href="#2-0-导入包" class="headerlink" title="2.0 导入包"></a>2.0 导入包</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import matplotlib</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">matplotlib.use(&apos;TkAgg&apos;)</span><br></pre></td></tr></table></figure>
<h5 id="2-1-Plotting-the-Data-绘制数据图"><a href="#2-1-Plotting-the-Data-绘制数据图" class="headerlink" title="2.1 Plotting the Data 绘制数据图"></a>2.1 Plotting the Data 绘制数据图</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def get_data():   # 读取数据，返回两个向量</span><br><span class="line">	population_profit = np.genfromtxt(&quot;exercise/ex1/ex1data1.txt&quot;, delimiter=&apos;,&apos;, dtype=float)</span><br><span class="line">	# print(population_profit.shape)</span><br><span class="line">	population = np.array(population_profit[:, 0])   # 以向量形式获取第一列</span><br><span class="line">	profit = np.array(population_profit[:, 1])  # 同上</span><br><span class="line">	return population,profit    # 以向量形式返回横轴数据和纵轴数据</span><br><span class="line"></span><br><span class="line">def draw_picture(x,y):  # 传入横轴和对应纵轴数据</span><br><span class="line">	population,profit=x,y</span><br><span class="line">	plt.xlim(4,24)    # plt.xlim(4,24,2) 表示以2为刻度单位显示横轴坐标范围为[4,24]</span><br><span class="line">	plt.ylim(-5,25)</span><br><span class="line">	plt.plot(population,profit,&apos;xr&apos;)  # ‘x’为样式，‘r’为颜色，可自主调整</span><br><span class="line">	plt.xlabel(&apos;Population of City in 10,000s&apos;)  # 添加横轴标签</span><br><span class="line">	plt.ylabel(&apos;Profit in $10,000s&apos;)</span><br><span class="line">	plt.title(&apos;one variable for the profits&apos;,fontsize=20)   # 设置图片标题</span><br><span class="line">	plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMDkvNWQyNDRlOWJlZjE0Nzg5MDcyLnBuZw" alt="14.png"></p>
<h5 id="2-2-Gradient-Descent-梯度下降"><a href="#2-2-Gradient-Descent-梯度下降" class="headerlink" title="2.2 Gradient Descent 梯度下降"></a>2.2 Gradient Descent 梯度下降</h5><ul>
<li>概念：<a href="https://sfz-lyq.cn/2019/06/05/%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E5%BD%A2%E5%9B%9E%E5%BD%92/">单变量线形回归</a></li>
<li>需要注意的是&theta;需要<strong>同时</strong>更新 - batch gradient descent algorith 批量梯度下降算法</li>
<li>核心在于计算代价函数CostFunction和theta</li>
<li>用python实现时最好把进行运算的对象都转化为相同类型</li>
<li>关于梯度下降同时求&theta;操作理解补充，请看图：<a href="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMTMvNWQyOTRkNDQzZmEwMjQ5MzQ1LnBuZw" target="_blank" rel="noopener">梯度下降.png</a>，这样就解释了为什么代码中每个训练样本x都要与hypothesis相乘再相加，</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def computer_cost(x,y,theta,m):   # 计算平方误差代价函数,x，y为横轴纵轴数据，m为训练样本数</span><br><span class="line">	# print(x.shape,theta.shape,y.shape)</span><br><span class="line">	hypothesis = np.dot(x, theta)   # 假设函数</span><br><span class="line">	# print(hypothesis.shape,y.shape)</span><br><span class="line">	# print((hypothesis-y).shape)</span><br><span class="line">	return  np.sum(((hypothesis-y)**2))/(2.0*m)   # 代价函数 J</span><br><span class="line"></span><br><span class="line">def Gradient_Descent(x,y,m,theta,alpha,iterations):</span><br><span class="line">	cost=np.zeros(iterations+1)  # 将初始值也算进去然后迭代iterations次</span><br><span class="line">	cost[0]=computer_cost(x,y,theta,m)  # 代价函数初始值</span><br><span class="line">	each_theta=np.zeros((2,iterations+1))</span><br><span class="line">	each_theta[0][0],each_theta[1][0]=theta[0][0],theta[1][0]</span><br><span class="line">	for iteration in range(1,iterations+1):</span><br><span class="line">		theta=theta-(alpha/m)*(x.T.dot((x.dot(theta)-y)))</span><br><span class="line">		cost[iteration]=computer_cost(x,y,theta,m)  # 同步记录每一个值，然后作3D图或等高线</span><br><span class="line">		each_theta[0][iteration], each_theta[1][iteration] = theta[0][0], theta[1][0]</span><br><span class="line">		# print(&apos;第%d次迭代，&apos; %iteration,&apos;代价函数值为：&apos;,cost[iteration],&apos;此时theta为：&apos;,theta.ravel())</span><br><span class="line">	return cost,theta,each_theta   # 返回所有的代价函数和参数</span><br></pre></td></tr></table></figure>
<p>设置好初始&theta;、&alpha;、迭代次数iterations后调用即可，接口如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def One_Variable_Linear_Regression(theta,alpha,iterations):</span><br><span class="line">	x, y = get_data()  # 获取数据</span><br><span class="line">	m = x.size   # 训练样本数</span><br><span class="line">	temp = np.ones((m, 1))  # 生成一列1，作为X0</span><br><span class="line">	x = x.reshape(m, 1)  # 将向量变化成矩阵</span><br><span class="line">	y = y.reshape(m, 1)  # 为了避免出错，务必保持参与运算的所有对象类型一致，务必务必</span><br><span class="line">	x = np.hstack((temp, x))  # 制造X0为1</span><br><span class="line">	# print(&apos;初始代价函数值为：&apos;,computer_cost(x,y,theta,m))</span><br><span class="line">	cost,final_theta,each_theta=Gradient_Descent(x,y,m,theta,alpha,iterations)</span><br><span class="line">	predict_profit(final_theta)   # 根据最终的theta预测利润，代码见下</span><br><span class="line">	Adapting(x,y,final_theta)   # 进行拟合，代码见下</span><br></pre></td></tr></table></figure></p>
<p>得到最终的theta后就可以进行预测了和拟合了<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def predict_profit(theta):</span><br><span class="line">	# 绘制出cost函数图找到中心对应的theta即我们要找的参数</span><br><span class="line">	# 用得出的参数来计算hypothesis假设函数即预测，再绘制出来观察是否和源数据拟合</span><br><span class="line">	print(theta)</span><br><span class="line">	predict1=(np.array([1,3.5])*theta).sum()</span><br><span class="line">	predict2=(np.array([1,7])*theta).sum()</span><br><span class="line">	print(&apos;当人口数为3.5W时，利润为：&apos;,predict1*10000)</span><br><span class="line">	print(&apos;当人口数为7w时，利润为：&apos;,predict2*10000)</span><br><span class="line"></span><br><span class="line">def Adapting(x,y,theta): # 拟合</span><br><span class="line">	hypothesis=x.dot(theta)    # 预测值</span><br><span class="line">	plt.xticks(np.arange(4,24,2))  # 设置横轴坐标范围和刻度</span><br><span class="line">	plt.yticks(np.arange(-5,25,5))</span><br><span class="line">	plt.xlabel(&apos;Population of City in 10,000s&apos;,fontsize=7)   # 设置横轴标签</span><br><span class="line">	plt.ylabel(&apos;Profit in $10,000s&apos;,fontsize=7)</span><br><span class="line">	plt.title(&apos;Training data with linear regression fit&apos;,fontsize=15)</span><br><span class="line">	plt.plot((x.T)[1],y,&apos;xr&apos;,label=&apos;Training data&apos;)</span><br><span class="line">	plt.plot((x.T)[1],hypothesis,label=&apos;Linear regression&apos;)</span><br><span class="line">	plt.legend(loc=&apos;lower right&apos;,fontsize=8)  # 线条含义说明</span><br><span class="line">	plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMTAvNWQyNWRiMjQ1MjljODU0Mzk0LnBuZw" alt="15.png"></p>
<h5 id="2-4-根据J-theta-绘制等高线和三维图"><a href="#2-4-根据J-theta-绘制等高线和三维图" class="headerlink" title="2.4 根据J(&theta;)绘制等高线和三维图"></a>2.4 根据J(&theta;)绘制等高线和三维图</h5><p>不会<br>占坑</p>
<h5 id="关于单变量线性回归的理解"><a href="#关于单变量线性回归的理解" class="headerlink" title="关于单变量线性回归的理解"></a>关于单变量线性回归的理解</h5><ol>
<li>含义：一个模型而已，目的是用来预测的。单变量指只有一个特征输入</li>
<li>用图片来讲，是让得出的预测模型跑出的数据和源数据更好的拟合如上图所示</li>
<li>那么为了这个目的提出了代价函数J（也叫损失函数）并给出了计算公式，表示模型和源数据的偏差程度，当然越小越好</li>
<li>这个计算公式是根据梯度下降算法得出的，根据偏导数更新。关于梯度下降有一篇比较容易理解的文章：<a href="https://blog.csdn.net/sword_csdn/article/details/77886858" target="_blank" rel="noopener">参考</a> ，里面说明了为什么梯度下降能使&theta;和代价函数最小化</li>
<li>梯度下降这里使用的是批量梯度下降，还有其他的方法，比如随机梯度下降以及….忘了，它有一个缺陷好像是只能达到局部最优，这个比较好理解，可能存在那么一个低谷，它的偏导数正好是0，那么就走不动了。</li>
<li>关于学习率：以前笔记写的是梯度下降每一步走的步长，这里说每一步是指一次迭代相当于走了一步，正常它是往下走的，为什么往下走上面那篇参考也给出了答案：在右边导数为正，那么减去正数，而在右边时导数为负，看似加上一个值，但初始&theta;设置的是0啊，所以是一个负数加上一个正数，还是往低谷走</li>
<li>为什么要绘制图形，等高线和三维图可以直观地反映出梯度下降的过程，所以作图能力Emmmmm</li>
<li>关于工具的使用，这个真的要熟悉语言特性，不然碰到很多很多问题</li>
</ol>
<h4 id="第三节-Linear-regression-with-multiple-variables多变量线性回归"><a href="#第三节-Linear-regression-with-multiple-variables多变量线性回归" class="headerlink" title="第三节 Linear regression with multiple variables多变量线性回归"></a>第三节 Linear regression with multiple variables多变量线性回归</h4><p><a href="https://paste.ubuntu.com/p/ZG6SpmTMj2/" target="_blank" rel="noopener">完整代码1</a></p>
<h5 id="3-1-特征缩放-标准化（Normalization）"><a href="#3-1-特征缩放-标准化（Normalization）" class="headerlink" title="3.1 特征缩放-标准化（Normalization）"></a>3.1 特征缩放-标准化（Normalization）</h5><ul>
<li>关于特征缩放：<a href="https://blog.csdn.net/cluster1893/article/details/80759738" target="_blank" rel="noopener">特征缩放</a></li>
<li>sklearn 库中有快速实现标准化的方法，但….窝不会这个库啊，所以还是用笨方法实现</li>
<li>特征缩放方法很多，这里是减去平均值再除以<a href="https://baike.baidu.com/item/%E6%A0%87%E5%87%86%E5%B7%AE/1415772?fr=aladdin" target="_blank" rel="noopener">标准差</a><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Feature_Normalization(x,m):  # 特征缩放，减去平均值再除以标准差</span><br><span class="line">	mean_value=x.sum(axis=0)/m  # 按列求和再取平均</span><br><span class="line">	std=math.sqrt(((x-mean_value)**2).sum()/m)  # 求标准差</span><br><span class="line">	x=(x-mean_value)/std   # 特征缩放</span><br><span class="line">	return x</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="3-2-梯度下降"><a href="#3-2-梯度下降" class="headerlink" title="3.2 梯度下降"></a>3.2 梯度下降</h5><ul>
<li>和单变量一样，只不过维数增加了，运算完全一样</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Computer_Cost(x,y,theta,m):</span><br><span class="line">	hypothesis=x.dot(theta) # 假设函数</span><br><span class="line">	return np.sum((hypothesis-y)**2)/2/m   # 代价函数</span><br><span class="line"></span><br><span class="line">def Gradient_Descent(x,y,theta,m,iterations,alpha):</span><br><span class="line">	cost=np.zeros(iterations+1)</span><br><span class="line">	cost[0]=Computer_Cost(x,y,theta,m)</span><br><span class="line">	for iteration in range(1,iterations+1):</span><br><span class="line">		theta=theta-np.dot(x.T,(alpha/m)*(x.dot(theta)-y))</span><br><span class="line">		cost[iteration]=Computer_Cost(x,y,theta,m)</span><br><span class="line">		# print(&apos;第%d次迭代，&apos;%iteration,&apos;代价函数为：&apos;,cost[iteration],&apos;此时theta为：&apos;,theta.ravel())</span><br><span class="line">	return cost,theta</span><br></pre></td></tr></table></figure>
<p>在函数中调用接口即可<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Draw_Iterations_Cost(iterations,y):  # 根据学习率绘制代价函数随迭代次数的变化</span><br><span class="line">	x=np.linspace(0,iterations,iterations)</span><br><span class="line">	plt.xlim(0,50)</span><br><span class="line">	plt.ylim(0,7e10)</span><br><span class="line">	plt.plot(x,y,&apos;y&apos;)</span><br><span class="line">	plt.xlabel(&apos;Number of iterations&apos;,fontsize=7)</span><br><span class="line">	plt.ylabel(&apos;Cost J&apos;,fontsize=7)</span><br><span class="line">	plt.title(&apos;Convergence of gradient descent&apos;,fontsize=13)</span><br><span class="line">	plt.show()</span><br><span class="line">	</span><br><span class="line">def Multiple_Variables_Linear_Regression(x,y,theta,m,iterations,alpha):</span><br><span class="line">	cost,final_theta=Gradient_Descent(x,y,theta,m,iterations,alpha)</span><br><span class="line">	Draw_Iterations_Cost(iterations+1,cost)  # 绘制代价函数收敛曲线</span><br></pre></td></tr></table></figure></p>
<p>获取数据与初始化</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Get_Data():</span><br><span class="line">	temp_data=np.genfromtxt(&apos;exercise/ex1/ex1data2.txt&apos;,delimiter=&apos;,&apos;,dtype=float)</span><br><span class="line">	X=temp_data[:,0:2].reshape(temp_data.shape[0],temp_data.shape[1]-1)  # 多特征</span><br><span class="line">	Y=temp_data[:,2].reshape(temp_data.shape[0],1)   # 输出只有一列</span><br><span class="line">	# print(X.shape,Y.shape)</span><br><span class="line">	return X,Y,X.shape[0]      # 返回X特征值，Y为输出，X.size为训练样本数</span><br><span class="line"></span><br><span class="line">x,y,m=Get_Data()</span><br><span class="line">x=Feature_Normalization(x,m)  # 特征缩放</span><br><span class="line">x=np.hstack((np.ones((m,1)),x))  # 添加 X0 = 1</span><br><span class="line">theta=np.zeros((x.shape[1],1))   # 设置初始theta</span><br><span class="line">iterations=100</span><br><span class="line">alpha=0.1  # 学习率 可尝试0.3，0.1，0.03，0.01，绘制出来的收敛曲线是不同的</span><br><span class="line">Multiple_Variables_Linear_Regression(x,y,theta,m,iterations,alpha)</span><br></pre></td></tr></table></figure>
<p>选取&alpha;=0.1绘制出来的收敛曲线如下：<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMTMvNWQyOWE5NTY3OTE5NjU2MTIxLnBuZw" alt="16.png"></p>
<p> TODO 绘制数据三维图与等高线</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>NumPy库学习</title>
    <url>/2019/07/11/NumPy%E5%BA%93%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h3 id="Python库之Numpy"><a href="#Python库之Numpy" class="headerlink" title="Python库之Numpy"></a><center>Python库之<a href="https://www.bilibili.com/video/av44798895" target="_blank" rel="noopener">Numpy</a></center></h3><p><a href="http://www.numpy.org/" target="_blank" rel="noopener">NumPy简介</a></p>
<hr>
<p><a href="https://www.numpy.org/devdocs/user/quickstart.html" target="_blank" rel="noopener">Numpy官方教程</a></p>
<hr>
<p><a href="https://www.runoob.com/numpy/numpy-tutorial.html" target="_blank" rel="noopener">NumPy菜鸟教程</a></p>
<hr>
<p><a href="https://www.bilibili.com/video/av44798895" target="_blank" rel="noopener">学习地址</a> </p>
<hr>
<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><h4 id="Python常用工具库"><a href="#Python常用工具库" class="headerlink" title="Python常用工具库"></a>Python常用工具库</h4><ul>
<li>Numpy</li>
<li>Pandas</li>
<li>Matplotlib</li>
<li>Scikit-learn</li>
<li>tensorflow</li>
</ul>
<h4 id="关于机器学习"><a href="#关于机器学习" class="headerlink" title="关于机器学习"></a>关于机器学习</h4><ul>
<li>流程：数据搜集与预处理（得到训练样本）-&gt;特征选择与模型构建（特征抽取-&gt;学习函数）-&gt; 评估与预测</li>
<li>机器学习本质：数学原理推导与实际应用技巧 </li>
<li>关于数学问题：边学边查，哪里不会查哪里即可</li>
</ul>
<h4 id="关于深度学习"><a href="#关于深度学习" class="headerlink" title="关于深度学习"></a>关于深度学习</h4><ul>
<li>是什么：机器学习中神经网络算法的延伸，应用较广</li>
<li>用处：深度学习在计算机视觉和自然语言处理中更厉害一些</li>
<li>which first？一切基础都是机器学习，打好坚实的基础</li>
</ul>
<h3 id="科学计算库numpy"><a href="#科学计算库numpy" class="headerlink" title="科学计算库numpy"></a>科学计算库numpy</h3><ul>
<li>IDE：pycharm</li>
<li>导入库：<code>import numpy as np</code></li>
</ul>
<h4 id="函数1-：numpy-genfromtxt-“file-name”-delimiter-’-’-dtype-str"><a href="#函数1-：numpy-genfromtxt-“file-name”-delimiter-’-’-dtype-str" class="headerlink" title="函数1 ：numpy.genfromtxt(“file_name”,delimiter=’,’,dtype=str)"></a>函数1 ：numpy.genfromtxt(“file_name”,delimiter=’,’,dtype=str)</h4><ul>
<li>用numpy打开数据，返回一个数据矩阵</li>
<li>为了方便直接在工作目录下打开文件名，当然也可以指定目录和文件名和保存图片一样</li>
<li>delimiter为指定的分隔符，会将数据按指定的分隔符分开</li>
<li>dtype为指定的数据类型，分割后生成的数据类型</li>
<li>实用方法：<code>print(help(numpy.genfromtxt))</code>，查看API手册</li>
</ul>
<h4 id="函数2：-numpy-array"><a href="#函数2：-numpy-array" class="headerlink" title="函数2： numpy.array([])"></a>函数2： numpy.array([])</h4><ul>
<li>括号里面传入list类型，自动生成对应数组，例如：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">np.array([1,2,3,4,5])  # 生成一维数组[1 2 3 4 5]</span><br><span class="line">np.array([ [1,2,3], [4,5,6], [7,8,9] ])  #生成二维数组[ [1 2 3] [4 5 6] [7 8 9] ]</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="函数3：object-shape"><a href="#函数3：object-shape" class="headerlink" title="函数3：object.shape"></a>函数3：object.shape</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vector=np.array([1,2,3,4])</span><br><span class="line">print(vector.shape)    #  输出:(4,) ，表示一维数组4个元素</span><br><span class="line">matrix=np.array([ [1,2,3], [4,5,6], [7,8,9] ])</span><br><span class="line">print(matrix.shape)  # 输出:(3,3)，表示3行3列</span><br></pre></td></tr></table></figure>
<h4 id="函数4：object-dtype"><a href="#函数4：object-dtype" class="headerlink" title="函数4：object.dtype"></a>函数4：object.dtype</h4><ul>
<li>Numpy array中的每一个元素类型必须想相同，当读入数据时Numpy会自动分清数据类型<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">numbers=np.array([1,2,3,4])</span><br><span class="line">print(numbers.dtype)   #  输出: int32</span><br><span class="line">numbers=np.array([1,2,3,4.0])</span><br><span class="line">print(numbers.dtype)  #输出: float64</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="数组取值"><a href="#数组取值" class="headerlink" title="数组取值"></a>数组取值</h4><ul>
<li>一维数组：object[x:y]表示取出下标从x到y-1的数生成数组，取单个数传入下标即可  </li>
<li>二维数组：object[x,y]或者object[x][y]即可，注意下标从0开始</li>
<li>二维数组取某个子矩阵的值：object[x1:x2,y1:y2]，下标从0开始到上限减一，返回的还是矩阵，可用.shape查看</li>
<li>获得第二行或列所有的元素：<code>A[2,:]、A[:,2]</code>，冒号<code>:</code>表示该行或该列的所有元素，返回的是个向量，类型是ndarray</li>
<li>基于上条，那么就可以给整行或整列赋值了：<code>A[:,2]=[1,2,3]</code>，假如A一个3*3的矩阵，这条语句就是将第三列改为[1 2 3]</li>
</ul>
<h4 id="矩阵比较"><a href="#矩阵比较" class="headerlink" title="矩阵比较"></a>矩阵比较</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">a=np.array([[1,2,3],[4,5,6],[7,8,9]])</span><br><span class="line">print(a==5)   # 输出一个和a同型的boolean类型矩阵</span><br><span class="line">judge=(a==5)</span><br><span class="line">print(a[judge]) # 输出：[5]</span><br></pre></td></tr></table></figure>
<h4 id="更改数据类型object-astype"><a href="#更改数据类型object-astype" class="headerlink" title="更改数据类型object.astype()"></a>更改数据类型object.astype()</h4><ul>
<li>比如object是int32类型的，可以使用object.astype(float)转换成float64类型的</li>
</ul>
<h4 id="Numpy常用函数"><a href="#Numpy常用函数" class="headerlink" title="Numpy常用函数"></a>Numpy常用函数</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">a=np.arange(5)</span><br><span class="line">print(a)  # 输出：[0 1 2 3 4]</span><br><span class="line">a=np.arange(15).reshape(3,5)</span><br><span class="line">print(a)  # 输出3行5列的二维矩阵</span><br><span class="line">print(a.shape)  # 输出：(3, 5)</span><br><span class="line">print(a.ndim)  # 输出：2 ，表示数组的维数</span><br><span class="line">print(a.dtype)  # 输出：int32，表示矩阵的数据类型</span><br><span class="line">print(a.size)  # 输出：15，表示数组的元素个数</span><br><span class="line">print(np.zeros((3,4)))  # 生成3行4列的0矩阵</span><br><span class="line">print(np.ones((2,3,4),dtype=np.int32))  # 生成3维矩阵，元素全为1，且数据类型为int32</span><br><span class="line">print(np.full((2,3),2))  # 生成2行3列的矩阵并用2填充整个矩阵</span><br><span class="line">print(np.arange(10,30,5)) # 生成一个10到30之间的等差数列，首项为10，公差为5</span><br><span class="line">print(np.random.random((2,3)))  # 生成一个2行3列的随机矩阵，元素范围在-1到+1之间</span><br><span class="line">print(np.linspace(0,2*pi,10)) # 生成10个数的等差数列，首项是0，末项是2*pi</span><br></pre></td></tr></table></figure>
<h4 id="矩阵运算"><a href="#矩阵运算" class="headerlink" title="矩阵运算"></a>矩阵运算</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from numpy import pi</span><br><span class="line">a=np.array([20,30,40,50])</span><br><span class="line">b=np.arange(1,5)   # 生成 [1,2,3,4]</span><br><span class="line">print(a-b)  # 输出： [19,28,37,46]，同型矩阵对应位置相加减</span><br><span class="line">print(b**2)  # 输出：[1,3,9,16]，同型矩阵对应位置相乘</span><br><span class="line">print(a&lt;35)  # 输出：[ True  True False False]</span><br><span class="line">print(a*b)  # 输出：[20 60 120 200]，同型矩阵对应位置相乘</span><br><span class="line"></span><br><span class="line">a=np.eye(2)</span><br><span class="line">print(a)  # 生成一个2维的单位矩阵,元素类型为float64</span><br><span class="line">print(a.dtype)   # 输出：float64</span><br><span class="line">a=a.astype(np.int32)  # 转为int32类型</span><br><span class="line">print(a.dtype)  # 输出：int32</span><br><span class="line">b=np.array([[1,2],[3,4]])</span><br><span class="line">print(a*b)  # 对应位置相乘</span><br><span class="line">print(&quot;--------&quot;)</span><br><span class="line">print(a.dot(b))  # 矩阵相乘，要求前一个矩阵的列数等于后一个矩阵的行数，但向量除外(支持矢量内积)</span><br><span class="line">print(&quot;--------&quot;)</span><br><span class="line">print(np.dot(a,b))  # 同上矩阵相乘，要求前一个矩阵的列数等于后一个矩阵的行数</span><br></pre></td></tr></table></figure>
<h4 id="矩阵常用操作"><a href="#矩阵常用操作" class="headerlink" title="矩阵常用操作"></a>矩阵常用操作</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from numpy import pi</span><br><span class="line">np.random.seed(0)</span><br><span class="line">a=np.arange(3)</span><br><span class="line">print(a)</span><br><span class="line">print(np.exp(a))  # 分别计算e的幂，以a中的元素为指数</span><br><span class="line">print(np.sqrt(a)) # 计算a中每个元素的开根</span><br><span class="line">a=np.floor(np.random.random((3,4))*10)  # 生成3行4列的随机矩阵，整体放大10倍后向下取整</span><br><span class="line">print(a)</span><br><span class="line">print(&apos;-----------&apos;)</span><br><span class="line">print(a.ravel()) # 把原矩阵拉成一个一维向量</span><br><span class="line">a.shape=(2,6)   # 重塑矩阵</span><br><span class="line">print(a)  # 生成2行6列的二维矩阵，元素个数一定要和原矩阵相同；指定一个维数时另一个就自动确定了</span><br><span class="line">print(&apos;---\n&apos;,a.T)  # 矩阵转置</span><br><span class="line">b=np.floor(np.random.random((2,6))*10)  # 生成2行6列的随机矩阵</span><br><span class="line">print(np.hstack((a,b)))  # 把矩阵b接在矩阵a后面，生成增广矩阵</span><br><span class="line">print(np.vstack((a,b))) # 把矩阵b接在矩阵a下面，拼接矩阵务必要求吻合</span><br><span class="line"></span><br><span class="line">a=np.floor(np.random.random((2,12))*10)</span><br><span class="line">print(a)</span><br><span class="line">print(np.hsplit(a,3)) # 把2行12列的矩阵按列均分为几个子矩阵</span><br><span class="line">print(np.hsplit(a,(3,4)))  # 在第三列和第4列后面切分成3个子矩阵</span><br><span class="line">a=a.T</span><br><span class="line">print(np.vsplit(a,3)) # 按行平均切分成3个子矩阵</span><br></pre></td></tr></table></figure>
<h4 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h4><ol>
<li><a href="https://blog.csdn.net/qq_27261889/article/details/80531931" target="_blank" rel="noopener">python+numpy</a></li>
<li><a href="https://blog.csdn.net/kwame211/article/details/80179747" target="_blank" rel="noopener">对NumPy中dot()函数的理解</a></li>
<li><a href="https://blog.csdn.net/Leekingsen/article/details/76242244" target="_blank" rel="noopener">numpy.sum()的使用</a></li>
</ol>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python与人工智能 资源</title>
    <url>/2019/07/11/%E5%87%A0%E4%B8%AA%E5%AD%A6%E4%B9%A0Python%E7%9A%84%E7%BD%91%E7%AB%99%E4%BB%A5%E5%8F%8A%E4%B8%80%E4%BA%9B%E7%9B%B8%E5%85%B3%E5%BA%93/</url>
    <content><![CDATA[<h4 id="Python教程"><a href="#Python教程" class="headerlink" title="Python教程"></a>Python教程</h4><ol>
<li><a href="https://docs.python.org/zh-cn/3.7/index.html" target="_blank" rel="noopener">Python tutorial</a> ，可以自选版本文档</li>
<li><a href="https://www.runoob.com/python/python-tutorial.html" target="_blank" rel="noopener">菜鸟教程</a> 适合新手入门</li>
</ol>
<h4 id="Numpy库"><a href="#Numpy库" class="headerlink" title="Numpy库"></a>Numpy库</h4><ol>
<li><a href="https://docs.scipy.org/doc/numpy/user/basics.html" target="_blank" rel="noopener">NumPy basics</a></li>
<li><a href="https://docs.scipy.org/doc/numpy/user/quickstart.html" target="_blank" rel="noopener">Quickstart tutorial</a></li>
<li><a href="https://scipy.org/" target="_blank" rel="noopener">SciPy</a></li>
<li><a href="https://docs.scipy.org/doc/" target="_blank" rel="noopener">Numpy and Scipy Documentation</a></li>
</ol>
<h4 id="Scikit-learn"><a href="#Scikit-learn" class="headerlink" title="Scikit-learn"></a>Scikit-learn</h4><ol>
<li><a href="https://sklearn.apachecn.org/#/" target="_blank" rel="noopener">scikit-learn (sklearn) 官方文档中文版</a></li>
<li><a href="https://scikit-learn.org/stable/" target="_blank" rel="noopener">官方网站</a></li>
</ol>
<h4 id="人工智能"><a href="#人工智能" class="headerlink" title="人工智能"></a>人工智能</h4><ol>
<li><a href="https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/80054762" target="_blank" rel="noopener">感动！有人将吴恩达的视频课程做成了文字版</a></li>
<li><a href="http://www.ai-start.com/" target="_blank" rel="noopener">AI初学者</a></li>
<li><a href="https://study.163.com/course/courseMain.htm?courseId=1004570029&amp;_trace_c_p_k2_=d8788a83641c44478e40885f7bdf4ddd" target="_blank" rel="noopener">吴恩达机器学习</a></li>
<li><a href="https://mooc.study.163.com/university/deeplearning_ai#/c" target="_blank" rel="noopener">deeplearning视频课程</a></li>
<li><a href="https://spinningup.openai.com/en/latest/user/introduction.html" target="_blank" rel="noopener">深度强化学习</a></li>
<li><a href="https://www.jiqizhixin.com/" target="_blank" rel="noopener">机器之心</a></li>
</ol>
<h4 id="两个顶会"><a href="#两个顶会" class="headerlink" title="两个顶会"></a>两个顶会</h4><ol>
<li><a href="https://www.ijcai.org/" target="_blank" rel="noopener">KDD官网</a></li>
<li><a href="https://www.ijcai.org/" target="_blank" rel="noopener">IJCAI官网</a></li>
</ol>
<hr>
<center> <font size="5">以上资源通过深度索引探索更多有趣资源，学无止境！</font> </center>]]></content>
      <categories>
        <category>资源分享</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>人工智能</tag>
      </tags>
  </entry>
  <entry>
    <title>2019.7.7</title>
    <url>/2019/07/07/2019-7-7/</url>
    <content><![CDATA[<h3 id="暑期计划"><a href="#暑期计划" class="headerlink" title="暑期计划"></a><center>暑期计划</center></h3><p><strong>时间</strong>：2019.7.1 - 2019.9.1，共计60天。<br><strong>阶段</strong>：</p>
<ul>
<li>第一阶段：2019.7.4 - 2019.8.3，共计30天。</li>
<li>第二阶段：2019.8.5 - 2019.9.1，共计30天。  </li>
</ul>
<p><strong>地点</strong>：第一阶段南航，第二阶段预计在家。  </p>
<hr>
<p>本文只作第一阶段计划，第二阶段暂定8.1号完成。</p>
<hr>
<h4 id="关于学习"><a href="#关于学习" class="headerlink" title="关于学习"></a>关于学习</h4><ol>
<li>MySQL不再往下学</li>
<li>机器学习进度较慢，尚未实操，matpoltlib入门完成，预计一周实现一个算法。</li>
<li>scikit库还未开始接触，预计基于机器学习涉及线性代数相关库入手。</li>
<li>python面向对象不再往下学习，基于第三条扩展相关库知识。</li>
<li>Django框架入门</li>
<li>尝试接触stack overflow</li>
<li>文献翻译计划，一周完成一篇英文刊物翻译，内容不限。原文与译文将公布在<strong><a href="https://sfz-lyq.cn/">种花家</a></strong>。</li>
<li>刷题计划，在NYOJ、codeforces和牛客网平台使用python与c进行编程训练，锻炼思维。</li>
<li>提高知识检索能力(运用百度能力)</li>
</ol>
<h4 id="关于生活"><a href="#关于生活" class="headerlink" title="关于生活"></a>关于生活</h4><ul>
<li>一周至少锻炼3次，每次不低于30分钟，内容包括跑步、引体、俯卧撑等。</li>
<li>无其它娱乐爱好，周末利用天时地利人和打点好生活起居。</li>
<li>每周自我总结</li>
</ul>
<h4 id="时间安排"><a href="#时间安排" class="headerlink" title="时间安排"></a>时间安排</h4><ul>
<li>周一至周五为正常工作时间，周六可以用来瞎搞其他东西，尽量作出完整作品，也可以解决本周遗留问题；周日为可选休息日。</li>
<li>正常时间预计为8：30am - 9：40pm，周末可选择性安排时间。</li>
</ul>
<p>以上计划均可灵活调整，但除周末外每日学习时间预计不低于8h。</p>
]]></content>
      <categories>
        <category>计划</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>计划</tag>
      </tags>
  </entry>
  <entry>
    <title>Python图形绘制</title>
    <url>/2019/07/07/Python%E5%9B%BE%E5%BD%A2%E7%BB%98%E5%88%B6/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://www.bilibili.com/video/av50240377/?p=237" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h4><ul>
<li>pycharm2018.3.3 x64</li>
<li>python3.7.2</li>
<li>matplotlib 3.1.0 </li>
</ul>
<h4 id="Matplotlib简介"><a href="#Matplotlib简介" class="headerlink" title="Matplotlib简介"></a>Matplotlib简介</h4><p>Matplotlib是Python的2D绘图库，与turtle绘图不同，turtle是内置在python库里面的，而Matplotlib是第三方库，需要安装。<br>Matplotlib可以生成直方图、功率谱、条形图、错误图、散点图等，让数据可视化。<br>官网：<a href="https://matplotlib.org" target="_blank" rel="noopener">https://matplotlib.org</a>  </p>
<h4 id="Matplotlib安装"><a href="#Matplotlib安装" class="headerlink" title="Matplotlib安装"></a>Matplotlib安装</h4><ul>
<li>windows命令安装：<code>pip install matplotlib</code></li>
<li>pycharm集成安装(推荐)：File-&gt;settings-&gt;Project Interpreter-&gt;Package左侧加号-&gt;找到它并安装   </li>
</ul>
<p>进入Python环境使用语句<code>import matplotlib.pyplot as plt</code>导入这个模块看看会不会报错，如果不会则安装成功  </p>
<h4 id="绘制基础"><a href="#绘制基础" class="headerlink" title="绘制基础"></a>绘制基础</h4><ol>
<li>两个最常用的场景：画点、画线。  </li>
<li>导入pyplot模块，并给它指定别名plt，避免反复输入pyplot。模块pyplot中包含很多用于生产图表的函数。  </li>
<li>matplotlib支持生成eps, pdf, pgf, png, ps, raw, rgba, svg, svgz等格式图片，方法为：<code>plt.savefig()</code>，括号内填入保存地址加图片名称，如<code>&#39;./1.png&#39;</code>，表示保存至当前工作目录下。<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMDcvNWQyMTYxNDM3NTFmZjgyNzgxLnBuZw" alt="V%_HTCGVJ4U$TL27%~`OM5P.png"></li>
</ol>
<h4 id="绘制直线"><a href="#绘制直线" class="headerlink" title="绘制直线"></a>绘制直线</h4><ol>
<li>导入模块</li>
<li>将绘制直线的坐标传递给函数plot</li>
<li>通过函数plot.show()打开Matplotlib查看器，显示绘制的图形  </li>
</ol>
<p>示例：绘制一条直线，起点是[0,1]，终点是[2,4]<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt  # 导入模块</span><br><span class="line"># 绘制一条直线，起点是[0,1]，终点是[2,4]</span><br><span class="line">plt.plot([0,2],[1,4])   # 传递两个列表，第一个列表是x坐标，第二个是y坐标</span><br><span class="line">plt.savefig(&quot;./1.jpg&quot;)  # 将图片存储到当前工作目录，支持eps, pdf, pgf, png, ps, raw, rgba, svg, svgz等格式。路径名与图片名任取</span><br><span class="line">plt.show()   # 显示图形</span><br></pre></td></tr></table></figure></p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMDcvNWQyMTY1MzZjZmRkNjIzMTk3LnBuZw" alt="1.png"></p>
<h4 id="绘制折线"><a href="#绘制折线" class="headerlink" title="绘制折线"></a>绘制折线</h4><p>同绘制直线。<br>令x=[1,2,3,4]，y=[1,4,9,16]这样一个平方数序列，然后将x和y传给plot函数:<code>plt.plot(x,y)</code>。<br>如图：<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMDcvNWQyMTY4NTllYjRiZTcxMzI2LnBuZw" alt="2.png"></p>
<h4 id="设置样式"><a href="#设置样式" class="headerlink" title="设置样式"></a>设置样式</h4><p> 在原图的基础上设置线条粗细、文字等。<br> 设置线条宽度：<code>plt.plot(x,y,linewidth=5)</code><br> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt  # 导入模块</span><br><span class="line">x=[1,2,3,4,5]</span><br><span class="line">y=[1,4,9,16,25]</span><br><span class="line">plt.plot(x,y,linewidth=2)   # linewidth属性设置线条宽度</span><br><span class="line">plt.title(&apos;Numbers&apos;,fontsize=25)   # 设置图表标题</span><br><span class="line">plt.xlabel(&apos;x&apos;,fontsize=14)  # 给坐标轴添加标签</span><br><span class="line">plt.ylabel(&apos;y&apos;,fontsize=14)</span><br><span class="line">plt.savefig(&apos;./plt.draw/3.png&apos;)  # 保存至相应路径并命名，生成一张保存一次</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p> <img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMDcvNWQyMTc3Yjg5ODViYTEzNTc1LnBuZw" alt="3.png"><br>Matplotlib默认不支持中文，如果要设置中文，在所有中文的最前面添加语句：<code>plt.rcParams[&#39;font.sans-serif&#39;] = [&#39;SimHei&#39;]</code>  ，但是对坐标轴负数的负号显示有影响，需要再加上语句：<code>plt.rcParams[&#39;axes.unicode_minus&#39;] = False</code><br>参考：<a href="https://www.cnblogs.com/hhh5460/p/4323985.html" target="_blank" rel="noopener">https://www.cnblogs.com/hhh5460/p/4323985.html</a></p>
<h4 id="绘制曲线"><a href="#绘制曲线" class="headerlink" title="绘制曲线"></a>绘制曲线</h4><h5 id="绘制一元二次方程曲线"><a href="#绘制一元二次方程曲线" class="headerlink" title="绘制一元二次方程曲线"></a>绘制一元二次方程曲线</h5> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> import matplotlib.pyplot as plt  # 导入模块</span><br><span class="line"> # 绘制一元二次曲线</span><br><span class="line">x=range(-100,100)  # 200个点的x坐标</span><br><span class="line">y=[i**2 for i in x]  # 生成对应y点的坐标</span><br><span class="line">plt.plot(x,y)  # 绘制一元二次曲线</span><br><span class="line">plt.savefig(&apos;./plt.draw/4&apos;)    # 如果不加图片格式默认生成png格式</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMDcvNWQyMTkxM2RiMWE4ODU0ODE3LnBuZw" alt="4.png"></p>
<h5 id="绘制正弦余弦曲线"><a href="#绘制正弦余弦曲线" class="headerlink" title="绘制正弦余弦曲线"></a>绘制正弦余弦曲线</h5><p>使用plt函数绘制任何曲线的第一步都是生成若干个坐标点(x,y)，理论上越多越好。<br>本例，取0到10之间100个等差数作为x的坐标，然后将这100个x坐标值传入Numpy的sin和cos函数，得到100个对应的y坐标值。最后使用plot绘制两种曲线。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import matplotlib</span><br><span class="line">#matplotlib.use(&apos;TkAgg&apos;)  # 可以动态调整图片 </span><br><span class="line">import matplotlib.pyplot as plt  # 导入模块</span><br><span class="line">import numpy as np  </span><br><span class="line"> # 绘制正弦余弦曲线</span><br><span class="line">x=np.linspace(0,10,100)  # 取0-10之间100个数作为x坐标</span><br><span class="line">sin_y=np.sin(x)   # 生成y坐标值</span><br><span class="line">plt.plot(x,sin_y)   # 绘制正弦曲线</span><br><span class="line">cos_y=np.cos(x)</span><br><span class="line">plt.plot(x,cos_y)  # 绘制余弦曲线</span><br><span class="line">plt.rcParams[&apos;font.sans-serif&apos;] = [&apos;SimHei&apos;]  # 解决中文显示乱码</span><br><span class="line">plt.rcParams[&apos;axes.unicode_minus&apos;] = False   # 解决上一行引起的负号显示乱码</span><br><span class="line">plt.title(&quot;sin_cos曲线&quot;,fontsize=25)</span><br><span class="line">plt.xlabel(&apos;x&apos;,fontsize=14)</span><br><span class="line">plt.ylabel(&apos;y&apos;,fontsize=14)</span><br><span class="line">plt.savefig(&apos;./plt.draw/5&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMDcvNWQyMTlhNzU4MDUxZDQ1MDk4LnBuZw" alt="5.png"></p>
<h4 id="subplot的使用"><a href="#subplot的使用" class="headerlink" title="subplot的使用"></a>subplot的使用</h4><p>将画布分区，把图表画到画布的指定区域<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import matplotlib</span><br><span class="line">matplotlib.use(&apos;TkAgg&apos;)  # 可以动态调整图片</span><br><span class="line">import matplotlib.pyplot as plt  # 导入模块</span><br><span class="line">import numpy as np</span><br><span class="line">x=np.linspace(0,10,100)  # 取0-10之间100个数作为x坐标</span><br><span class="line">sin_y=np.sin(x)   # 生成y坐标值</span><br><span class="line">plt.subplot(2,2,1) # 将图画分为2行2列4个区域，将图画到画布的1区域</span><br><span class="line">plt.plot(x,sin_y)   # 绘制正弦曲线</span><br><span class="line">plt.rcParams[&apos;font.sans-serif&apos;] = [&apos;SimHei&apos;]  # 解决中文显示乱码</span><br><span class="line">plt.rcParams[&apos;axes.unicode_minus&apos;] = False   # 解决上面引起的负号显示乱码</span><br><span class="line">plt.title(&quot;sin曲线&quot;,fontsize=15)</span><br><span class="line">plt.xlabel(&apos;x&apos;,fontsize=7)</span><br><span class="line">plt.ylabel(&apos;y=sinx&apos;,fontsize=7)</span><br><span class="line">cos_y=np.cos(x)</span><br><span class="line">plt.subplot(2,2,3)  # 指定3区域</span><br><span class="line">plt.plot(x,cos_y)  # 绘制余弦曲线</span><br><span class="line">plt.rcParams[&apos;font.sans-serif&apos;] = [&apos;SimHei&apos;]  # 解决中文显示乱码</span><br><span class="line">plt.rcParams[&apos;axes.unicode_minus&apos;] = False   # 解决上面引起的负号显示乱码</span><br><span class="line">plt.title(&quot;cos曲线&quot;,fontsize=15)</span><br><span class="line">plt.xlabel(&apos;x&apos;,fontsize=7)</span><br><span class="line">plt.ylabel(&apos;y=cosx&apos;,fontsize=7)</span><br><span class="line">#plt.savefig(&apos;./plt.draw/5&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMDcvNWQyMTllYTY1NmMwNTE3OTEyLnBuZw" alt="6.png"><br>如果想修改x、y坐标范围，可以调用plt.xlim(left,right)或者plt.ylim(left,right)方法，left和right为指定上下限。</p>
<h4 id="绘制散点图"><a href="#绘制散点图" class="headerlink" title="绘制散点图"></a>绘制散点图</h4><p>scatter函数可以绘制随机点，该函数需要接收x和y的坐标序列。<br>与plot函数不同，scatter绘制的是不连续的点。当然，plot函数也可以实现绘制点：<code>plt.plot(x,y,&#39;o&#39;)</code>，o表示圆标记，和scatter绘制的普通散点图无差别，而且性能优于scatter。<br>如果所绘制的点的形状(颜色大小)有要求，必须使用scatter。<br>示例：绘制10种大小，100种颜色的散点图<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"> # 绘制10种大小，100种颜色的散点图</span><br><span class="line">np.random.seed(0)  # 执行多次时每次获取的随机数是一样的</span><br><span class="line">x=np.random.rand(100)  # 随机生成100个数</span><br><span class="line">y=np.random.rand(100)</span><br><span class="line">size=np.random.rand(10)*1000 #生成10种大小</span><br><span class="line">color=np.random.rand(100) # 生成100中颜色</span><br><span class="line">plt.scatter(x,y,s=size,c=color,alpha=0.7) # s表示点的大小，c表示点的颜色，alpha表示点的透明度</span><br><span class="line">plt.savefig(&apos;./plt.draw/7&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMDcvNWQyMWE1NGZiY2I4ZTg1OTM2LnBuZw" alt="7.png"><br>注意：点的个数和点的颜色个数要相同；点的个数和点的大小的个数可以不同，用完了就循环使用大小。</p>
<h4 id="绘制不同样式不同颜色的线"><a href="#绘制不同样式不同颜色的线" class="headerlink" title="绘制不同样式不同颜色的线"></a>绘制不同样式不同颜色的线</h4><p>线的样式如图：<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMDcvNWQyMWE5NGEzZmEyYzEwNzI5LnBuZw" alt="格式化字符.png"><br>线的颜色如图：<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMDcvNWQyMWE5ODg5MDU0YzY0OTUxLnBuZw" alt="字符颜色.png"></p>
<p>示例：绘制不同颜色和样式的线条<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">x=np.linspace(0,10,100)</span><br><span class="line">plt.xlim(-2,10)</span><br><span class="line">plt.ylim(-2,10)</span><br><span class="line"># 使用legend()图例，给plot方法添加参数label</span><br><span class="line">plt.plot(x,x+0,label=&apos;null&apos;)</span><br><span class="line">plt.plot(x,x+1,&apos;--g&apos;,label=&apos;--g&apos;)</span><br><span class="line">plt.plot(x,x-1,&apos;.b&apos;,label=&apos;.b&apos;)</span><br><span class="line">plt.plot(x,x+2,&apos;-.r&apos;,label=&apos;-.r&apos;)</span><br><span class="line">plt.plot(x,x-2,&apos;:c&apos;,label=&apos;:c&apos;)</span><br><span class="line">plt.legend(loc=&apos;lower right&apos;)  # 默认在左上角upper left，可以通过loc修改</span><br><span class="line">plt.savefig(&apos;./plt.draw/8&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMDcvNWQyMWFlNzNkNWM0ODM5NDQxLnBuZw" alt="8.png"></p>
<h4 id="绘制柱状图"><a href="#绘制柱状图" class="headerlink" title="绘制柱状图"></a>绘制柱状图</h4><p>使用bar函数可以绘制柱状图，同样需要x与y的坐标列表。<br>示例：绘制销售图<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">x=[1980,1985,1990,1995]</span><br><span class="line">x_label=[&apos;1980年&apos;,&apos;1985年&apos;,&apos;1990年&apos;,&apos;1995年&apos;]</span><br><span class="line">y=[1000,3000,4000,5000]</span><br><span class="line">plt.bar(x,y,width=3)  # 调用bar函数绘制柱状图，width修改柱的宽度</span><br><span class="line">plt.rcParams[&apos;font.sans-serif&apos;] = [&apos;SimHei&apos;]  # 正常显示中文标签</span><br><span class="line">plt.xticks(x,x_label)  # 修改x轴显示值</span><br><span class="line">plt.xlabel(&apos;年份&apos;,fontsize=7)  # 给坐标轴添加名称</span><br><span class="line">plt.ylabel(&apos;销量&apos;,fontsize=7)</span><br><span class="line">plt.title(&apos;按年份销售变化图&apos;)  # 设置图表标题</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMDcvNWQyMWIyMTNkNTkxNTIwMjM5LnBuZw" alt="9.png"></p>
<h5 id="bar与barh函数的使用"><a href="#bar与barh函数的使用" class="headerlink" title="bar与barh函数的使用"></a>bar与barh函数的使用</h5><p>示例：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">np.random.seed(0)</span><br><span class="line">x=np.arange(5)  # 生成[0,1,2,3,4,5]的列表</span><br><span class="line">y=np.random.randint(-5,5,5)  # 在[-5,5]之间取5个数</span><br><span class="line">plt.subplot(1,2,1)</span><br><span class="line">var_bar=plt.bar(x,y,color=&apos;blue&apos;) # 在第一个区域画bar</span><br><span class="line">plt.axhline(0,color=&apos;blue&apos;,linewidth=2)</span><br><span class="line">for bar,height in zip(var_bar,y):   # 对不同的y值的柱设置不同的颜色，zip是聚合，万物皆对象</span><br><span class="line">	if height&lt;0:</span><br><span class="line">		bar.set(color=&apos;green&apos;)</span><br><span class="line">plt.subplot(1,2,2)</span><br><span class="line">plt.barh(x,y,color=&apos;red&apos;) # 在第二个区域花barh，barh将y和x进行对换，竖着方向为x轴</span><br><span class="line">plt.axvline(0,color=&apos;red&apos;,linewidth=2)</span><br><span class="line">plt.savefig(&apos;./plt.draw/10&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMDcvNWQyMWMxMTg4Y2RiNjExMDY4LnBuZw" alt="10.png"></p>
<h4 id="绘制饼图"><a href="#绘制饼图" class="headerlink" title="绘制饼图"></a>绘制饼图</h4><p>pie函数可以绘制饼状图，饼状图主要用来呈现比例，只要传入比例数据即可。<br>示例：绘制男女比例<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">man,woman=71351,68187   # 男生和女生的数量</span><br><span class="line">man_perc=man/(woman+man)</span><br><span class="line">woman_perc=woman/(woman+man)</span><br><span class="line">plt.rcParams[&apos;font.sans-serif&apos;] = [&apos;SimHei&apos;]</span><br><span class="line"> # 绘制饼状图并设置名称和颜色,分割饼图,设置百分比，先后顺序无所谓</span><br><span class="line">paches,texts,autotexts=plt.pie([man_perc,woman_perc],colors=[&apos;blue&apos;,&apos;red&apos;],labels=[&apos;男&apos;,&apos;女&apos;],explode=(0,0.05),autopct=&apos;%.2f%%&apos;)</span><br><span class="line">for text in autotexts:  # 设置饼状图字体颜色</span><br><span class="line">	text.set_color(&apos;white&apos;)</span><br><span class="line">for text in texts+autotexts:</span><br><span class="line">	text.set_fontsize(15)</span><br><span class="line">plt.title(&apos;男女比例图&apos;,fontsize=25)</span><br><span class="line">plt.savefig(&apos;./plt.draw/11&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMDcvNWQyMWRhNTk5MjdiMDE3NjUzLnBuZw" alt="11.png"></p>
<h4 id="绘制直方图"><a href="#绘制直方图" class="headerlink" title="绘制直方图"></a>绘制直方图</h4><p>直方图与柱状图类似，但直方图关注的是分布状态的，而柱状图关注的是数据。使用hist函数绘制直方图。<br>示例: 绘制正太分直方图<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">x=np.random.randn(1000)  # 生成1000个标准的正太分布随机数</span><br><span class="line">plt.hist(x,bins=100)  # 将10个柱绑在一起显示,等于设置柱体宽度</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMDcvNWQyMWRlZGU4YjUxODk0MTQ0LnBuZw" alt="12.png"></p>
<h4 id="绘制等高线和三维图"><a href="#绘制等高线和三维图" class="headerlink" title="绘制等高线和三维图"></a>绘制等高线和三维图</h4><p>参考：<a href="https://www.cnblogs.com/xingshansi/p/6777945.html" target="_blank" rel="noopener">python绘制三维图</a><br>示例：  等高线<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">plt.rcParams[&apos;font.sans-serif&apos;] = [&apos;SimHei&apos;]</span><br><span class="line">plt.rcParams[&apos;axes.unicode_minus&apos;] = False</span><br><span class="line">x=np.linspace(-10,10,100) # 取100个等差的数</span><br><span class="line">y=np.linspace(-10,10,100)</span><br><span class="line">X,Y=np.meshgrid(x,y)  # 计算x和y的相交点，产生n^2个点</span><br><span class="line">Z=np.sqrt(X**2+Y**2)  # 根据相交点的x、y值计算z值,z的计算公式不一样画出来的等高线图也是不一样的</span><br><span class="line"> # plt.contour(X,Y,Z)</span><br><span class="line">plt.contourf(X,Y,Z)  # 填充等高线之间的空隙颜色，呈现出区域的分划状</span><br><span class="line">plt.xlabel(&apos;x&apos;,fontsize=10)</span><br><span class="line">plt.ylabel(&apos;y&apos;,fontsize=10)</span><br><span class="line">plt.title(&apos;等高线图&apos;,fontsize=25)</span><br><span class="line"># 颜色越深表示值越小，中间的黑色表示z=0</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDcvMDcvNWQyMWUyM2Y5ZWFiZjQ3MjU1LnBuZw" alt="13.png"></p>
<p>示例：三维图<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import matplotlib</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">from mpl_toolkits.mplot3d import Axes3D  #导入3D包</span><br><span class="line">x=[1,1,2,2]</span><br><span class="line">y=[3,4,4,3]</span><br><span class="line">z=[1,100,1,1]</span><br><span class="line">figure=plt.figure()  # 创建画布</span><br><span class="line">ax=Axes3D(figure) # 创建Axes3D对象</span><br><span class="line">ax.plot_trisurf(x,y,z)  # 绘制三维三角表面图，可尝试其他类型图</span><br><span class="line">plt.xlabel(&apos;x&apos;,fontsize=7)  # 设置坐标轴标签</span><br><span class="line">plt.ylabel(&apos;y&apos;,fontsize=7)</span><br><span class="line">ax.set_zlabel(&apos;z&apos;,fontsize=10) </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>多变量线形回归</title>
    <url>/2019/06/24/%E5%A4%9A%E5%8F%98%E9%87%8F%E7%BA%BF%E5%BD%A2%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029" target="_blank" rel="noopener">学习地址</a></center></h3><p>还是预测房屋价格案例<br>当<br>知道的不仅仅是房子面积<br>还有房间数、楼层数以及房子年龄等特征时<br>这个时候就称为多变量(特征)   </p>
<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>n：特征数，相当与单变量训练集数m<br>x<sup>(i)</sup>：第i个训练样本，即第i个特征(输入)，可能是一个向量了<br>x<sub>j</sub><sup>(i)</sup>：第i个训练样本中第j个特征量的值<br>此时的假设函数：H<sub>&theta;</sub>(x)=&theta;<sub>0</sub>+&theta;<sub>1</sub>x<sub>1</sub>+&theta;<sub>2</sub>x<sub>2</sub>+…&theta;<sub>n</sub>x<sub>n</sub>  =&theta;<sup>T</sup>x  </p>
<p>以上即是<strong>多元线形回归模型</strong></p>
<p>假设函数： H<sub>&theta;</sub>(x)= &theta;<sup>T</sup>x =&theta;<sub>0</sub>+&theta;<sub>1</sub>x<sub>1</sub>+&theta;<sub>2</sub>x<sub>2</sub>+…&theta;<sub>n</sub>x<sub>n</sub><br>参数：&theta;：&theta;<sub>0</sub>…&theta;<sub>n</sub><br>代价函数： J(&theta;<sub>0</sub>,&theta;<sub>1</sub>,…,&theta;<sub>n</sub>)=1/2/m*&sum;(h<sub>&theta;</sub>(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>  </p>
<h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><p><img src="https://i.loli.net/2019/06/24/5d10c9a2d057b24695.png" alt="2019-06-24 20-59-04 的屏幕截图.png">   </p>
<hr>
<p><font size="7">梯度下降使用技巧</font>   </p>
<hr>
<p><font color="red" size="6px">特征缩放</font>  </p>
<ul>
<li>目的：将特征的取值约束到-1到+1的范围内，不同的特征值可能需要除以不同的数控制在这个范围，一般除以最大值</li>
<li>范围在一定范围是可以接受的，比如[-3,+3]，[-1/3,+1/3]，但不能太突兀 </li>
<li>进行特征缩放是让特征变量的范围在数值上相差不大，这样作出的等值线近似圆，便于找最佳点</li>
</ul>
<p><font color="green" size="6px">均值化</font>  </p>
<ul>
<li>将特征值进行同等变换，使得所有同类型的特征值的均值为0。比如房屋面积X<sub>1</sub>在[0,2000]内，那么这个变换可以是(X<sub>1</sub>-1000)/2000   </li>
</ul>
<h4 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h4><ul>
<li>梯度下降算法没有正常工作：使用更小的学习率&alpha;；&alpha;选用过大可能会错过最佳点</li>
<li>数学家已经证明如果&alpha;足够小那么J(&theta;)一定会下降；但如果&alpha;过小，那么梯度下降算法收敛得很慢；</li>
</ul>
<h4 id="特征和多项式回归"><a href="#特征和多项式回归" class="headerlink" title="特征和多项式回归"></a>特征和多项式回归</h4><ul>
<li>使用新的特征代替原来的特征，得到新的假设与更好的模型</li>
<li>使用二次型或者三次型函数来拟合数据，不必拘泥与一次函数</li>
<li>结合以上：用新的特征取代高次特征，再用线形回归拟合这个模型<br><img src="https://i.loli.net/2019/06/25/5d12105f181d275633.png" alt="2019-06-25 20-15-10 的屏幕截图.png">  </li>
</ul>
<h4 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h4><p><img src="https://i.loli.net/2019/06/26/5d12e999d670514247.png" alt="2019-06-26 11-41-43 的屏幕截图.png"><br><img src="https://i.loli.net/2019/06/26/5d12ea460080371075.png" alt="2019-06-26 11-44-53 的屏幕截图.png"><br><img src="https://i.loli.net/2019/06/26/5d12eaa8abe2693344.png" alt="2019-06-26 11-46-35 的屏幕截图.png">  </p>
<h5 id="正规方程在矩阵XTX不可逆的解法"><a href="#正规方程在矩阵XTX不可逆的解法" class="headerlink" title="正规方程在矩阵XTX不可逆的解法"></a>正规方程在矩阵<em>X</em><sup>T</sup><em>X</em>不可逆的解法</h5><ul>
<li>不可逆矩阵：奇异矩阵/退化矩阵</li>
<li>如果一个特征是另一个特征的倍数关系，那么完全可以删除其中一个特征，即删除冗余特征</li>
<li>在线代中，如果列数n大于行数m，那么一定线形相关，列数就是特征数，可以删除多于无用的特征</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL学习笔记(七)</title>
    <url>/2019/06/23/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%83/</url>
    <content><![CDATA[<h3 id="学习地址-p28-p29"><a href="#学习地址-p28-p29" class="headerlink" title="学习地址 p28-p29"></a><center><a href="https://www.bilibili.com/video/av19538278" target="_blank" rel="noopener">学习地址 p28-p29</a></center></h3><h5 id="1-N查询模式与左连接"><a href="#1-N查询模式与左连接" class="headerlink" title="1+N查询模式与左连接"></a>1+N查询模式与左连接</h5><p>所谓1+N查询模式就是一步步查询出最终结果，效率差  </p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>线代回顾</title>
    <url>/2019/06/06/%E7%BA%BF%E4%BB%A3%E5%9B%9E%E9%A1%BE/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029" target="_blank" rel="noopener">学习地址</a></center></h3><p>矩阵</p>
<ul>
<li>维数：行数&times;列数 [4&times;2]</li>
<li>元素集合：R<sup>nm</sup></li>
<li>元素：A<sub>ij</sub>  </li>
</ul>
<p>向量 Vector</p>
<ul>
<li>n&times;1的矩阵</li>
<li>元素个数叫做维数</li>
<li>y<sub>i</sub>：第i个元素</li>
</ul>
<blockquote>
<p>通常用大写字母表示矩阵，小写字母表示标量或向量</p>
</blockquote>
<p>矩阵加减法</p>
<ul>
<li>同型矩阵对应元素相加减，注意只有同型矩阵满足加减法</li>
</ul>
<p>矩阵乘法</p>
<ul>
<li>数乘矩阵 kA：矩阵每个元素都乘以k</li>
<li>矩阵除以数相当与乘以倒数，本质还是乘法</li>
<li>A<sub>n&times;m</sub>&times;B<sub>m&times;n</sub>=C<sub>n&times;n</sub>  ，不可逆，前一个列数必须等与后一个行数。<br>C语言实现：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">matrix multiply(mattix A,matrix B)&#123;</span><br><span class="line"> matrix res;</span><br><span class="line"> memset(res,0,sizeof(res));</span><br><span class="line"> for(int i=0;i&lt;n;i++)</span><br><span class="line">   for(int j=0;j&lt;n;j++)</span><br><span class="line">     for(int k=0;k&lt;m;k++)</span><br><span class="line">     res[i][j]+=A[i][k]*B[k][j];</span><br><span class="line">return res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>单位矩阵</p>
<ul>
<li>用<em>I</em>表示，或者I<sub>n&times;n</sub></li>
<li>主对角线上元素都为1，其余都为0</li>
<li>对于任意矩阵A，A&times;I=I&times;A=A</li>
</ul>
<p>矩阵的逆和转置</p>
<ul>
<li>逆：A<sub>nn</sub>A<sup>-1</sup><sub>nn</sub>=E</li>
<li>转置A<sup>T</sup>：swap(A[i][j],A[j][i])，行列互换</li>
<li>矩阵可逆：矩阵是方阵，并且秩R(A)=n</li>
<li>奇异矩阵：不可逆的方阵</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title>NYOJ 113-字符串替换</title>
    <url>/2019/06/06/NYOJ-114-%E6%9F%90%E7%A7%8D%E5%BA%8F%E5%88%97/</url>
    <content><![CDATA[<h3 id="NYOJ-113-字符串替换"><a href="#NYOJ-113-字符串替换" class="headerlink" title="NYOJ 113-字符串替换"></a><center><a href="http://nyoj.top/problem/113" target="_blank" rel="noopener">NYOJ 113-字符串替换</a></center></h3><p>题意：输入一个字符串，将其中的”you”替换成”we”。<br>Python3解法：使用str.replace(old,new)即可。<br>代码如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/user/bin/python</span><br><span class="line">while True:</span><br><span class="line">    try:</span><br><span class="line">        str=input()</span><br><span class="line">        str=str.replace(&quot;you&quot;,&quot;we&quot;);</span><br><span class="line">        print(str)</span><br><span class="line">    except:break;</span><br></pre></td></tr></table></figure></p>
<h3 id="Python字符串的一些知识点"><a href="#Python字符串的一些知识点" class="headerlink" title="Python字符串的一些知识点"></a>Python字符串的一些知识点</h3><p>在Python中可以使用一对双引号或者一对单引号定义一个字符串，双引号里面嵌套双引号需要转义，而在单引号里面就可以直接嵌套了</p>
<h4 id="字符串常用操作"><a href="#字符串常用操作" class="headerlink" title="字符串常用操作"></a>字符串常用操作</h4><p>统计长度： len(str)<br>统计某个小字符串出现的次数：str.count(tmp_str)<br>取某个子字符串出现的位置： str.index(tmp_str)  # 返回第一次出现tmp_str的位置 ,如果不存在则报错<br>截取字符串： str[begin:end:num]  在指定范围内每隔num个字符截取一个字符</p>
<h4 id="Python内置提供的方法"><a href="#Python内置提供的方法" class="headerlink" title="Python内置提供的方法"></a>Python内置提供的方法</h4><ol>
<li><p>判断类型   </p>
<pre><code>str.isspace()   # 如果字符串中只包含空白，则返回 True，否则返回 False.   
str.isalnum()   # 如果字符串至少有一个字符并且所有字符都是字母或数字则返 回 True,否则返回 False  
str.isalpha()   # 如果字符串至少有一个字符并且所有字符都是字母则返回 True, 否则返回 False  
str.isdigit()   # 如果字符串只包含数字，1或者(1)或者半角字符，则返回 True 否则返回 False.  
str.isdecimal() # 检查字符串是否只包含十进制字符，如果是返回 true，否则返回 false。   
str.islower()   # 如果字符串中包含至少一个区分大小写的字符，并且所有这些(区分大小写的)字符都是小写，则返回 True，否则返回 False  
str.isnumeric() # 如果字符串中只包含数字字符比如(1)或者半角字符或者零一之类，则返回 True，否则返回 False  
str.isupper()   # 如果字符串中包含至少一个区分大小写的字符，并且所有这些(区分大小写的)字符都是大写，则返回 True，否则返回 False  
str.capitalize() # 将字符串的第一个字符转换为大写  
str.lower()      # 转换字符串中所有大写字符为小写  
str.upper()      # 转换字符串中所有大写字符为小写  
str.swapcase()   # 将字符串中大写转换为小写，小写转换为大写  
</code></pre></li>
<li><p>查找和替换  </p>
<pre><code>str.startswith(substr, beg,end) #检查字符串是否是以指定子字符串 substr 开头，是则返回 True。如果beg 和 end 指定值，则在指定范围内检查，也可以缺省     
str.endwith(substr, beg,end)  # 和上述类似
str.find(str, beg=0, end=len(string))  # 注意和index方法的区别
#检测 str 是否包含在字符串中，如果指定范围 beg 和 end ，则检查是否包含在指定范围内，如果包含则返回开始的索引值，否则返回-1
str.replace(old, new [, max])  # 将字符串中的 str1 替换成 str2,如果 max 指定，则替换不超过 max 次，返回替换后的字符串，原字符串并不会改变  
</code></pre></li>
<li><p>文本对齐</p>
<pre><code>str.ljust(width,fillchar) 
# 返回一个原字符串左对齐,并使用 fillchar 填充至长度 width 的新字符串，fillchar 默认为空格。
str.rjust(width,fillchar) 
# 返回一个原字符串右对齐,并使用fillchar(默认空格）填充至长度 width 的新字符串
str.center(width, fillchar)
# 返回一个指定的宽度 width 居中的字符串，fillchar 为填充的字符，默认为空格。
</code></pre></li>
<li><p>去除空白字符</p>
<pre><code>str.lstrip()    # 截掉字符串左边的空格或指定字符，即把以指定（默认为空格）开头的去掉再输出
str.rstrip()    # 删除字符串末尾的空格.
str.strip()     # 删除字符串两边的空白字符.
</code></pre></li>
<li><p>拆分和拼接字符字符串  </p>
<pre><code>string.pattition(str)   # 把字符串string 分成一个3元素的元组(str前面,str,str后面)
string.split(str,num)  # 以str为分隔符拆分string，如果num有指定值则拆分num次(str至少出现num次), 返回列表
string.join(seq)  # 以string作为分隔符，将 seq 中所有的元素(的字符串表示)合并为一个新的字符串返回
</code></pre></li>
</ol>
]]></content>
      <categories>
        <category>题解</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>题解</tag>
        <tag>NYOJ</tag>
      </tags>
  </entry>
  <entry>
    <title>NYOJ 114-某种序列</title>
    <url>/2019/06/06/NYOJ/</url>
    <content><![CDATA[<h3 id="NYOJ-114-某种序列"><a href="#NYOJ-114-某种序列" class="headerlink" title="NYOJ 114-某种序列"></a><center><a href="http://nyoj.top/problem/114" target="_blank" rel="noopener">NYOJ 114-某种序列</a></center></h3><p>题意：A[n]=A[n-1]+A[n-2]+A[n-3]，给出A[0]、A[1]、A[2]。求A[99]<br>Python3解法：直接用列表模拟数组就行，Python不用担心整数溢出问题。<br>代码如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/user/bin/python</span><br><span class="line">while True:</span><br><span class="line">    try:</span><br><span class="line">        A=list(map(int,input().split()))  #输入转化为列表</span><br><span class="line">        for index in range(3,100):</span><br><span class="line">            A.append(A[index-1]+A[index-2]+A[index-3])</span><br><span class="line">        print(A[99])</span><br><span class="line">    except:break;</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>题解</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>题解</tag>
        <tag>NYOJ</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL学习笔记(六)</title>
    <url>/2019/06/06/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%85%AD/</url>
    <content><![CDATA[<h3 id="学习地址-p23-p27"><a href="#学习地址-p23-p27" class="headerlink" title="学习地址 p23-p27"></a><center><a href="https://www.bilibili.com/video/av19538278" target="_blank" rel="noopener">学习地址 p23-p27</a></center></h3><h4 id="子查询"><a href="#子查询" class="headerlink" title="子查询"></a>子查询</h4><h5 id="where型子查询"><a href="#where型子查询" class="headerlink" title="where型子查询"></a>where型子查询</h5><p>把内层的查询结果作为外层查询的比较条件</p>
<ol>
<li>查出本网站最新的(goods_id最大的)一条商品<ul>
<li>思路：按goods_id desc排序，再取第一行</li>
<li>select goods_id,goods_name,from goods order by goods_id desc limit 1;</li>
</ul>
</li>
<li>查出本网站最新的(goods_id最大的)一条商品，要求不用排序<ul>
<li>思路：拆两步走，先查出goods_id最大的，然后再根据结果查出goods_name。</li>
<li><code>select max(goods_id) from goods;</code> + <code>select goods_id,goods_name from goods where goods_id=?</code></li>
<li>两条语句合并就是子查询了：<code>select goods_id,goods_name from goods where goods_id=(select max(goods_id) from goods);</code>  </li>
</ul>
</li>
</ol>
<h5 id="from型子查询"><a href="#from型子查询" class="headerlink" title="from型子查询"></a>from型子查询</h5><p>内层sql的查询结果当作一张临时表，这张表只是各种运算出来的结果看不出原数据表的结构，而这张临时表供外层sql再次查询。</p>
<h5 id="exists型子查询"><a href="#exists型子查询" class="headerlink" title="exists型子查询"></a>exists型子查询</h5><p>把外层sql的结果拿到内层sql去测试，如果内层sql成立，则该行取出</p>
<ul>
<li>查出有商品的栏目</li>
<li>思考：什么样的表叫做下面有商品</li>
<li>答：<code>select * from goods where cat_id=X</code>能取出数据  </li>
</ul>
<h5 id="关于NULL"><a href="#关于NULL" class="headerlink" title="关于NULL"></a>关于NULL</h5><p>建表时列后面not null default ‘’/0 表示让这个列不允许为null，如果这个列没有填值，也有默认值，也不为null<br>null是一种类型，null的比较需要用特殊的运算符，<code>is null</code>和<code>is not null</code></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>HTML语法在Markdown的一些应用</title>
    <url>/2019/06/05/HTML%E8%AF%AD%E6%B3%95%E5%9C%A8Markdown%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BA%94%E7%94%A8/</url>
    <content><![CDATA[<h4 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h4><ul>
<li>插入数学公式，一些数学符号显示问题</li>
<li>字体颜色、大小</li>
</ul>
<h4 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h4><h5 id="上标与下标"><a href="#上标与下标" class="headerlink" title="上标与下标"></a>上标与下标</h5><p><code>&lt;sup&gt;xxx&lt;/sup&gt;</code><br>示例：a<sup>b</sup><br><code>&lt;sub&gt;xxx&lt;/sub&gt;</code><br>示例：a<sub>b</sub></p>
<h5 id="缩进问题"><a href="#缩进问题" class="headerlink" title="缩进问题"></a>缩进问题</h5><p><code>&amp;nbsp;</code><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;markdown里直接打空格的话是不行的，直接按键盘的空格，三个以内都没有效果，从第四个开始就变成了代码片段了。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;文章全是中文的话，需要打六个上述符号就能空出两个中文字啦。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;哈哈哈</p>
<h5 id="常用特殊符号"><a href="#常用特殊符号" class="headerlink" title="常用特殊符号"></a>常用特殊符号</h5><p>α：<code>&amp;alpha;</code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; β：<code>&amp;beta;</code><br>γ：<code>&amp;gamma;</code> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; θ：<code>&amp;theta;</code><br>λ：<code>&amp;lambda;</code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Σ：<code>&amp;Sigma;</code><br>≤：<code>&amp;le;</code> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;≥：<code>&amp;ge;</code><br>≠：<code>&amp;ne;</code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;±：<code>&amp;plusmn;</code></p>
<h5 id="字体和颜色"><a href="#字体和颜色" class="headerlink" title="字体和颜色"></a>字体和颜色</h5><p>字体大小： <code>&lt;font size=&quot;12px&quot;&gt;字体&lt;/font&gt;</code> &nbsp;&nbsp;&nbsp;示例：<font size="12px">字体</font><br>颜色： <code>&lt;font color=&quot;red&quot;&gt;&lt;/font&gt;</code> &nbsp;&nbsp;&nbsp;示例：<font color="red">红色</font><br>加粗：<code>&lt;strong&gt;粗体&lt;strong&gt;</code> &nbsp;&nbsp;&nbsp;示例：<strong>粗体<strong><br>下划线：<code>&lt;u&gt;下化线&lt;/u&gt;</code>&nbsp;&nbsp;&nbsp;&nbsp;示例：<u>下划线</u><br>字体颜色一起设置：<br>    <code>&lt;p style=&quot;font-size:12;color:#0F3&quot;&gt;课程&lt;/p&gt;</code>&nbsp;&nbsp;&nbsp;&nbsp;示例： <p style="font-size:30px;color:#0F3">课程</p><br><code>&lt;font color=&quot;red&quot; size=&quot;12px&quot;&gt;红色&lt;/font&gt;</code>&nbsp;&nbsp;&nbsp;&nbsp;示例：<font color="red" size="12">红色</font></strong></strong></p>
<h5 id="背景色"><a href="#背景色" class="headerlink" title="背景色"></a>背景色</h5><p><code>&lt;table&gt;&lt;td bgcolor=yellow&gt;&lt;center&gt;背景色yellow&lt;/center&gt;&lt;/td&gt;&lt;/table&gt;</code><br>示例：<table><td bgcolor="yellow"><center>背景色yellow</center></td></table></p>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><p><a href="https://www.jianshu.com/p/80ac23666a98" target="_blank" rel="noopener">如何在markdown中打出上标、下标和一些特殊符号</a><br><a href="https://blog.csdn.net/html5_/article/details/21639475" target="_blank" rel="noopener">HTML中的特殊符号</a><br><a href="http://www.divcss5.com/html/h50807.shtml" target="_blank" rel="noopener">HTML 符号</a><br><a href="https://blog.csdn.net/sinat_34719507/article/details/53693418" target="_blank" rel="noopener">HTML H5数学符号、希腊字符、各种箭头符号、科技符号以及形状转义符号
</a><br><a href="https://zhidao.baidu.com/question/460760819492379085.html" target="_blank" rel="noopener">html怎么改变字体大小和颜色</a><br><a href="https://blog.csdn.net/heimu24/article/details/81189700" target="_blank" rel="noopener">Markdown进阶</a></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>HTML</tag>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>单变量线形回归</title>
    <url>/2019/06/05/%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E5%BD%A2%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h4 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://study.163.com/course/courseMain.htm?courseId=1004570029" target="_blank" rel="noopener">学习地址</a></center></h4><p>训练集</p>
<ul>
<li>监督学习中会有一个数据集，这便是训练集。在卖房案例中已有房价便是数据集，工作便是如何从这个数据集中预测房价。</li>
</ul>
<p>定义</p>
<ul>
<li>m ：训练样本的数量</li>
<li>x‘s：输入变量/特征</li>
<li>y‘s：输出变量（要预测的目标变量）</li>
<li>(x,y)表示一个训练样本</li>
<li>(x<sup>(i)</sup>,y<sup>(i)</sup>)表示第i个训练样本；i只是索引表示表格第i行，不是i次幂。</li>
</ul>
<p>单变量线形回归</p>
<ul>
<li>单变量值指只有一个变量x，根据训练集-&gt;学习函数模型得出一个假设h(hypothesis)，类似函数。x作为输入，得出y，这便是线形回归，卖房案例是一元线形回归–&gt;单变量线形回归。</li>
</ul>
<p>代价函数</p>
<ul>
<li>Hypothesis：  h<sub>&theta;</sub>(x)=&theta;<sub>0</sub>+&theta;<sub>1</sub>x    </li>
<li>&theta;<sub>i</sub>是参数Parameters，该如何选择参数的值来让假设函数表示的直线尽量与训练集中的数据点很好的<font color="red">拟合</font>。  </li>
<li>在线形回归中，我们要解决的是一个最小化的问题，使得h(x)与y尽量接近。</li>
<li>令： J(&theta;<sub>0</sub>,&theta;<sub>1</sub>)=1/2/m*&sum;(h<sub>&theta;</sub>(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>，目标使J最小化，本质求 &theta;。</li>
<li>J(&theta;<sub>0</sub>,&theta;<sub>1</sub>)叫做平方误差代价函数，代价函数有很多，平方误差相对常用。</li>
</ul>
<p>梯度下降(Gradient descent algorithm)  </p>
<ul>
<li>temp0=&theta;<sub>0</sub>-&alpha;(&part;J(&theta;<sub>0</sub>,&theta;<sub>1</sub>)/&part;&theta;<sub>0</sub>)</li>
<li>temp1=&theta;<sub>1</sub>-&alpha;(&part;J(&theta;<sub>0</sub>,&theta;<sub>1</sub>)/&part;&theta;<sub>1</sub>)</li>
<li>&theta;<sub>0</sub>=temp0, &theta;<sub>1</sub>=temp1</li>
<li>&alpha;叫做学习率，用来控制梯度下降时迈出多大的步子。</li>
<li>(&part;J(&theta;<sub>0</sub>,&theta;<sub>1</sub>)/&part;&theta;<sub>i</sub>)是偏导数。</li>
<li>注意&theta;<sub>0</sub>和&theta;<sub>1</sub>要同步更新</li>
<li>重复直到收敛，&alpha;要动态变化，刚开始可以很大，后面慢慢减小接近最低点。</li>
</ul>
<p>线形回归的梯度下降</p>
<ul>
<li>将梯度下降法应用到线形回归模型中</li>
<li>(&part;J(&theta;<sub>0</sub>,&theta;<sub>1</sub>)/&part;&theta;<sub>i</sub>)是偏导数，具体如下：</li>
<li>当i=0： (&part;J(&theta;<sub>0</sub>,&theta;<sub>1</sub>)/&part;&theta;<sub>0</sub>)=1/m*&sum;(h<sub>&theta;</sub>(x<sup>(i)</sup>)-y<sup>(i)</sup>)</li>
<li>当i=1： (&part;J(&theta;<sub>0</sub>,&theta;<sub>1</sub>)/&part;&theta;<sub>1</sub>)=1/m*&sum;(h<sub>&theta;</sub>(x<sup>(i)</sup>)-y<sup>(i)</sup>)x<sup>(i)</sup>)</li>
<li>那么，上面的梯度下降算法就可以用偏导数替换了。</li>
</ul>
<p>尚存疑问</p>
<ul>
<li>具体实现时如何试探性选择参数&theta;</li>
<li>如何选择梯度下降出发点</li>
<li>没有给出严格证明为什么能得到最优解？</li>
<li>参数&alpha;如何自动更新</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>送快递!</title>
    <url>/2019/06/05/%E9%80%81%E5%BF%AB%E9%80%92/</url>
    <content><![CDATA[<h4 id="如何插入图片：网络-本地"><a href="#如何插入图片：网络-本地" class="headerlink" title="如何插入图片：网络/本地"></a>如何插入图片：网络/本地</h4><h5 id="插入网络图片"><a href="#插入网络图片" class="headerlink" title="插入网络图片"></a>插入网络图片</h5><ul>
<li><code>![Alter text](链接)</code> ：[]里面是可选填字段，如果不加<code>!</code>直接显示文字，点击在新页面打开图片；如果不填而是这种<code>![]</code>格式，直接显示网络图片。<br>示例：  <ol>
<li>直接打开图片<br><img src="https://i0.hdslb.com/bfs/album/177d83cc433a147406b711f13a59dfeb9880fe36.jpg@518w_1e_1c.jpg" alt>   </li>
<li><a href="https://i0.hdslb.com/bfs/album/177d83cc433a147406b711f13a59dfeb9880fe36.jpg@518w_1e_1c.jpg" target="_blank" rel="noopener">点击打开图片</a>  </li>
</ol>
</li>
</ul>
<h5 id="插入本地图片"><a href="#插入本地图片" class="headerlink" title="插入本地图片"></a>插入本地图片</h5><ul>
<li>尚未学会！</li>
<li>目前先采用上传至在线免费图床，然后使用在线图片。有道云和<a href="https://i.loli.net/" target="_blank" rel="noopener">SM.MS</a>都可以。</li>
</ul>
<h4 id="自古评论出人才"><a href="#自古评论出人才" class="headerlink" title="自古评论出人才"></a>自古评论出人才</h4><p><img src="https://i.loli.net/2019/06/05/5cf73391bf0fd18983.png" alt="2019-06-05 10-54-16 的屏幕截图.png"><br><img src="https://i.loli.net/2019/06/05/5cf7342c0d69528535.png" alt="2019-06-05 10-54-54 的屏幕截图.png"></p>
<p><del>您的东风快递，请签收！</del><br>哈哈哈哈</p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>娱乐</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习概述</title>
    <url>/2019/06/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/</url>
    <content><![CDATA[<h3 id="学习地址"><a href="#学习地址" class="headerlink" title="学习地址"></a><center><a href="https://study.163.com/course/courseMain.htm?courseId=1004570029" target="_blank" rel="noopener">学习地址</a></center></h3><h4 id="什么是机器学习"><a href="#什么是机器学习" class="headerlink" title="什么是机器学习"></a>什么是机器学习</h4><h5 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h5><p>Samuel：在没有明确设置的情况下，使计算机具有学习能力的研究领域。<br>Mitchell：计算机程序从经验E中学习，解决某一任务T，进行某一性能度量P，通过P测定在T上的表现因经验E而提高。</p>
<h5 id="机器学习算法"><a href="#机器学习算法" class="headerlink" title="机器学习算法"></a>机器学习算法</h5><p></p><h6>Main</h6><br>监督学习： Supervised learning<br>非监督学习：Unsupervised learning<p></p>
<p></p><h6>Others</h6><br>强化学习：Reinforcement  learning<br>推荐系统：recommender systems <p></p>
<h4 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h4><p>我们给出数据集，有输入和输出。里面可能包含了想要的答案，而电脑要做的是给出更多的正确答案。</p>
<ul>
<li>回归：预测连续值(线形/非线形)的属性(输出)    –&gt; 卖房案例</li>
<li>分类：预测离散值的输出(0/1)   –&gt; 肿瘤是否良性  </li>
</ul>
<h4 id="非监督学习"><a href="#非监督学习" class="headerlink" title="非监督学习"></a>非监督学习</h4><p>给出数据集，但除了裸的数据外什么都没有，要求在里面找到某种结构。</p>
<ul>
<li>聚类：将数据集分类   –&gt; Gogle news案例</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>伤离别</title>
    <url>/2019/06/02/%E4%BC%A4%E7%A6%BB%E5%88%AB/</url>
    <content><![CDATA[<ol>
<li>终于要离开了，4年还没来得及品味就过去了。   </li>
<li>并不想在朋友圈发诸如<del>江湖再见</del>之类的话，但昨晚从汇森楼出来的时候仰望图书馆心里不免也会有一阵触动。一个地方，无论过去多么难堪，可终究是4年青春，或欢喜或愤慨，终点总是离别，未来尚不可期，伴随的只有回忆。  </li>
<li>不知道同班同学走了多少人，但我希望悄悄的走，免得牵出更多的情绪。  </li>
<li>为什么选择这个时候走呢。记得去年是6月10号左右答辩的，去年正处考研阶段，和学长们在汇森也是挺融洽的，也不太记得他们是什么时候离开的。今年答辩提前了半个月，毕业照聚餐等事宜都在5月底前结束了。想来18号答辩完，中途各种琐事，到现在也已经2周有余了，原来我已经在宿舍颓靡了这么久了。没有了起床的动力每天睡到正午起然后外卖维生，靠着自己的一点点羞愧心理下午或者说傍晚去找个地方学习一会，然后晚上很开心地回到宿舍玩到2点。导师并没有给出确定的日期去学校，但给了一个<strong>机器学习</strong>的视频课程，6月应该是相对空闲的一个月，自己想搭建一个个人博客，折腾了两天用模版勉强搭了个能看的。又不会PHP、MySQL，导师给的视频也没看，Python进阶课程也没看。所以6月预计每天拿出6个小时?学习。最近倒是有在学习MySQL，这一周也就搭了博客和学习了点MySQL吧。如果再像之前那样我留在学校的意义也就没有了，6级也没报，估计以后也不会报了吧。   </li>
<li>答辩完那天晚上教练赵sir请院里几个领导和我们一群大四的吃饭，领导们的讲话对我也是有一番的激励作用。我想过以后要回来，至少带回点东西给学弟学妹们，也算是回馈母校了，还有赵老师的恩情。但，自己一事无成又怎么有脸回来呢，未来不知道会怎样，自己默默地想要努力，一定要功成名就。  </li>
<li>之前一直有人问我什么时候走，我回答的是这周，临近周末才匆匆买票。不是旺季火车票也很容易买，只是这次没有学生价优惠。既然说了这周要走那么就一定得做到，所以买了最后一天的票。昨晚临时决定今天来图书馆呆一天，要走了剩下的时间每分每秒都要珍惜，毕竟也是考研呆了半年的地方。想不到今天来发现位置寥寥无几，比去年要凶残的多。是考研的人多了还是学弟学妹们这么爱学习。在这群学子中，我不过是个普通人，没人知道我是哪一级的，也没有人知道我在做什么。</li>
<li>早上出门发现宿舍楼墙上贴着各种条幅：<u>以后可以用大功率电器了，但要注意用电安全</u>，中午回来才明白意思，倒是很应景，下午出门发现那条又撤了，哈哈哈哈。</li>
<li>昨晚领到了班级毕业照，发现我的脸被挡住了，哈哈哈哈，倒也没有太在意，我不求别人以后记得我，但我自己以后闯出一片天地就行。  </li>
<li>好像重点都在未来啊，也是，<strong>往者不可谏，来者犹可追</strong>。都过去了，就不必再翻出回忆了，未来才是重点。</li>
<li>我不想谈论遗憾这种话题，这让我觉得矫情。好像都还行，也没有忘记初心，也没有堕落到哪去，一直在做自己认为对的和有意义的。但就是成绩不够理想，这几年上课确实没用过功。几年的体会让我们同级的有个共同的感想：大学，<strong>把所有的时间投入学习才是最值的的事</strong>。  </li>
<li>想和所有认识的人一一道别，互相鼓励。但也只是想想，大学确实开阔了眼界，也让我成长了不少，认识了许多人也经历了许多事。都不敢回想自己以前的样子，青涩？甚至有许多现在看来愚蠢的行为，也许只有自己会在意吧。</li>
<li>凌晨两点的火车，以后不会再回宿舍睡一觉了，想带走所有东西可有什么用呢。现在甚至不知道自己在感伤什么，也许晚上的情绪会更浓烈一点。  </li>
<li>想想这4年我做了什么啊。大一激情澎湃，有幸加入工作室，之后便是集训、刷题、比赛。大二成为算法组组长，满腔热血想要带好工作室，但气氛被我压的很严肃，第二个学期东奔西走参加了一些竞赛倒没有取得什么优异的成绩。大三打区域赛，2铜2铁，无地自容，第二学期一来便投入考研了，一年的时间做一件事，初试362，无奈复试被刷，之后调剂。回来就是毕设，答辩，毕业。总的来说也就做了两个决定，加入工作室和考研。  </li>
<li>大一参加了两个社团，一个校里的一个系里的，许多的人就是这样认识的，可后面有自己的事了也几乎没有联系过了，哈哈哈哈，能陪你走到最后的基本没有，<strong>大部分的时候还得自己习惯孤独</strong>，一条路走着走着就剩你自己在走了。</li>
</ol>
]]></content>
      <categories>
        <category>乱七八糟</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL学习笔记(五)</title>
    <url>/2019/06/02/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%BA%94/</url>
    <content><![CDATA[<h3 id="学习地址-p15-p22"><a href="#学习地址-p15-p22" class="headerlink" title="学习地址 p15-p22"></a><center><a href="https://www.bilibili.com/video/av19538278" target="_blank" rel="noopener">学习地址 p15-p22</a></center></h3><h4 id="表修改之列的增删改"><a href="#表修改之列的增删改" class="headerlink" title="表修改之列的增删改"></a>表修改之列的增删改</h4><ol>
<li>增加列：<code>Alter table 表名 add 列名 列类型 列参数;</code>[默认加在表末尾]<br>加在指定列后面：<code>Alter table 表名 add 列名 列类型 列参数 after 指定列名;</code><br>加在最前面：<code>Alter table 表名 add 列名 列类型 列参数 first;</code>  </li>
<li>删除列：<code>Alter table 表名 drop 列名;</code>  </li>
<li>修改列类型：<code>Alter table 表名 modify 列名 新类型 新参数;</code>  </li>
<li>修改列名及列类型：<code>Alter table 表名 change 旧列名 新列名 新类型 新参数</code></li>
</ol>
<h6 id="question：如果列类型改了导致数据存不下？"><a href="#question：如果列类型改了导致数据存不下？" class="headerlink" title="question：如果列类型改了导致数据存不下？"></a>question：如果列类型改了导致数据存不下？</h6><p>比如 int 改成smallint列，如果<strong>不匹配数据将会丢失</strong>。</p>
<h4 id="表的查询"><a href="#表的查询" class="headerlink" title="表的查询"></a>表的查询</h4><p>select 5种子句</p>
<ul>
<li>where  条件查询</li>
<li>group by  分组查询</li>
<li>having 筛选</li>
<li>order by 排序</li>
<li>limit 限制结果条数</li>
</ul>
<h5 id="where-常用运算符"><a href="#where-常用运算符" class="headerlink" title="where 常用运算符"></a>where 常用运算符</h5><p>比较运算符 + 逻辑运算符</p>
<ol>
<li>比较运算符：<code>&lt;</code>、<code>&lt;=</code>、<code>&gt;</code>、<code>&gt;=</code>、<code>=</code>、<code>!=</code>或<code>&lt;&gt;</code>、<code>in</code>在某集合内、<code>between</code>在某范围内</li>
<li>逻辑运算符：<code>NOT</code>或<code>!</code>、<code>OR</code>或<code>||</code>、<code>AND</code>或<code>&amp;&amp;</code><br>注意优先级问题，和C语言类似</li>
</ol>
<h5 id="where-匹配"><a href="#where-匹配" class="headerlink" title="where 匹配"></a>where 匹配</h5><p>like 模糊匹配</p>
<ol>
<li><code>%</code>通配任意字符</li>
<li><code>_</code>匹配单一字符  </li>
</ol>
<h4 id="SQL查询模型"><a href="#SQL查询模型" class="headerlink" title="SQL查询模型"></a>SQL查询模型</h4><ul>
<li>将<strong>列看成变量</strong>，查询时哪一行符合条件就取出哪一行</li>
<li>既然是变量，就支持运算。</li>
<li>列之间运算结果叫“广义投影”，列的运算结果可以当成列看，还可以起列别名：<code>() as column_name</code></li>
<li>where是对表中的数据发挥作用，where发挥作用时表上实际并没有别名列，发挥完作用形成的结果里才有别名列。</li>
<li>如果想在结果列(别名列)再进行筛选，得用<code>having</code>  </li>
</ul>
<p>一道面试题：一个num整数列，用一条语句把[20,29]的数改为20且把[30,39]之间的数改为30：<code>update test set num=floor(num/10)*10 where num bwtween 20 and 39;</code>  </p>
<p></p><h5>大胆把列当成变量看！</h5>  <p></p>
<h4 id="group分组及统计函数详解"><a href="#group分组及统计函数详解" class="headerlink" title="group分组及统计函数详解"></a>group分组及统计函数详解</h4><ul>
<li>查询最贵的商品价格：<code>select max(shop_price) from goods;</code></li>
<li>查询商品库存总量：<code>select sum(goods_num) from goods;</code></li>
<li>查询所所有商品的平均价格：<code>select avg(shop_price) from goods;</code></li>
<li>统计有多少种商品：<code>select count(*) from goods</code>  <ul>
<li><code>select count(*) from table_name</code> 查询的是绝对行数，不管是否NULL</li>
<li><code>select count(列名) from table_name</code>  查询的是该列不为null的行数</li>
</ul>
</li>
<li>一次计算完每个栏目下的库存量之和：<code>select sum(goods_number) from goods group by cat_id;</code>  </li>
</ul>
<h4 id="having-筛选"><a href="#having-筛选" class="headerlink" title="having 筛选"></a>having 筛选</h4><p><strong> 在结果集中筛选。</strong></p>
<ul>
<li><p>查询栏目的积压货款，且筛选出积压金额&gt;20000的栏目：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select cat_id,sum(goods_number*shop_price) as tot </span><br><span class="line">group by cat_id</span><br><span class="line">having tot&gt;20000</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询比市场价省的钱，且省的钱&gt;200的商品<br>where实现：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select goods_id,market_price,shop_price,market_price-shop_price as discount </span><br><span class="line">from goods</span><br><span class="line">where market_price-shop_price&gt;200</span><br></pre></td></tr></table></figure>
<p>having 实现：  </p>
<pre><code>select goods_id,market_price,shop_price,market_price-shop_price as discount 
from goods
having discount&gt;200
</code></pre></li>
</ul>
<h4 id="综合练习"><a href="#综合练习" class="headerlink" title="综合练习"></a>综合练习</h4><ol>
<li>查询所有同学平均分：<code>select name,avg(score) from result group by name;</code></li>
<li>查询挂科两门以上的同学：<code>select name from result having count(score&lt;60)&gt;=2;</code></li>
<li>查询每个人的平均分及挂科数：<code>select name,avg(score),sum(score&lt;60) from result group by name;</code></li>
<li>查询挂科数2门以上的同学及平均分：<code>select name,avg(score) sum(score&lt;60) as gks from result group by name having gks&gt;=2;</code></li>
<li>count的缺陷：count原理根据列的某一行是否为null计数的，而不是逻辑判断结果计数！！！</li>
<li>正向思维是先查出谁的挂科数&gt;=2，找到人再查这些人的平均分。逆向思维是先查所有人的平均分再筛选，上述就是用的逆向思维，正向比较麻烦要用子查询嵌套，一定要搞清楚查询逻辑模型。  </li>
</ol>
<h4 id="Order-by-与-limit详解"><a href="#Order-by-与-limit详解" class="headerlink" title="Order by 与 limit详解"></a>Order by 与 limit详解</h4><p>最终结果集出来后可以再排序，也就是排序针对最终结果集。即<code>order by</code>要放在<code>where/group/having</code>后面。  </p>
<ul>
<li>按字段排序：<code>order by 列名 desc/asc</code></li>
<li>多字段排序：<code>order by 列1 desc/asc,列2 desc/asc,...</code></li>
</ul>
<p>limit在语句的最后，起限制条目的作用：<code>limit [offset,] N</code>  </p>
<ul>
<li>offset：偏移量，从第几条开始</li>
<li>N：取出条目</li>
<li><code>offset,</code>如果不写则相当于<code>limit 0,N</code></li>
</ul>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL学习笔记(四)</title>
    <url>/2019/06/02/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9B%9B/</url>
    <content><![CDATA[<h3 id="学习地址-p14"><a href="#学习地址-p14" class="headerlink" title="学习地址 p14"></a><center><a href="https://www.bilibili.com/video/av19538278" target="_blank" rel="noopener">学习地址 p14</a></center></h3><h4 id="网站建表实战及优化分析"><a href="#网站建表实战及优化分析" class="headerlink" title="网站建表实战及优化分析"></a>网站建表实战及优化分析</h4><blockquote>
<p>在开发中，把频繁用到的信息优先考虑效率，储存到一张表中<br>不常用的信息和比较占据空间的信息优先考虑空间效率，存储到辅表中  </p>
</blockquote>
<ul>
<li><strong>分析需求</strong></li>
<li><strong>建立数据字典</strong><br>员工信息例子：   </li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:center">列名称</th>
<th style="text-align:center">列类型</th>
<th style="text-align:center">默认值</th>
<th style="text-align:center">是否主键</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Id</td>
<td style="text-align:center">Int unsigned</td>
<td style="text-align:center"></td>
<td style="text-align:center">PRI</td>
</tr>
<tr>
<td style="text-align:center">Username</td>
<td style="text-align:center">Varchar(20)</td>
<td style="text-align:center">‘’</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">gender</td>
<td style="text-align:center">Char(1)/Tinyint</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Weigth</td>
<td style="text-align:center">Tinyint unsigned</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Birth</td>
<td style="text-align:center">Date</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Salary</td>
<td style="text-align:center">Decimal(8,2)</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">lastlogin</td>
<td style="text-align:center">Int unsigned</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Introduce</td>
<td style="text-align:center">Varchar(1500)</td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<ul>
<li><strong>优化</strong><br>除了Username和Introduce外，其他都是定长的。<br>从时间上优化，不妨让所有的列都是定长的，可以极大提高查询效率。<br>从空间上优化，Introduce列是否常用或者常改动，考虑这些我们把它单独拿出来新建一张表。</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:center">列名称</th>
<th style="text-align:center">列类型</th>
<th style="text-align:center">默认值</th>
<th style="text-align:center">是否主键</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Id</td>
<td style="text-align:center">Int unsigned</td>
<td style="text-align:center"></td>
<td style="text-align:center">PRI</td>
</tr>
<tr>
<td style="text-align:center">Username</td>
<td style="text-align:center">Char(20)</td>
<td style="text-align:center">‘’</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">gender</td>
<td style="text-align:center">Char(1)/Tinyint</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Weigth</td>
<td style="text-align:center">Tinyint unsigned</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Birth</td>
<td style="text-align:center">Date</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Salary</td>
<td style="text-align:center">Decimal(8,2)</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">lastlogin</td>
<td style="text-align:center">Int unsigned</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align:center">列名称</th>
<th style="text-align:center">列类型</th>
<th style="text-align:center">默认值</th>
<th style="text-align:center">是否主键</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Id</td>
<td style="text-align:center">Int unsigned</td>
<td style="text-align:center"></td>
<td style="text-align:center">PRI</td>
</tr>
<tr>
<td style="text-align:center">Username</td>
<td style="text-align:center">Char(20)</td>
<td style="text-align:center">‘’</td>
<td style="text-align:center"></td>
<td></td>
</tr>
<tr>
<td style="text-align:center">Introduce</td>
<td style="text-align:center">Varchar(1500)</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<ul>
<li><strong>建表</strong><br>用到建表语法<br>实质是一个声明列的过程<br>Create table 表名(<br>列名1 列类型1 列1参数,<br>列名2 列类型2 列2参数,<br>…<br>…<br>列名n 列类型n 列n参数<br>)engine Innodb/mysiam/bdb charset utf8/gbk/latin1…</li>
</ul>
<h4 id="建立一个学生成绩-学籍管理系统"><a href="#建立一个学生成绩-学籍管理系统" class="headerlink" title="建立一个学生成绩/学籍管理系统"></a>建立一个学生成绩/学籍管理系统</h4><ul>
<li>占坑</li>
</ul>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL学习笔记(三)</title>
    <url>/2019/06/01/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%89/</url>
    <content><![CDATA[<h3 id="学习地址-p8-p13"><a href="#学习地址-p8-p13" class="headerlink" title="学习地址 p8-p13"></a><center><a href="https://www.bilibili.com/video/av19538278" target="_blank" rel="noopener">学习地址 p8-p13</a></center></h3><h4 id="建表过程与字符类型意义"><a href="#建表过程与字符类型意义" class="headerlink" title="建表过程与字符类型意义"></a>建表过程与字符类型意义</h4><p> 建表过程就相当于在A4纸上画表头的过程：<strong>声明字段</strong><br> 建表和列类型有什么关系：</p>
<ul>
<li><strong>分析</strong>：A4纸是数据的存储空间，大小有限。请问：你准备给学号字段留多宽？给姓名字段留多宽？</li>
<li><strong>想要能够容纳放置的内容但又不浪费。</strong></li>
<li>储存同样的数据，不同的列类型，所占据的空间和效率是不一样的，这就是建表前要知道列类型的意义。</li>
<li>故，<strong>重点</strong>：列类型的存储范围与占据的字节关系。</li>
</ul>
<h4 id="MySQL三大列类型"><a href="#MySQL三大列类型" class="headerlink" title="MySQL三大列类型"></a>MySQL三大列类型</h4><h5 id="数值型"><a href="#数值型" class="headerlink" title="数值型"></a>数值型</h5><ul>
<li>整形：<strong>默认是有符号型</strong><ul>
<li>Tinyint：1字节，-128-127，0-255</li>
<li>Smallint：2字节，-32768-32767，0-65535</li>
<li>Mediuint：3字节，-8388608-8388607，0-16777215</li>
<li>Int：4字节，-2147483648-2147483647，0-4294967295</li>
<li>bigint：8字节，-9223372036854775808-9223372036854775807，0-18446744073709551615</li>
<li>M、unsigned、zerofill意义：声明时在列类型后面加unsigned表示其为无符号类型，zerofill表示位数不够时用0填充，M表示补0宽度，和zerofill配合使用才有意义。</li>
</ul>
</li>
<li>小数（浮点型/定点型）<ul>
<li>Float(M,D)：M叫“精度”，代表总位数；D是“标度”，代表小数点后几位</li>
<li>decimal(M,D)：定点型将整数部分和小数部分分开存储，比float精确（0.23与0.25）。</li>
</ul>
</li>
</ul>
<h5 id="字符串型：M限制的是字符不是字节"><a href="#字符串型：M限制的是字符不是字节" class="headerlink" title="字符串型：M限制的是字符不是字节"></a>字符串型：M限制的是字符不是字节</h5><ul>
<li>Char(M)：定长字符串(0&lt;=M&lt;=255)，方便计算文件指针偏移量，磁盘查找效率更优。如果不够长度用空格在尾部补齐，如果本身右侧有空格会丢失，读出时右侧所有空格被清除。</li>
<li>Varchar(M)：变长字符串(0&lt;=M&lt;=65535)，列内容前有1-2个字节前缀来标记实际内容长度。</li>
<li>Text：文本类型，可以存储较大的文本段，搜索速度稍慢，不用加默认值（加了也没用）    ，一般用来存储文章内容、新闻内容等。</li>
<li>Blob：二进制类型，用来存储图像、音频等二进制信息，防止因字符集问题导致信息丢失。</li>
</ul>
<h5 id="日期时间类型"><a href="#日期时间类型" class="headerlink" title="日期时间类型"></a>日期时间类型</h5><ul>
<li>Date日期：3个字节，格式YYYY-MM-DD，1000-01-01～9999-12-31</li>
<li>Time 时间：3个字节，HH:MM:SS，-838:59:59’和838:59:59‘</li>
<li>Datetime日期时间：8个字节，YYYY-MM-DD HH:MM:SS，1000-01-01 00:00:00～9999-12-31 23:59:59</li>
<li>Year 年类型：1个字节，YYYY和YY，1901-2155</li>
</ul>
<hr>
<h6 id="终端下显示时间日期"><a href="#终端下显示时间日期" class="headerlink" title="终端下显示时间日期"></a>终端下显示时间日期</h6><ul>
<li>date：显示日期时间</li>
<li>cal：显示本月第几天，<code>-y</code>显示本年第几天</li>
</ul>
<hr>
<h6 id="ERROR-1366问题："><a href="#ERROR-1366问题：" class="headerlink" title="ERROR 1366问题："></a>ERROR 1366问题：</h6><ul>
<li>错误原因，客户端没有声明字符集</li>
</ul>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>NYOJ 101-两点距离</title>
    <url>/2019/06/01/NYOJ-101-%E4%B8%A4%E7%82%B9%E8%B7%9D%E7%A6%BB/</url>
    <content><![CDATA[<h3 id="NYOJ-101-两点距离"><a href="#NYOJ-101-两点距离" class="headerlink" title="NYOJ 101-两点距离"></a><center><a href="http://nyoj.top/problem/101" target="_blank" rel="noopener">NYOJ 101-两点距离</a></center></h3><p>题意：</p>
<ul>
<li>t组数据，每组4个实数表示两点坐标，求两点间距离，保留两位数。</li>
</ul>
<p>Python解法：</p>
<ul>
<li>实数，在map映射的时候用float：<code>x1,y1,x2,y2=map(float,input().split())</code></li>
<li>开根需要手动导入math包，然后使用math.sqrt(x)</li>
<li>保留两位有效小数：输出时使用<code>%.2f</code>控制。</li>
</ul>
<p><strong>关于开根号</strong><br>几种常用方法：</p>
<ol>
<li>使用math包的math.sqrt(x)函数（x&gt;=0）</li>
<li>使用<code>**</code>运算符：<code>x**0.5</code></li>
<li>pow(x,r)函数：<code>pow(x,1.0/r)</code></li>
</ol>
<p>代码如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/user/bin/python</span><br><span class="line">import math</span><br><span class="line">t=int(input())</span><br><span class="line">for index in range(t):</span><br><span class="line">    x1,y1,x2,y2=map(float,input().split())</span><br><span class="line">    print(&quot;%.2f&quot; %math.sqrt((x1-x2)**2+(y1-y2)**2))</span><br><span class="line">   # print(&quot;%.2f&quot; %pow((x1-x2)**2+(y1-y2)**2,0.5))</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>题解</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>题解</tag>
        <tag>NYOJ</tag>
      </tags>
  </entry>
  <entry>
    <title>Python容器类型公共方法</title>
    <url>/2019/05/31/Python%E5%AE%B9%E5%99%A8%E7%B1%BB%E5%9E%8B%E5%85%AC%E5%85%B1%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h4 id="Python-内置函数"><a href="#Python-内置函数" class="headerlink" title="Python 内置函数"></a>Python 内置函数</h4><pre><code>len(item)   # 计算容器中元素个数
del(item)   # 删除变量,内存被回收
max(item)   # 返回容器中元素最大值，如果是字典，只针对key比较
min(item)   # 返回容器中元素最小值，如果是字典，只针对key比较
cmp(item1,item2)  #比较两个值,-1小于，0相等，1大于，python 3.x取消了cmp函数
字符串比较符合规则： &apos;0&apos;&lt;&apos;A&apos;&lt;&apos;a&apos;
</code></pre><h4 id="切片"><a href="#切片" class="headerlink" title="切片"></a>切片</h4><ul>
<li>支持数据类型：字符串、列表、元组</li>
<li>使用索引值来限定范围，从一个大的字符串中切出小的字符串，左闭右开</li>
<li>字典是一个无序集合，是使用键值对保存数据，无法切片</li>
</ul>
<h4 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h4><pre><code>+        合并运算符，支持字符串、列表、元组
*        重复运算符，支持字符串、列表、元组
in       判断元素是否存在，支持字符串、列表、元组、字典，在对字典操作时，判断的是字典的键
not in   判断元素是否不存在，同上
</code></pre>]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python字符串</title>
    <url>/2019/05/31/Python%E5%AD%97%E7%AC%A6%E4%B8%B2/</url>
    <content><![CDATA[<h4 id="字符串定义"><a href="#字符串定义" class="headerlink" title="字符串定义"></a>字符串定义</h4><ul>
<li>在Python中可以使用一对双引号或者一对单引号定义一个字符串，双引号里面嵌套双引号需要转义，而在单引号里面就可以直接嵌套了  </li>
</ul>
<h4 id="字符串常用操作"><a href="#字符串常用操作" class="headerlink" title="字符串常用操作"></a>字符串常用操作</h4><ul>
<li>统计长度： len(str)</li>
<li>统计某个小字符串出现的次数：str.count(tmp_str)</li>
<li>取某个子字符串出现的位置： str.index(tmp_str)  # 返回第一次出现tmp_str的位置 ,如果不存在则报错</li>
<li>截取字符串： str[begin:end:num]  在指定范围内每隔num个字符截取一个字符</li>
</ul>
<h4 id="Python内置提供的方法"><a href="#Python内置提供的方法" class="headerlink" title="Python内置提供的方法"></a>Python内置提供的方法</h4><ul>
<li>判断类型<pre><code>str.isspace()   # 如果字符串中只包含空白，则返回 True，否则返回 False.
str.isalnum()   # 如果字符串至少有一个字符并且所有字符都是字母或数字则返 回 True,否则返回 False
str.isalpha()   # 如果字符串至少有一个字符并且所有字符都是字母则返回 True, 否则返回 False
str.isdigit()   # 如果字符串只包含数字，1或者(1)或者半角字符，则返回 True 否则返回 False.
str.isdecimal()   # 检查字符串是否只包含十进制字符，如果是返回 true，否则返回 false。
str.islower()   # 如果字符串中包含至少一个区分大小写的字符，并且所有这些(区分大小写的)字符都是小写，则返回 True，否则返回 False
str.isnumeric()   # 如果字符串中只包含数字字符比如(1)或者半角字符或者零一之类，则返回 True，否则返回 False
str.isupper()   # 如果字符串中包含至少一个区分大小写的字符，并且所有这些(区分大小写的)字符都是大写，则返回 True，否则返回 False
str.capitalize()   # 将字符串的第一个字符转换为大写
str.lower()    # 转换字符串中所有大写字符为小写
str.upper()    # 转换字符串中所有大写字符为小写
str.swapcase()   # 将字符串中大写转换为小写，小写转换为大写
</code></pre></li>
<li>查找和替换<pre><code>str.startswith(substr, beg,end)  #检查字符串是否是以指定子字符串 substr 开头，是则返回 True。如果beg 和 end 指定值，则在指定范围内检查，也可以缺省
str.endwith(substr, beg,end)  # 和上述类似
str.find(str, beg=0, end=len(string))  # 注意和index方法的区别
 #检测 str 是否包含在字符串中，如果指定范围 beg 和 end ，则检查是否包含在指定范围内，如果包含则返回开始的索引值，否则返回-1
str.replace(old, new [, max]) # 将字符串中的 str1 替换成 str2,如果 max 指定，则替换不超过 max 次，返回替换后的字符串，原字符串并不会改变
</code></pre></li>
<li>文本对齐<pre><code>str.ljust(width,fillchar)   # 返回一个原字符串左对齐,并使用 fillchar 填充至长度 width 的新字符串，fillchar 默认为空格。
str.rjust(width,fillchar)   # 返回一个原字符串右对齐,并使用fillchar(默认空格）填充至长度 width 的新字符串
str.center(width, fillchar)  # 返回一个指定的宽度 width 居中的字符串，fillchar 为填充的字符，默认为空格。
</code></pre></li>
<li>去除空白字符<pre><code>str.lstrip()    # 截掉字符串左边的空格或指定字符，即把以指定（默认为空格）开头的去掉再输出
str.rstrip()    # 删除字符串末尾的空格.
str.strip()     # 删除字符串两边的空白字符.
</code></pre></li>
<li>拆分和拼接字符字符串   <pre><code>string.pattition(str)   # 把字符串string 分成一个3元素的元组(str前面,str,str后面)
string.split(str,num)   # 以str为分隔符拆分string，如果num有指定值则拆分num次(str至少出现num次), 返回列表
string.join(seq)   # 以string作为分隔符，将 seq 中所有的元素(的字符串表示)合并为一个新的字符串返回
</code></pre></li>
</ul>
<h4 id="eval函数"><a href="#eval函数" class="headerlink" title="eval函数"></a>eval函数</h4><p><code>eval</code>函数十分强大 – 将字符串当成有效的表达式来求值并返回计算结果<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 基本数学运算</span><br><span class="line">print(eval(&apos;1+1&apos;))</span><br><span class="line">print(eval(&apos;2*3&apos;))</span><br><span class="line"># 字符串重复</span><br><span class="line">print(eval(&quot;&apos;*&apos;*10&quot;))</span><br><span class="line"># 将字符串转换成列表</span><br><span class="line">print(type(eval(&apos;[1,2,3,4,5]&apos;)))</span><br><span class="line"># 将字符串转换成字典</span><br><span class="line">print(type(eval(&quot;&#123;&apos;name&apos;:&apos;xiaoming&apos;,&apos;age&apos;:&apos;18&apos;&#125;&quot;)))</span><br></pre></td></tr></table></figure></p>
<h5 id="案例-计算器"><a href="#案例-计算器" class="headerlink" title="案例 - 计算器"></a>案例 - 计算器</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">input_str=input(&quot;请输入算术题： &quot;)</span><br><span class="line">print(eval(input_str))</span><br><span class="line"></span><br><span class="line">- - - - - - - - -</span><br><span class="line">请输入算术题： 2*3</span><br><span class="line">6</span><br></pre></td></tr></table></figure>
<h5 id="不要滥用eval"><a href="#不要滥用eval" class="headerlink" title="不要滥用eval"></a>不要滥用<code>eval</code></h5><p>在开发时不要使用<code>eval</code>直接转换<code>input</code>的输出结果<br>存在安全隐患<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">__import__(&apos;os&apos;).system(&apos;ls&apos;)</span><br><span class="line"></span><br><span class="line"># 等价于</span><br><span class="line">import os</span><br><span class="line">os.system(&apos;终端命令&apos;)</span><br></pre></td></tr></table></figure></p>
<p>执行成功返回0，否则返回错误信息</p>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python字典</title>
    <url>/2019/05/31/Python%E5%AD%97%E5%85%B8/</url>
    <content><![CDATA[<h4 id="字典的定义-dic"><a href="#字典的定义-dic" class="headerlink" title="字典的定义: dic={}"></a>字典的定义: dic={}</h4><ul>
<li>dictionary(字典)是除列表以外python中最灵活的数据类型</li>
<li>字典同样可以用来储存多个数据，通常用于储存描述一个物体的相关信息</li>
<li>和列表的区别<br> 列表是有序的对象集合<br>字典是无序的对象集合，在使用print函数输出字典时，通常输出的顺序和定义的顺序是不一致的！<br>字典用{}定义<br>字典使用 键值对 储存数据，键值对之间使用<code>,</code>分隔<br> 键 key 是索引<br>值 value 是数据<br>  键和值之间使用 : 分隔<br>键必须是唯一的!<br>值可以取任何数据类型，但键只能使用字符串、数组或元组</li>
</ul>
<h4 id="字典的常用操作"><a href="#字典的常用操作" class="headerlink" title="字典的常用操作"></a>字典的常用操作</h4><ul>
<li>取值： dict_name[key]<br>代表 key 对应的值，key必须存在</li>
<li>增加/修改： dict_name[key]=value<br>如果key存在则用value覆盖之前的值，如果key不存在则新增键值对</li>
<li>删除元素： dict_name.pop(key)<br>如果key不存在则会报错</li>
<li>删除字典： del dict_name</li>
<li>统计键值对的数量： len（dict_name)<br>返回dic_name中键值对的数量</li>
<li>合并字典： dict_name.update(tmp_dict)<br>如果被合并的字典中包含已经存在的键值对，会覆盖原有的键值对</li>
<li>清空字典： dict_name.clear()</li>
</ul>
<h4 id="字典的循环遍历"><a href="#字典的循环遍历" class="headerlink" title="字典的循环遍历"></a>字典的循环遍历</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for key in dict_name:</span><br><span class="line">     print(key,&apos;:&apos;,dict_name[key])</span><br></pre></td></tr></table></figure>
<h4 id="字典和列表组合应用"><a href="#字典和列表组合应用" class="headerlink" title="字典和列表组合应用"></a>字典和列表组合应用</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">     card_list=[</span><br><span class="line">  &#123; &quot;name&quot;:&quot;张三&quot;</span><br><span class="line">    &quot;qq&quot;:&quot;12345&quot;</span><br><span class="line">    &quot;phone&quot;:&quot;119&quot;</span><br><span class="line">  &#125;</span><br><span class="line">  &#123;&quot;name&quot;:&quot;李四&quot;</span><br><span class="line">   &quot;qq&quot;:&quot;54321&quot;</span><br><span class="line">   &quot;phone&quot;:&quot;10088&quot;</span><br><span class="line">   &#125;</span><br><span class="line">]</span><br><span class="line">     print(card_list)   # 列表是有序的对象集合，会先输出张三的信息，再输出李四的信息</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python元组</title>
    <url>/2019/05/31/Python%E5%85%83%E7%BB%84/</url>
    <content><![CDATA[<h4 id="元组的定义：-tuple-name"><a href="#元组的定义：-tuple-name" class="headerlink" title="元组的定义： tuple_name=()"></a>元组的定义： tuple_name=()</h4><ul>
<li>Tuple(元组)与列表类似，不同之处再与元组的元素 不能修改</li>
<li>元组表示多个元素组成的序列，数据类型可以不同</li>
<li>用于存储一串信息，数据之间用<code>,</code>分隔</li>
<li>元组用()定义，索引从0开始，tuple_name[index]</li>
<li>元组中只包含一个元素时，需要在元素后面添加逗号<code>tup1 = (50,)</code> ，不加逗号，则此时tup1是int型</li>
</ul>
<h4 id="元组的常用操作"><a href="#元组的常用操作" class="headerlink" title="元组的常用操作"></a>元组的常用操作</h4><ul>
<li>取值： tuple_name[index]<br> index超出范围会报错：IndexError: tuple index out of range</li>
<li>取索引： tuple_name.index(value)<br> 返回第一次出现value的位置</li>
<li>统计计数： tuple_name.count(value)<br>返回value在元组中出现的次数</li>
<li>统计元组中包含的元素个数: len(tuple_name)<br> 返回元组的长度</li>
<li>截取元组中的值： tuple_name[start:end]<br>列表也可以进行此项操作</li>
</ul>
<h4 id="元组变量的循环遍历"><a href="#元组变量的循环遍历" class="headerlink" title="元组变量的循环遍历"></a>元组变量的循环遍历</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for name in tuple_name:</span><br><span class="line">         statement(s)</span><br></pre></td></tr></table></figure>
<h4 id="元组的连接和删除"><a href="#元组的连接和删除" class="headerlink" title="元组的连接和删除"></a>元组的连接和删除</h4><ul>
<li>元组中的元素时不允许修改的，但可以对元组进行连接</li>
<li>tmp_tuple=tuple1+tuple2</li>
<li>使用del 删除整个元组<code>del tuple_name</code></li>
</ul>
<h4 id="元组和列表相互转换"><a href="#元组和列表相互转换" class="headerlink" title="元组和列表相互转换"></a>元组和列表相互转换</h4><ul>
<li>使用list函数可以把元组转换成列表<code>list(tuple_name)</code> ，返回一个列表</li>
<li>使用tuple函数可以把列表转换成元组<code>tuple(list_name)</code>， 返回一个元组</li>
</ul>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Pyhton列表</title>
    <url>/2019/05/31/Pyhton/</url>
    <content><![CDATA[<ul>
<li>非数字型： 字符串、列表、元组、字典</li>
<li>在python中，所有非数字型变量都支持以下特点:<ol>
<li>都是一个序列sequence，也可以理解为容器</li>
<li>取值[]</li>
<li>遍历 for in</li>
<li>计算长度、最大值/最小值、比较、删除</li>
<li>链接 . 和重复</li>
<li>切片</li>
</ol>
</li>
</ul>
<h4 id="列表的定义：-list-name"><a href="#列表的定义：-list-name" class="headerlink" title="列表的定义： list_name=[]"></a>列表的定义： list_name=[]</h4><ul>
<li>List(列表)是Python中使用最频繁的数据类型，在其他语言中通常叫做 数组</li>
<li>专门用于存储一串信息，数据类型可以不同，但在使用排序方法时要求列表内所有数据类型相同</li>
<li>列表用[]定义，数据之间使用 , 分隔</li>
<li>列表的索引(下标)从0开始，从列表中取值时，如果超出索引范围，程序会报错： IndexError: list index out of range</li>
</ul>
<h4 id="列表常用操作方法：-列表名-方法名"><a href="#列表常用操作方法：-列表名-方法名" class="headerlink" title="列表常用操作方法：  列表名.方法名()"></a>列表常用操作方法：  列表名.方法名()</h4><ul>
<li>知道索引取值： list_name[index]</li>
<li>知道内容取索引： list_name.index(value)  <ol>
<li>如果有多个相同的内容，则取第一个出现的位置</li>
<li>如果列表中无此内容，则程序报错： ValueError: ‘value’ is not in list</li>
</ol>
</li>
<li>修改指定位置的值： list_name[index]=value # 指定的索引不能超出范围：IndexError: list assignment index out of range</li>
<li>向列表中增加数据:  <ol>
<li>ist_name.append(value) # 向列表末尾追加数据</li>
<li>list_name.insert(index,value) # 在index位置插入数据 </li>
<li>list_name.extend(temp_list) # 把另一个列表的内容追加到当前列表末尾</li>
</ol>
</li>
<li>从列表中删除数据：  <ol>
<li>list_name.remove(value)  # 多个value只删第一个，如果value不在列表中则报错</li>
<li>list_name.pop(index)  # 默认删除最后一个元素，可以带索引参数删除指定位置值</li>
<li>list_name.clear() # 清空列表</li>
</ol>
</li>
<li>使用 del 关键字删除列表元素：  del list_name[index]  <ol>
<li>del 关键字本质上是用来将一个变量从内存中删除，后续代码就不能再使用此变量</li>
</ol>
</li>
<li>统计列表长度： len(list_name)  # 返回列表长度</li>
<li>统计某个值出现次数： list_name.count(value)  #返回value 在列表中出现的次数</li>
<li>列表的排序和反转: <ol>
<li>list_name.sort()    # 升序排序，排序时要求列表中的元素类型都相同</li>
<li>list_name.sort(reverse=True)  # 降序排序</li>
<li>list_name.reverse()  # 反转                 </li>
</ol>
</li>
<li>Tips： 对于上述方法并不用死记硬背，变量名后面输入 . ，然后选择要针对这个变量进行的操作，如果对于方法的功能不熟悉，只需用CTRL + Q 查看</li>
</ul>
<h4 id="列表的循环遍历"><a href="#列表的循环遍历" class="headerlink" title="列表的循环遍历"></a>列表的循环遍历</h4><ul>
<li>在Python 中为了提高列表的遍历效率，专门提供的迭代iteration 遍历</li>
<li>使用 for 循环就能实现迭代遍历  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for name in list_name:</span><br><span class="line">    print(name)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="列表的截取和连接"><a href="#列表的截取和连接" class="headerlink" title="列表的截取和连接"></a>列表的截取和连接</h4><ul>
<li>list_name[start:end]<br> 输出一个子列表，如果end小与start 输出的是空列表</li>
<li>tmp_list=list1 + list2<br> 和extend函数类似</li>
</ul>
<h4 id="用列表定义二维数组"><a href="#用列表定义二维数组" class="headerlink" title="用列表定义二维数组"></a>用列表定义二维数组</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">list=[[0]*n for i in range(n)]</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python函数与模块</title>
    <url>/2019/05/31/Python%E5%87%BD%E6%95%B0%E4%B8%8E%E6%A8%A1%E5%9D%97/</url>
    <content><![CDATA[<h4 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h4><ul>
<li>把具有独立功能的代码块组织为一个小模块，在需要的时候调用，可以提高编写效率以及代码的重用。</li>
<li>函数的使用包含两个步骤：<ol>
<li>定义函数–封装独立的功能</li>
<li>调用函数，如果不主动调用函数，函数是不会执行的</li>
</ol>
</li>
</ul>
<h4 id="函数定义"><a href="#函数定义" class="headerlink" title="函数定义"></a>函数定义</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def 函数名():     # def 是define的缩写，函数名的命名应该符合标识符的命名规则，小括号内部可以填写参数</span><br><span class="line">         函数封装的代码</span><br><span class="line">         ...</span><br></pre></td></tr></table></figure>
<ul>
<li>Tips： python函数的定义必须在调用前面(函数调用函数例外)</li>
<li>PyCharm 代码调式工具：<br>在代码某行添加端点，debug 时会在断点停止执行，在下方debug界面的Console界面可以看到执行结果<br> F8 : step over 可以单步执行代码，会把函数调用看作是一行代码直接执行<br> F7 : Step Into 可以单步执行代码，如果是函数，会进入函数内部</li>
<li>函数的文档注释：<br>在开发中，如果希望给函数添加注释，应该在 定义函数 的下方，使用连续的三对引号<br>在连续的三对引号之间编写对函数的说明文字<br>在 函数调用 位置，使用快捷键 CTRL + Q 可以查看函数的说明信息<br>注：因为函数体相对比较独立，函数定义的上方，应该和其他代码（包括注释）保留两个空行</li>
</ul>
<h4 id="函数参数"><a href="#函数参数" class="headerlink" title="函数参数"></a>函数参数</h4><ul>
<li>在函数名后面的小括号内部填写参数名，自动识别参数类型 </li>
<li>多个参数之间使用 <code>,</code>分隔   </li>
<li><p>例子： 判断素数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def judge(x):</span><br><span class="line">    i=2;</span><br><span class="line">    while(i*i&lt;=x):</span><br><span class="line">        if(x%i==0):</span><br><span class="line">            return False</span><br><span class="line">        i+=1;</span><br><span class="line">    return True</span><br><span class="line">x=int(input());</span><br><span class="line">if judge(x):</span><br><span class="line">    print(&apos;YES&apos;)</span><br><span class="line">else:</span><br><span class="line">    print(&apos;NO&apos;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>关于形参和实参<br>定义函数时，小括号中的参数，用来接收参数，在函数内部作为变量使用<br>调用函数时，小括号中的参数，用来把数据传递到函数内部</p>
</li>
<li>函数的参数问题：   <ol>
<li>无论是传递的参数是可变还是不可变,只要针对参数使用 赋值 语句，会在函数内部修改局部变量的引用，不会影响到外部变量的引用   </li>
<li>在函数内部使用方法修改 可变参数 会影响外部参数，列表变量在调用 += 运算符本质上是在调用列表的 .extend()方法  </li>
<li>缺省参数：定义函数时可以给某个参数指定一个默认值，具有默认值的参数就叫做缺省参数，调用函数时就可以灵活传入缺省参数的值了</li>
<li>必须保证带有默认值的缺省参数在参数列表的末尾，如果有多个缺省参数需要指定参数名给指定的参数赋值</li>
<li>多值参数：不确定参数的个数时，参数名前加一个<code>*</code>可以接收元组如 <code>*args</code>，加两个星号<code>**</code>可以接收字典如 <code>**kwargs</code>  </li>
<li>元组和字典的拆包：如果参数中有多个多值参数，这个时候希望准确传参就需要拆包了，</li>
<li>拆包的方式：在实参数前加一个<em>会把实参传给接收元组的多值参数，加两个</em>传给接收字典变量的多值参数</li>
</ol>
</li>
</ul>
<h4 id="函数返回值"><a href="#函数返回值" class="headerlink" title="函数返回值"></a>函数返回值</h4><ul>
<li>返回值时函数完成工作后，最后返回给调用者的一个结果，也可以用元组(可以缺省括号)返回多个结果或者多个变量接收多个返回结果</li>
<li>在函数中使用return 关键字可以返回结果</li>
<li>执行了返回语句后，函数内后续代码都不会执行</li>
</ul>
<h4 id="函数的嵌套调用"><a href="#函数的嵌套调用" class="headerlink" title="函数的嵌套调用"></a>函数的嵌套调用</h4><ul>
<li>一个函数里面调用另一个函数。</li>
<li>函数嵌套可以不分函数定义的先后顺序</li>
<li>例子： 求和  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def f1(x,y):</span><br><span class="line">    return f2(x,y);   # 在上方可以调用下方定义的函数</span><br><span class="line">def f2(x,y):</span><br><span class="line">    return x+y;</span><br><span class="line">x,y=int(input()),int(input());</span><br><span class="line">print(f1(x,y));</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="模块"><a href="#模块" class="headerlink" title="模块"></a>模块</h4><ul>
<li>模块就好比是工具包，要想使用这个工具包中的工具，就需要import导入这个模块</li>
<li>每一个以扩展名py结尾的Python源代码文件都是一个模块</li>
<li>在模块中定义的全局变量、函数都是模块能够提供给外面能够直接使用的工具</li>
<li>在调用模块中的函数时，必须这样引用： 模块名.函数名</li>
<li>当解释器遇到 import 语句，如果模块在当前的搜索路径就会被导入。</li>
<li>搜索路径是一个解释器会先进行搜索的所有目录的列表。</li>
<li>如想要导入模块 support.py，需要把命令放在脚本的顶端：</li>
<li>一个模块只会被导入一次，不管你执行了多少次import。这样可以防止导入模块被一遍又一遍地执行。</li>
</ul>
<h4 id="pass关键字"><a href="#pass关键字" class="headerlink" title="pass关键字"></a>pass关键字</h4><ul>
<li>如果在开发程序时，不希望立刻编写分支内部的代码，可以使用pass关键字，表示一个占位符，能够保证代码的结构正确</li>
<li>程序运行时，pass关键字不会执行任何操作</li>
</ul>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python循环语句</title>
    <url>/2019/05/31/Python%E5%BE%AA%E7%8E%AF%E8%AF%AD%E5%8F%A5/</url>
    <content><![CDATA[<p>程序开发的三大流程： 顺序 、 分支 、 循环</p>
<h4 id="while-循环的基本使用"><a href="#while-循环的基本使用" class="headerlink" title="while 循环的基本使用"></a>while 循环的基本使用</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">     while 条件:        # 和if语句类似，条件可以用小括号括起来，注意冒号</span><br><span class="line">           ...             #  while语句以及缩进部分是一个完整代码</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">         处理条件(计数器+1)</span><br></pre></td></tr></table></figure>
<h4 id="break-和-continue"><a href="#break-和-continue" class="headerlink" title="break 和 continue"></a>break 和 continue</h4><ul>
<li>和 C语言一样。</li>
</ul>
<h4 id="while-循环嵌套"><a href="#while-循环嵌套" class="headerlink" title="while 循环嵌套"></a>while 循环嵌套</h4><ul>
<li><p>和if 类似，可与for循环相互嵌套</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">     while expression1:</span><br><span class="line">while expression2:</span><br><span class="line">    statement(s2)</span><br><span class="line">         statement(s1)</span><br></pre></td></tr></table></figure>
</li>
<li><p>例子： 求100以内的素数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">i=2;</span><br><span class="line">while (i&lt;=100):</span><br><span class="line">    x=2;</span><br><span class="line">    f=1;</span><br><span class="line">    while (x*x&lt;=i):</span><br><span class="line">        if(i%x==0):</span><br><span class="line">            f=0;</span><br><span class="line">            break;</span><br><span class="line">        x+=1;</span><br><span class="line">    if(f): print(i,end=&apos; &apos;)   # end默认以空行结尾，这里修改end为空格，每输出一个就带一个空格</span><br><span class="line">    i+=1;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="关于print函数输出结尾问题"><a href="#关于print函数输出结尾问题" class="headerlink" title="关于print函数输出结尾问题"></a>关于print函数输出结尾问题</h3><ul>
<li>如果不希望末尾增加换行，可以再print函数输出内容的后面增加 ,end=””</li>
<li>其中 “” 中间可以指定print函数输出内容之后，继续希望显示的内容</li>
<li>在控制台输出一个制表符 ‘\t’，协助在输出文本时 垂直方向 保持对齐</li>
</ul>
<h4 id="Python-for-循环语句"><a href="#Python-for-循环语句" class="headerlink" title="Python for 循环语句"></a>Python for 循环语句</h4><ul>
<li>完整的for循环语法  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for 变量 in 集合:</span><br><span class="line">     ...</span><br><span class="line"> else:</span><br><span class="line">    #没有用break跳出循环，上述循环结束会执行的代码</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="几个例子"><a href="#几个例子" class="headerlink" title="几个例子"></a>几个例子</h5><ul>
<li><p>遍历字符串：  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">name = &apos;I love lyq&apos;</span><br><span class="line">length=len(name);  # 取字符串长度</span><br><span class="line">for letter in name:</span><br><span class="line">    print(letter,end=&quot;&quot;)</span><br><span class="line">    if(letter==name[length-1]):</span><br><span class="line">        print(&quot;&quot;)</span><br><span class="line">    else:</span><br><span class="line">        print(end=&quot;&quot;);</span><br></pre></td></tr></table></figure>
</li>
<li><p>通过序列索引迭代  </p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/usr/bin/python</span><br><span class="line"># -*- coding: UTF-8 -*-</span><br><span class="line"> </span><br><span class="line">fruits = [&apos;banana&apos;, &apos;apple&apos;,  &apos;mango&apos;]</span><br><span class="line">for index in range(len(fruits)):    # range() 返回一个序列的数。</span><br><span class="line">   print &apos;当前水果 :&apos;, fruits[index]</span><br><span class="line">   </span><br><span class="line">print &quot;Good bye!&quot;</span><br></pre></td></tr></table></figure>
<ul>
<li>打印 1 - 100 之间的素数  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for index1 in range(2,100):</span><br><span class="line">    flag=1</span><br><span class="line">    for index2 in range(2,index1):</span><br><span class="line">        if(index2*index2&gt;index1): break;   # 把判断语句写在循环体内部</span><br><span class="line">        if(index1%index2==0):</span><br><span class="line">            flag=0</span><br><span class="line">            break</span><br><span class="line">    if(flag==1):</span><br><span class="line">        print(index1,end=&quot; &quot;)</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python判断语句</title>
    <url>/2019/05/31/Python%E5%88%A4%E6%96%AD%E8%AF%AD%E5%8F%A5/</url>
    <content><![CDATA[<p>条件判断语句又叫“分支语句”.</p>
<h4 id="if-语句基本语法"><a href="#if-语句基本语法" class="headerlink" title="if 语句基本语法"></a>if 语句基本语法</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if 判断条件:   #必须要加`:`，回车会自动在下一行增加缩进tab(4个空格)</span><br><span class="line">       条件成立，要做的事      # 后面带 缩进 的代码都属于这个判断条件内</span><br><span class="line">       .</span><br><span class="line">       .</span><br><span class="line">       .</span><br><span class="line">elif 判断条件:      # else if () </span><br><span class="line">        ...</span><br><span class="line"> else :            # else 相当于default,后面不能再接判断条件</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<ul>
<li>Tips：  if 语句和下方的带缩进部分的代码是一个完整的代码块<pre><code>所以，在python中，一定要注意空格问题
判断条件也可以用括号括起来，这样可以可以与逻辑运算符联用，使得判断语句更加复杂
</code></pre></li>
</ul>
<h4 id="比较运算符"><a href="#比较运算符" class="headerlink" title="比较运算符"></a>比较运算符</h4><ul>
<li>和C语言完全相同。在python2.x中判断不等于还可以使用&lt;&gt;运算符  </li>
</ul>
<h4 id="逻辑运算符"><a href="#逻辑运算符" class="headerlink" title="逻辑运算符"></a>逻辑运算符</h4><ul>
<li>and 、or 、 not      # 注意优先级，关键字最好用空格单独隔开，避免语法错误，养成规范代码习惯 </li>
<li>比如判断闰年：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">    x=int(input(&apos;year = &apos;))</span><br><span class="line">if (not(x%400==0 or (x%4==0 and x%100!=0))):</span><br><span class="line">    print(&apos;NO&apos;)</span><br><span class="line"> else:</span><br><span class="line">        print(&apos;YES&apos;)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="if语句的嵌套"><a href="#if语句的嵌套" class="headerlink" title="if语句的嵌套"></a>if语句的嵌套</h4><ul>
<li><p>还是用判断闰年的例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">    x=int(input(&apos;year = &apos;))</span><br><span class="line">if x%400==0:</span><br><span class="line">   print(&apos;YES&apos;)</span><br><span class="line">else:</span><br><span class="line">   if x%4==0 and x%100!=0:</span><br><span class="line">      print(&apos;YES&apos;)</span><br><span class="line">   else:</span><br><span class="line">          print(&apos;NO&apos;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>tips：在pycharm中有一个非常好用的技巧：选中几行，按下tab会统一给这几行增加缩进，如果想取消，按下 shift+tab 即可</p>
</li>
</ul>
<h4 id="python-中随机数的处理"><a href="#python-中随机数的处理" class="headerlink" title="python 中随机数的处理"></a>python 中随机数的处理</h4><ul>
<li>在Python 中，要使用随机数，首先需要导入 随机数 的模块–工具包</li>
<li>导入模块后，可以直接在模块名称后面接一个 . 然后按Tab 键，会提示该模块中包含的所有函数</li>
<li>random.randint(a,b) , 返回[a,b]之间的整数，下限 a 必须小于等于上限 b<ul>
<li>Tips： 在导入工具包的时候，应该将导入的语句放在文件的顶部，这样方便下方的代码在任何需要的时候使用工具 </li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python运算符和表达式</title>
    <url>/2019/05/31/%E8%BF%90%E7%AE%97%E7%AC%A6%E5%92%8C%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
    <content><![CDATA[<p>Python运算符和C语言基本类似，有意思的是，它比C语言要多了一些运用。</p>
<h4 id="算术运算符"><a href="#算术运算符" class="headerlink" title="算术运算符"></a>算术运算符</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">div=10/3</span><br><span class="line">print(&apos;%d / %d = %d&apos; %(10,3,10/3));</span><br><span class="line">&gt;&gt;&gt;10 / 3 = 3</span><br><span class="line">fdiv=10//3</span><br><span class="line">print(&apos;%d // %d = %d&apos; %(10,3,10//3));</span><br><span class="line">&gt;&gt;&gt;10 // 3 = 3</span><br><span class="line">power=10**3</span><br><span class="line">print(&apos;%d ** %d = %d&apos; %(10,3,10**3));</span><br><span class="line">&gt;&gt;&gt;10 ** 3 = 1000</span><br><span class="line">mod=10%3</span><br><span class="line">print(&apos;%d %% %d = %d&apos; %(10,3,10%3));</span><br><span class="line">&gt;&gt;&gt;10 % 3 = 1</span><br><span class="line">a=&quot;abc&quot;</span><br><span class="line">print(a*2)</span><br><span class="line">&gt;&gt;&gt;abcabc</span><br></pre></td></tr></table></figure>
<ul>
<li>未列出来的项目均与C类似</li>
<li>对于大数类运算python有着不可比拟的优势。</li>
</ul>
<h4 id="2-赋值运算符"><a href="#2-赋值运算符" class="headerlink" title="2.赋值运算符"></a>2.赋值运算符</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x/=3 =&gt; x=x/3</span><br><span class="line">x**=3 =&gt; x=x**3</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br></pre></td></tr></table></figure>
<h4 id="3-逻辑运算符"><a href="#3-逻辑运算符" class="headerlink" title="3.逻辑运算符"></a>3.逻辑运算符</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a,b=0,1</span><br><span class="line">if(a and b):   # and or not </span><br><span class="line">	print(&apos;%d and %d is True!&apos; %(a,b))</span><br><span class="line">else :</span><br><span class="line">	print(&apos;%d is false or %d is false!&apos; % (a, b))</span><br><span class="line">&gt;&gt;&gt;0 is false or 1 is false!</span><br></pre></td></tr></table></figure>
<h4 id="4-关系运算符"><a href="#4-关系运算符" class="headerlink" title="4.关系运算符"></a>4.关系运算符</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a,b=&apos;abc&apos;,&apos;abc&apos;</span><br><span class="line">if(&apos;abc&apos;!=&apos;abc&apos;):</span><br><span class="line">	print(&apos;%s is not equal to %s&apos; %(a,b))</span><br><span class="line">else :</span><br><span class="line">	print(&apos;%s is equal to %s&apos; % (a, b))</span><br><span class="line">&gt;&gt;&gt;abc is equal to abc</span><br></pre></td></tr></table></figure>
<h4 id="5-字符串运算符和表达式"><a href="#5-字符串运算符和表达式" class="headerlink" title="5.字符串运算符和表达式"></a>5.字符串运算符和表达式</h4><ul>
<li><p>5.1 字符串连接：+  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a=&apos;Hello&apos;</span><br><span class="line">b=&apos; World&apos;</span><br><span class="line">print(&apos;a + b = &apos;,a+b)</span><br><span class="line">&gt;&gt;&gt;a + b =  Hello World</span><br></pre></td></tr></table></figure>
</li>
<li><p>5.2 重复输出字符串：*</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a=&apos;hello&apos;</span><br><span class="line">b=&apos;world&apos;</span><br><span class="line">print(&apos;a * 2 =  &apos;,a*2,&apos;\nb * 3 =  &apos;,b*3)   # python2 和 python3执行结果不同，本代码基于python3</span><br><span class="line">&gt;&gt;&gt;a * 2 =   hellohello </span><br><span class="line">&gt;&gt;&gt;b * 3 =   worldworldworld</span><br></pre></td></tr></table></figure>
</li>
<li><p>5.3 成员运算符：in / not in</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a=&apos;Hello&apos;</span><br><span class="line">if(&apos;H&apos; not in a ):</span><br><span class="line"> print(&apos;True&apos;)</span><br><span class="line">else:</span><br><span class="line">  print(&apos;False&apos;)</span><br><span class="line">&gt;&gt;&gt;False</span><br></pre></td></tr></table></figure>
</li>
<li><p>5.4 原始字符串：r/R</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(R&apos;\\n&apos;)</span><br><span class="line">&gt;&gt;&gt;\\n</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="6-变量间的计算"><a href="#6-变量间的计算" class="headerlink" title="6.变量间的计算"></a>6.变量间的计算</h4><ul>
<li>6.1 数字型变量可以直接计算，布尔型变量在参与数字型变量计算时，真对应数字1，假对应数字0参与运算。</li>
<li>6.2 拼接字符串的两种方式<br>   字符串变量之间使用 + 拼接字符串<br>   字符串变量可以和整数使用 * 重复拼接相同的字符串</li>
</ul>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python变量的理解</title>
    <url>/2019/05/31/python%E5%8F%98%E9%87%8F%E7%9A%84%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<ul>
<li>Python属于动态数据类型语言，类型是在运行过程中自动决定的，不需要通过代码申明，根据所赋值来决定其数据类型。</li>
<li>其中，Python变量的命名遵循标识符（类似C）命名规则。  </li>
<li>python 数据类型： 数字型和非数字型<ul>
<li>数字型：整形(int)、浮点型(float)、布尔型(bool)、复数型(complex)</li>
<li>非数字型： 字符串、列表、元组、字典  </li>
</ul>
</li>
</ul>
<h5 id="数据赋值"><a href="#数据赋值" class="headerlink" title="数据赋值"></a>数据赋值</h5><ol>
<li>简单赋值： x=2</li>
<li>序列赋值：等号左侧是元组、列表表示的多个变量名，右侧是元组、列表或字符串表示的值。例如：</li>
<li><p>使用省略圆括号的元组赋值</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a,b=1,2</span><br><span class="line">print(&apos;hahha %d %d&apos; %(a,b))</span><br><span class="line">&gt;&gt;&gt;hahha 1 2</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用元组赋值<br><code>(a,b)=1,2</code></p>
</li>
<li>使用列表赋值<br><code>[a,b]=[30,&#39;abc&#39;]</code></li>
<li>用字符串赋值<br>此时python会将字符串分解为单个字符，依次赋值给每个变量【变量个数必须和字符个数相等，否则报错】<br><code>(a,b,c)=&#39;abc&#39;</code></li>
<li><p>在变量名前使用<code>*</code>，为创建列表对象引用<br>此时，不带星号的变量匹配一个值，剩余的作为列表对象。例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x,*y=&apos;abc&apos;</span><br><span class="line">print(x,y);</span><br><span class="line">&gt;&gt;&gt;a [&apos;b&apos;, &apos;c&apos;]</span><br><span class="line">*y,x=&apos;a&apos;</span><br><span class="line">print(x,y);</span><br><span class="line">a []</span><br></pre></td></tr></table></figure>
</li>
<li><p>多目标赋值：指用多个连续的等号为变量赋值： <code>a=b=c=10</code>  </p>
<ul>
<li>Tips: 使用type()函数可以查看一个变量的类型</li>
<li>在python3中没有了long数据类型，只有int   </li>
</ul>
</li>
<li><p>变量的格式化输出  </p>
<ul>
<li>%s 字符串</li>
<li>%d 有符号十进制整数，%06d 表示输出的整数显示位数，不足的地方用0补齐</li>
<li>%f 浮点数，%.02f 表示小数点后面只显示两位</li>
<li>%% 输出%</li>
<li>以下几种常见输出格式:<blockquote>
<ul>
<li>print(‘my name is %s’ %name)</li>
<li>print(‘my name is %s , this is %s’ %(my_name,others))</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p>标识符与关键字   </p>
<ul>
<li>标识符：由字母数字和下划线组成，首字母不能是数字。</li>
<li>关键字：内置标识符，具有特殊的功能和定义，标识符不能和关键字重命。</li>
<li>以下命令可以查看python中的关键字：  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">   import keyword</span><br><span class="line">print(keyword.kwlist)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">[&apos;False&apos;, &apos;None&apos;, &apos;True&apos;, &apos;and&apos;, &apos;as&apos;, &apos;assert&apos;, &apos;async&apos;, &apos;await&apos;, &apos;break&apos;, &apos;class&apos;, &apos;continue&apos;, &apos;def&apos;, </span><br><span class="line">&apos;del&apos;, &apos;elif&apos;, &apos;else&apos;, &apos;except&apos;, &apos;finally&apos;, &apos;for&apos;, &apos;from&apos;, &apos;global&apos;, &apos;if&apos;, &apos;import&apos;, &apos;in&apos;, &apos;is&apos;, &apos;lambda&apos;, </span><br><span class="line">   &apos;nonlocal&apos;, &apos;not&apos;, &apos;or&apos;, &apos;pass&apos;, &apos;raise&apos;, &apos;return&apos;, &apos;try&apos;, &apos;while&apos;, &apos;with&apos;, &apos;yield&apos;]</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>变量的引用</p>
<ul>
<li>变量和数据都是保存在内存中的，Python中函数的参数传递以及返回值都是靠引用传递的</li>
<li>在Python中，变量和数据是分开存储的，数据保存在内存中的一个位置，变量中保存数据的内存地址</li>
</ul>
<ul>
<li>变量中记录数据的地址就叫做 引用，当再次给变量赋值时，实际上是修改了数据的引用</li>
<li>使用id()函数可以查看数据的内存地址 </li>
</ul>
<ul>
<li>调用函数传递实参：调用函数时，本质上是传递的是实参保存数据的引用，而不是实参保存的数据</li>
<li>函数返回值传递引用： 返回的是数据的引用，而不是数据本身</li>
</ul>
</li>
<li><p>可变类型和不可变类型</p>
<ul>
<li>不可变类型：内存中的数据不允许被修改 – 数字类型、字符串、元组</li>
<li>可变类型：内存中的数据可以被修改 – 列表、字典   </li>
<li>字典的key不能是可变类型，值可以是任意类型的数据。 </li>
</ul>
</li>
<li><p>hash函数</p>
<ul>
<li>python中的hash(o)函数接收一个不可变的数据作为参数，返回结果是一个整数</li>
<li>哈希是一种算法，其作用就是提取数据的特征码（指纹）,相同的内容得到相同的结果。</li>
<li>在Python中，设置字典的键值对时，会首先对key进行hash,决定如何在内存中保存字典的数据，以方便后续对字典的操作</li>
</ul>
</li>
<li>局部变量和全局变量<ul>
<li>局部变量是在函数内部定义的变量，只能在函数内部使用，函数执行结束，局部变量会被系统回收</li>
<li>全局变量是在函数外部定义的变量，所有函数内部都可以使用这个变量</li>
<li>局部变量的生命周期：从创建到被系统回收<blockquote>
<ul>
<li>全局变量的使用：在函数内部，可以通过全局变量的引用获取对应的数据，但是在函数内部不允许直接修改全局变量(可变数据类型除外)的引用而是新创建局部变量</li>
<li>在函数内部修改全局变量的值：用 global 声明与一下变量即可，这句只负责声明不能带赋值</li>
<li>全局变量定义的位置：在开发时，应该把模块中的所有全局变量定义在所有函数的上方</li>
</ul>
</blockquote>
</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python数据类型</title>
    <url>/2019/05/31/python%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</url>
    <content><![CDATA[<h4 id="python3-中6大标准数据类型："><a href="#python3-中6大标准数据类型：" class="headerlink" title="python3 中6大标准数据类型："></a>python3 中6大标准数据类型：</h4><ul>
<li>数字（Number）</li>
<li>字符串（string）</li>
<li>列表（List）</li>
<li>元组（Tuple）</li>
<li>集合（Sets）</li>
<li>字典（Dictionary）</li>
</ul>
<h4 id="数字"><a href="#数字" class="headerlink" title="数字"></a>数字</h4><ul>
<li>数字：整型、浮点型、布尔型、复数以及无穷精度的长整型（long）</li>
<li>复数：实部+虚部，虚部以J或j结尾。可用complex函数来创建复数。其基本格式为：complex(实部，虚部)</li>
<li>type()函数可以查询变量所指的对象类型。</li>
</ul>
<h4 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h4><ul>
<li>单引号与双引号：在表示字符常量的时候，单引号和双引号可以互换，还可以相互嵌套。</li>
<li>三引号：’’’  ，通常用来表示多行字符串，也被称为块字符。三引号还可以作为文档注释，被三引号包含的代码作为多行注释使用。</li>
<li>字符串可以使用+运算符连接起来，或者用*运算符重复字符串。例如：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;print(&apos;abc&apos;+&apos;def&apos;,&apos;my&apos;*3);</span><br><span class="line">&gt;&gt;&gt;string mymymy</span><br><span class="line">    字符串的索引方式：第一种是从左往右，下标从0开始增加；第二种是从右往左，从-1开始减少。例如：</span><br><span class="line">&gt;&gt;&gt;word=&quot;Hello&quot;</span><br><span class="line">&gt;&gt;&gt;print(word[0],word[4])</span><br><span class="line">Ho</span><br><span class="line">&gt;&gt;&gt;print(word[-1],word[-5])</span><br><span class="line">oH</span><br><span class="line">    字符串切片获取子串，格式：变量[头下标：尾下标]，前闭后开。例如：</span><br><span class="line">&gt;&gt;&gt;word = &apos;Ilovepython&apos;</span><br><span class="line">&gt;&gt;&gt;print(word[1:5])</span><br><span class="line">&gt;&gt;&gt;love</span><br><span class="line">&gt;&gt;&gt;print(word[:])</span><br><span class="line">&gt;&gt;&gt;Ilovepython</span><br><span class="line">&gt;&gt;&gt;print(word[5:])</span><br><span class="line">&gt;&gt;&gt;python</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python语言基础</title>
    <url>/2019/05/31/python-%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h5 id="1-注释"><a href="#1-注释" class="headerlink" title="1. 注释"></a>1. 注释</h5><ul>
<li>单行注释：以#开头,为了增加代码的可读性，#后面通常接一个空格</li>
<li>多行注释：每行以#开头或三引号 ‘’’ … ‘’’ 、 “”” … “””</li>
<li>TODO注释：在程序开发时，为了避免自己忘记某项功能的实现，在系统框架搭建好了后可以在相应功能处增加注释，以便后续开发。</li>
<li>TODO 功能描述，注意空格，表示待完成的任务，也可以指定开发人员执行</li>
</ul>
<h5 id="2-多行语句"><a href="#2-多行语句" class="headerlink" title="2. 多行语句"></a>2. 多行语句</h5><ul>
<li>python通常是一行写完一条语句，但如果语句很长，我们可以使用反斜杠 \ 来实现多行语句，例如：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  sum=  num1+ \</span><br><span class="line">num2+ \</span><br><span class="line">        num3</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="3-同一行显示多条语句"><a href="#3-同一行显示多条语句" class="headerlink" title="3. 同一行显示多条语句"></a>3. 同一行显示多条语句</h5><ul>
<li>通常每行代码负责一个动作，同时python 可以在同一行显示多条语句，语句之间使用分号 ；分割。</li>
</ul>
<h5 id="4-输出语句：print"><a href="#4-输出语句：print" class="headerlink" title="4. 输出语句：print()"></a>4. 输出语句：print()</h5><p>  4.1 输出数值常量：print(10)<br>  4.2 输出字符串常量：print(“Hello World!”); print(‘Hello World!’);<br>  4.3 输出字符型或数值型变量:print(x)<br>  4.4 一行输出多个变量（自动加空格）：printf(a,b)，例如：<br>  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">   a=5;</span><br><span class="line">b=6;</span><br><span class="line">printf(a,b);</span><br><span class="line"> &gt;&gt;&gt;5 6</span><br></pre></td></tr></table></figure></p>
<p>  4.5 手动控制输出<br>  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">   a=5;</span><br><span class="line">print(&quot;this is a test !\n %d&quot; %a);</span><br><span class="line">&gt;&gt;&gt;this is a test !</span><br><span class="line">&gt;&gt;&gt; 5</span><br><span class="line">a,b=5,6</span><br><span class="line">print(&apos;a=%d,b=%d\n&apos;%(a,b));//多个变量输出</span><br><span class="line">&gt;&gt;&gt;a=5,b=6</span><br></pre></td></tr></table></figure></p>
<ol start="5">
<li>输入语句：input(),将键盘输入的字符串保存到变量中，python3 默认是字符串类型，但如果输入合法则可以用类型转换函数进行转换<br>name=input(‘请输入您的名字：’); #提示用户输入名字<br>print(‘你刚才输入的是%s’ %name); #显示你刚才输出的字符串  %s是字符串转义</li>
</ol>
<h5 id="6-python常见错误"><a href="#6-python常见错误" class="headerlink" title="6. python常见错误"></a>6. python常见错误</h5><ul>
<li>缩进错误：IndentationError:unexpected indent  #目前的python代码每行前不需要增加多于空格</li>
<li>语法错误：SyntaxError :invalid syntax</li>
<li>命令错误：NameError: name ‘***’ is not defined</li>
</ul>
<h5 id="7-Python版本"><a href="#7-Python版本" class="headerlink" title="7. Python版本"></a>7. Python版本</h5><ul>
<li>目前市场上有两个python 版本并存，分别是python 2.x 和 Python 3.x</li>
<li>Python 2.x默认不支持中文，解释器名称是python</li>
<li>Python 3.x的解释器名称是python3</li>
</ul>
<h5 id="8-交互式运行python程序"><a href="#8-交互式运行python程序" class="headerlink" title="8. 交互式运行python程序"></a>8. 交互式运行python程序</h5><ul>
<li>在终端中输入python便可直接运行解释器，不需要输入要执行的文件名</li>
<li>在python的shell中直接输入Python的代码回车即可看到执行结果，适合学习/验证Python语法或者局部代码，但代码不能保存</li>
<li>用 exit() 或 CTRL + d  退出解释器</li>
</ul>
<h5 id="9-Python的IDE-–-Pycharm"><a href="#9-Python的IDE-–-Pycharm" class="headerlink" title="9. Python的IDE – Pycharm"></a>9. Python的IDE – Pycharm</h5><ul>
<li>集成开发环境（IDE）,集成了开发软件需要的所有工具，一般包括： 图形用户界面、代码编辑器、编译器/解释器、调试器等</li>
<li>PyCharm 适合开发大型项目</li>
<li>Pycharm 常用快捷键：<ul>
<li>替换全文：选中内容然后 shift + F6会提示修改全文匹配信息。 </li>
<li>注释与取消注释： CTRL + /</li>
<li>收缩所有的代码块：Ctrl + Shift + -</li>
<li>展开所有的代码块：Ctrl + Shift + +</li>
</ul>
</li>
</ul>
<h5 id="10-类型转换函数"><a href="#10-类型转换函数" class="headerlink" title="10. 类型转换函数"></a>10. 类型转换函数</h5><ul>
<li>int(x)  将x转换为一个整数</li>
<li>float(x) 将x转换为一个浮点数</li>
<li>list(str) 将str转化为list类型</li>
</ul>
<h5 id="11-Linux上的Shebang符号"><a href="#11-Linux上的Shebang符号" class="headerlink" title="11. Linux上的Shebang符号(#!)"></a>11. Linux上的Shebang符号(#!)</h5><ul>
<li>#!这个符号叫Shebang或者Sha-bang</li>
<li>#!通常在Unix系统脚本的第一行开头使用</li>
<li>指明执行这个脚本文件的解释程序</li>
<li>用which 查看python解释器缩在路径 一般是/usr/bin/python</li>
<li>在源程序第一行增加 #!/usr/bin/python</li>
<li>然后修改主python文件的权限增加执行权限即可在终端直接执行</li>
</ul>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>NYOJ 95-众数问题</title>
    <url>/2019/05/31/NYOJ-95-%E4%BC%97%E6%95%B0%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h4 id="NYOJ-95-众数问题"><a href="#NYOJ-95-众数问题" class="headerlink" title="NYOJ 95-众数问题"></a><center>NYOJ 95-众数问题</center></h4><p>题意：</p>
<ul>
<li>t组数据。每组两行，第一行n（n&lt;=100），第二行n个数。求这n个数中出现次数最多的数与次数。</li>
</ul>
<p>Python解法：</p>
<ul>
<li>所用解释器版本Pyhton3.7</li>
<li>使用input按行读入然后分隔再用map映射再用list转为列表元素</li>
<li>使用列表排序再遍历O(n)解法得出答案，空间复杂度O(n)。</li>
</ul>
<p>代码如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/user/bin/python</span><br><span class="line">t=int(input())</span><br><span class="line">for index in range(t):</span><br><span class="line">    n=int(input())</span><br><span class="line">    num=list(map(int,input().split())) // 转化为int列表</span><br><span class="line">    num.sort()   // 列表默认从小到大排序，但元素类型必须相同</span><br><span class="line">    ans1=num[0]</span><br><span class="line">    tmp=ans2=1</span><br><span class="line">    for i in range(1,len(num)):</span><br><span class="line">        if num[i]==num[i-1]:</span><br><span class="line">            tmp+=1</span><br><span class="line">        else:tmp=1</span><br><span class="line">        if tmp&gt;ans2:</span><br><span class="line">            ans1=num[i]</span><br><span class="line">            ans2=tmp</span><br><span class="line">    print(ans1,ans2)</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>题解</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>题解</tag>
        <tag>NYOJ</tag>
      </tags>
  </entry>
  <entry>
    <title>NYOJ 75-日期计算</title>
    <url>/2019/05/31/NYOJ-75-%E6%97%A5%E6%9C%9F%E8%AE%A1%E7%AE%97/</url>
    <content><![CDATA[<h4 id="NYOJ-75-日期计算"><a href="#NYOJ-75-日期计算" class="headerlink" title="NYOJ 75-日期计算"></a><center>NYOJ 75-日期计算</center></h4><p>题意：</p>
<ul>
<li>t组数据，每组给出年月日，求这一天是这一年的第几天</li>
</ul>
<p>Python解法：</p>
<ul>
<li>所用Pyhton解释器为Python3.7</li>
<li>input按行读入作为字符串，使用input().split()函数默认按空格分隔数据</li>
<li>同时使用map(int,input().split())将每个数据映射为int类型</li>
<li>编写函数判断闰年</li>
<li>range(a,b)函数左闭右开，故列表数组第一个设值应注意。  </li>
</ul>
<p>代码如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/user/bin/python</span><br><span class="line">def is_run(x):</span><br><span class="line">    if (x%400==0) or (x%4==0 and x%100!=0):return True</span><br><span class="line">    return False</span><br><span class="line">t=int(input())</span><br><span class="line">for index in range(t):</span><br><span class="line">    year,month,day=map(int,input().split())</span><br><span class="line">    normal_year=[0,31,28,31,30,31,30,31,31,30,31,30,31]</span><br><span class="line">    ans=day</span><br><span class="line">    for i in range(1,month):</span><br><span class="line">        ans+=normal_year[i]</span><br><span class="line">    if is_run(year) and month&gt;2:ans+=1</span><br><span class="line">    print(ans)</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>题解</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>题解</tag>
        <tag>NYOJ</tag>
      </tags>
  </entry>
  <entry>
    <title>NYOJ 62-笨小熊</title>
    <url>/2019/05/31/NYOJ-62-%E7%AC%A8%E5%B0%8F%E7%86%8A/</url>
    <content><![CDATA[<h4 id="NYOJ-62-笨小熊"><a href="#NYOJ-62-笨小熊" class="headerlink" title="NYOJ 62-笨小熊"></a><center>NYOJ 62-笨小熊</center></h4><p>题意：</p>
<ul>
<li>t组数据，每组一个字符串。求每个字符串中字母出现最多的字母数量减去最少的是否是质数  </li>
</ul>
<p>Python解法：</p>
<ul>
<li>所用解释器为Python3.7</li>
<li>input按行读入</li>
<li>使用str_name.count(str_tmp)求子串出现次数(不含重复)。这里子串就是单个子符不影响</li>
<li>写个判断素数函数  </li>
</ul>
<p>代码如下:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/user/bin/python</span><br><span class="line">def judge(x):</span><br><span class="line">    if x&lt;=1:return False</span><br><span class="line">    for i in range(2,x):</span><br><span class="line">        if i*i&lt;=x:</span><br><span class="line">            if x%i==0:return False</span><br><span class="line">        else:break;</span><br><span class="line">    return True</span><br><span class="line">t=int(input())   </span><br><span class="line">for index in range(t):</span><br><span class="line">    str=input()</span><br><span class="line">    mi=1000</span><br><span class="line">    ma=0</span><br><span class="line">    for i in str:</span><br><span class="line">        mi=min(mi,str.count(i))</span><br><span class="line">        ma = max(ma, str.count(i))</span><br><span class="line">    # print(str,ma,mi)</span><br><span class="line">    if judge(ma-mi):</span><br><span class="line">        print(&quot;Lucky Word\n%d&quot; %(ma-mi))</span><br><span class="line">    else:print(&quot;No Answer\n0&quot;)</span><br></pre></td></tr></table></figure></p>
<p>Pyhton学习教程： <a href="https://www.runoob.com/python3/python3-tutorial.html" target="_blank" rel="noopener">https://www.runoob.com/python3/python3-tutorial.html</a></p>
]]></content>
      <categories>
        <category>题解</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>题解</tag>
        <tag>NYOJ</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL学习笔记(二)</title>
    <url>/2019/05/31/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%BA%8C/</url>
    <content><![CDATA[<h3 id="学习地址-p4-p7"><a href="#学习地址-p4-p7" class="headerlink" title="学习地址 p4-p7"></a><center><a href="https://www.bilibili.com/video/av19538278" target="_blank" rel="noopener">学习地址 p4-p7</a></center></h3><h4 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h4><h5 id="乱码问题"><a href="#乱码问题" class="headerlink" title="乱码问题"></a>乱码问题</h5><ul>
<li>视频教程中使用的是GBK编码，显示乱码解决方法告诉服务器客户端使用的GBK编码：<code>set name gbk;</code>服务器端返回的数据就是gbk编码格式。</li>
<li>.php是utf8编码，.php需要连接mysql，同上设置<code>set name utf8;</code>  </li>
</ul>
<h5 id="语法问题"><a href="#语法问题" class="headerlink" title="语法问题"></a>语法问题</h5><ul>
<li>输入语句不执行继续下一行等待输入：sql语句可以换行，遇到<code>;</code>时认为这条语句结束。</li>
<li>如果语句打错了怎么办（1064问题）：仔细检察，<code>\c</code>跳出执行当前语句，centos下可以按<code>上</code>键重新输入。  </li>
</ul>
<h5 id="大小写问题"><a href="#大小写问题" class="headerlink" title="大小写问题"></a>大小写问题</h5><ul>
<li>MySQL在Win系统下默认都不区分大小写，配置信息在my.ini文件中</li>
<li>在Linux下，数据库名、表名、变量名严格区分大小写，配置信息在/etc/my.cnf文件中    </li>
</ul>
<h5 id="创建数据表"><a href="#创建数据表" class="headerlink" title="创建数据表"></a>创建数据表</h5><p>通用语法：<code>CREATE TABLE table_name (column_name column_type);</code><br>实例：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TABLE IF NOT EXISTS `runoob_tbl`(</span><br><span class="line">   id INT  UNSIGNED AUTO_INCREMENT,</span><br><span class="line">   title  VARCHAR(100) NOT NULL,</span><br><span class="line">   author  VARCHAR(40) NOT NULL,</span><br><span class="line">   date  DATE,</span><br><span class="line">   PRIMARY KEY ( `runoob_id` )</span><br><span class="line">)ENGINE=InnoDB DEFAULT CHARSET=utf8;</span><br></pre></td></tr></table></figure></p>
<p><strong>实例解析：</strong>  </p>
<ul>
<li>如果你不想字段为 NULL 可以设置字段的属性为 NOT NULL， 在操作数据库时如果输入该字段的数据为NULL ，就会报错。</li>
<li>AUTO_INCREMENT定义列为自增的属性，一般用于主键，数值会自动加1。</li>
<li>PRIMARY KEY关键字用于定义列为主键。 您可以使用多列来定义主键，列间以逗号分隔。</li>
<li>ENGINE 设置存储引擎，CHARSET 设置编码格式。</li>
</ul>
<hr>
 <center><strong>教师语录： 多百度，拒绝做伸手党 !  </strong></center>  

<hr>
<h4 id="增删改查-之-insert"><a href="#增删改查-之-insert" class="headerlink" title="增删改查 之 insert"></a>增删改查 之 <code>insert</code></h4><ul>
<li>查看表结构：<code>desc table_name</code></li>
<li>插入数据的逻辑：<code>往哪张表添加行</code> + <code>给哪几列添加值</code> + <code>添加什么值</code></li>
<li><p>通用插入语法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">NSERT INTO table_name ( field1, field2,...fieldN )</span><br><span class="line">                       VALUES</span><br><span class="line">                       ( value1, value2,...valueN );</span><br><span class="line">  //   注： 如果数据是字符型，必须使用单引号或者双引号，如：&quot;value&quot;。</span><br></pre></td></tr></table></figure>
</li>
<li><p>读取数据表：<code>select * from table_name;</code>  </p>
<p><strong>注：</strong></p>
<ul>
<li>如果设置了自增属性可以不指定值，插入省去这列即可</li>
<li>如果设置了默认值不指定值则设为默认值</li>
<li>如果赋值行插入所有列，则可以不声明待插入的列</li>
<li>如果设置了不为空但未插入值则报错</li>
<li>列与值必须一一对应！  </li>
</ul>
</li>
</ul>
<h4 id="增删改查-之-update"><a href="#增删改查-之-update" class="headerlink" title="增删改查 之 update"></a>增删改查 之 <code>update</code></h4><ul>
<li>更改逻辑： <code>改哪张表</code> + <code>改哪几列</code> +  <code>改成什么值</code> + <code>改哪几行</code></li>
<li>更改通用语法：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">update table_name set</span><br><span class="line">列1=value1,列2=value2,...</span><br><span class="line">where expresion</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>注：</strong>  </p>
<ul>
<li>expresion是表达式，符合条件的行都会执行更改。 <ul>
<li>上面语句可以在一行中执行  </li>
</ul>
</li>
</ul>
<h4 id="增删改查-之-delete"><a href="#增删改查-之-delete" class="headerlink" title="增删改查 之 delete"></a>增删改查 之 <code>delete</code></h4><ul>
<li>删除逻辑：<code>哪张表上的数据要删</code> + <code>删除哪些行</code></li>
<li>不存在删除列，只能删除整行</li>
<li>删除语法： <code>delete from table_name where expresion</code></li>
</ul>
<h4 id="增删改查-之-query"><a href="#增删改查-之-query" class="headerlink" title="增删改查 之 query"></a>增删改查 之 <code>query</code></h4><ul>
<li>MySQL 数据库使用SQL SELECT语句来查询数据。</li>
<li><p>查询通用语法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT column_name,column_name</span><br><span class="line">FROM table_name</span><br><span class="line">[WHERE Clause]</span><br><span class="line">[LIMIT N][ OFFSET M]</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询语句中你可以使用一个或者多个表，表之间使用逗号(,)分割，并使用WHERE语句来设定查询条件。</p>
</li>
<li>SELECT 命令可以读取一条或者多条记录。</li>
<li>你可以使用星号<code>*</code>来代替其他字段，SELECT语句会返回表的所有字段数据，即在读取整张数据表的基础上限制某些条件。</li>
<li>你可以使用 WHERE 语句来包含任何条件。</li>
<li>你可以使用 LIMIT 属性来设定返回的记录数。</li>
<li>你可以通过OFFSET指定SELECT语句开始查询的数据偏移量。默认情况下偏移量为0。</li>
</ul>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL学习笔记(一)</title>
    <url>/2019/05/30/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%80/</url>
    <content><![CDATA[<h3 id="学习地址p1-p3"><a href="#学习地址p1-p3" class="headerlink" title="学习地址p1-p3"></a><center><a href="https://www.bilibili.com/video/av19538278" target="_blank" rel="noopener">学习地址p1-p3</a></center></h3><ul>
<li>数据库(DB)：DataBase。DB是长期存储在计算机内的有组织、可共享的大量的数据集合。它可以供各种用户共享，具有最小冗余度和较高的数据独立性。不同的厂商对sql标准的执行有微笑差异，比如mysql没有全连接<code>full join</code>，也没有sql server中的<code>topN</code>用法。</li>
<li>数据库管理员(DBA)：DataBase Administrator。</li>
<li>数据库管理系统(DBMS)：DataBase Management System。DBMS是由DBA对DB的查询、更新、删除、修改操作的。DBMS用来操纵和管理DB的软件，用于建立、使用和维护DB。它对DB进行统一的管理和控制，以保证DB的安全性和完整性，用户可以通过DBMS访问DB中的数据，DBA也可以通过DBMS进行DB的维护工作，它可使多个应用程序和用户拥有不同的方法在同时或不同时刻去建立、修改和询问DB。主流有MySQL与oracle等。</li>
<li>数据库系统(DBS)：DataBase System。包括DB、DBMS、应用系统、DBA。<blockquote>
<p>数据库系统属于系统软件，数据库管理系统属于应用软件。  </p>
</blockquote>
</li>
</ul>
<h5 id="为什么学习MySQL？"><a href="#为什么学习MySQL？" class="headerlink" title="为什么学习MySQL？"></a>为什么学习MySQL？</h5><ol>
<li>MySQL与linux、php、apache配合紧密，LAMP架构。</li>
<li>mysql开源免费。</li>
</ol>
<h5 id="MySQL安装"><a href="#MySQL安装" class="headerlink" title="MySQL安装"></a>MySQL安装</h5><ul>
<li>过程略。视频中版本是5.5的，我所使用的是Ver 14.14 Distrib 5.7.26, for Linux (x86_64) using  EditLine wrapper。<a href="https://blog.csdn.net/zhwyj1019/article/details/80274269" target="_blank" rel="noopener">安装</a><a href="https://blog.csdn.net/li_wei_quan/article/details/78549891" target="_blank" rel="noopener">教程</a>，<a href="https://www.runoob.com/mysql/mysql-tutorial.html" target="_blank" rel="noopener">使用教程</a></li>
<li>软件，尤其是开源软件尽量安装稳定版的，不要追求最新版。</li>
<li>在终端敲mysql时其实是调用了一个可执行文件，比如Win系统下的mysql.exe，这个可执行文件在你的安装目录的/bin里面。</li>
<li>但，系统如何知道去对应目录下找到可执行文件呢，不一定知道。系统有<strong>环境变量</strong>的概念，在环境变量指定的几个目录及当前目录下寻找啊。</li>
<li>所以，安装软件时如果自定义安装目录要么添加环境变量(centos系统在/etc/profile中添加即可)，要么到指定目录下执行。  </li>
</ul>
<h4 id="mysql十条基本入门语句"><a href="#mysql十条基本入门语句" class="headerlink" title="mysql十条基本入门语句"></a>mysql十条基本入门语句</h4><p><strong>1.  连接服务器</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ mysql -u root -p</span><br><span class="line">Enter password:</span><br></pre></td></tr></table></figure></p>
<ul>
<li>执行成功后出现<code>mysql&gt;</code> 命令提示窗口，你可以在上面执行任何 SQL 语句。</li>
<li>如果想退出：<code>mysql&gt;exit</code> 即可</li>
<li>连接成功后面对的是<strong>库</strong></li>
</ul>
<p><strong>2.  选则库与查看表</strong>  </p>
<ul>
<li>选库语句: <code>use 库名</code></li>
<li>如果不知道存在哪些库想查看所有库：<code>mysql&gt;show databases</code></li>
<li>选完库面对的是<strong>表</strong>  </li>
<li>查看库下面所有的表：<code>show tables;</code></li>
</ul>
<p><strong>3.  创建与删除库</strong></p>
<ul>
<li>创建语句：<code>create database 数据库名 [charset 字符集];</code>  <ul>
<li>注：后面的是可选字段；创建库同时也会创建一个简单表。</li>
</ul>
</li>
<li>删除库：<code>drop database 数据库名;</code></li>
<li>MySQL中表/列可以改名，database不能改名，但phpAdmin似乎有这个功能，它是先新建库，把表复制到新库，再删除旧库完成的。    </li>
</ul>
<p><strong>4.  创建和删除表</strong></p>
<ul>
<li>删除表：<code>drop table table_name;</code></li>
<li>创建表：<code>create table table_name;</code></li>
</ul>
<p><strong>5.  重命名和清空表</strong></p>
<ul>
<li>重命名：<code>rename table table_name1 to table_name2;</code></li>
<li>清空表：<code>truncate table_name;</code><blockquote>
<p>truncate与delete区别：truncate相当于删表再重建一张相同结构的表；delete是把数据清除了而已。如果决定全清空truncat要更快一些。</p>
</blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>我是如何一步步搭建起这个博客的</title>
    <url>/2019/05/30/%E6%88%91%E6%98%AF%E5%A6%82%E4%BD%95%E4%B8%80%E6%AD%A5%E6%AD%A5%E6%90%AD%E5%BB%BA%E8%B5%B7%E8%BF%99%E4%B8%AA%E5%8D%9A%E5%AE%A2%E7%9A%84/</url>
    <content><![CDATA[<p>首先声明我的配置环境:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">centos 7.?</span><br><span class="line">git version 1.8.3.1 [下载地址](https://git-scm.com/） </span><br><span class="line">Node.js -v10.15.3 [下载地址](https://nodejs.org/en/)  </span><br><span class="line">github新建仓库：username.git.io</span><br><span class="line">腾讯云域名解析到：https://username.github.io </span><br><span class="line">npm 6.4.1</span><br><span class="line">hexo: 3.8.0</span><br><span class="line">hexo-cli: 1.1.0</span><br></pre></td></tr></table></figure></p>
<h5 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h5><p>此博客使用hexo模版，托管在github中。其中使用了腾讯云公网域名解析到个人博客。</p>
<h5 id="1-在github中新建库"><a href="#1-在github中新建库" class="headerlink" title="1. 在github中新建库"></a>1. 在github中新建库</h5><p>没有github账号则先自行创建。地址：<code>https://github.com/new</code><br>库的名字格式很重要：<code>username.git.io</code><br>username 就是你的github账号名<br>创建好之后进入此库，然后在settings界面的GitHub Pages选择主题，成功后在浏览器地址栏输入：<code>https://username.github.io</code>即可看到你的博客初始界面了。</p>
<h5 id="2-安装git"><a href="#2-安装git" class="headerlink" title="2. 安装git"></a>2. 安装git</h5><p>可以在官网下载压缩包也可以在终端输入命令： <code>yum install git</code>。成功后查看版本，若不成功再找博客寻求解决办法。</p>
<h5 id="3-安装Node-js"><a href="#3-安装Node-js" class="headerlink" title="3. 安装Node.js"></a>3. 安装Node.js</h5><p>我是在官网下载的压缩包，所以自行解压到设定目录后改名：<code>mv node-v6.11.3-linux-x64 node</code>。设置环境变量，在<code>/etc/profile</code>文件末尾添加<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PATH=/[file_path]/node/bin:$PATH</span><br><span class="line">export PATH</span><br></pre></td></tr></table></figure></p>
<p>保存后输入<code>source /etc/profile</code> 使刚才的修改生效（重启电脑长期有效，不然老是断开服务）。</p>
<h5 id="4-安装hexo"><a href="#4-安装hexo" class="headerlink" title="4. 安装hexo"></a>4. 安装hexo</h5><p>在终端输入：<code>npm install -g hexo-cli</code><br>再配置hexo，新建一个空目录作为个人博客文件夹，以后的操作都在里面进行。<br>进入目录，终端输入：<code>hexo init</code><br>完成之后再输入：<code>npm install</code><br>完成后可以看到你的博客目录新出现了一些文件，重点在_config.yml、source和themes这三个文件，后续讲解。<br>启动hexo服务，输入：<code>hexo -g</code>和<code>hexo server</code>，然后在浏览器地址栏输入：<code>localhost:4000</code>应该能看到你的本地博客初始界面，主题为<code>landscape</code>。<br>如果无响应可能4000端口被占用试试：<code>hexo server -p 5000</code>，换个端口然后同样在地址栏输入<code>localhost:5000</code>查看博客初始界面。</p>
<h5 id="5-配置git"><a href="#5-配置git" class="headerlink" title="5. 配置git"></a>5. 配置git</h5><p>远程端博客配置好了，本地博客文件也配置好了，接下来需要将本地博客文件上传至远程端。<br>生成SSH密码，在终端输入<code>ssh-keygen -t rsa -C &quot;your E-mail address&quot;</code>，会有提示信息，将id_rsa.pub里面的内容复制到github博客仓库settings的SSH keys里面。<br>输入<code>ssh -T git@github.com</code>，测试添加ssh是否成功。如果看到Hi后面是你的用户名，就说明成功了。<br>如果你想上传本地内容还需要设置一些信息。<br>设置Git的user name和email，如果你没有设置的话第一次上传会失败然后提示你。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git config --global user.name &quot;username&quot;</span><br><span class="line">git config --global user.mail &quot;usermail&quot;</span><br></pre></td></tr></table></figure></p>
<h5 id="6-修改-config-yml"><a href="#6-修改-config-yml" class="headerlink" title="6. 修改_config.yml"></a>6. 修改_config.yml</h5><p>修改repo值（在末尾）<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repository: git@github.com:username/username.github.io.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure></p>
<p>保存。<br>开头处会有title设置，这个自行设定然后预览查看效果即可。</p>
<h5 id="7-写博文"><a href="#7-写博文" class="headerlink" title="7. 写博文"></a>7. 写博文</h5><p>先要安装一个扩展：<code>npm install hexo-deployer-git --save</code><br>成功之后如果你的<code>./source/_post</code>文件夹没有文档先在博客目录下新建一篇博文<code>hexo new post &quot;title&quot;</code>，会自动在<code>./source/_post</code>目录下生成对应文档，然后你可以进行Markdown语法的编辑。</p>
<h5 id="8-上传博客"><a href="#8-上传博客" class="headerlink" title="8. 上传博客"></a>8. 上传博客</h5><p>基本到这里以上步骤都完成了，第7步和第8步是最常用的两个步骤。<br>先输入<code>hexo clean</code>清除缓存<br>然后快速部署<code>hexo d -g</code>。d就是deploy的意思，g是generate的意思，生成静态html文件然后上传至github中。<br>刷新你的github博客仓库页面可以看到新更新的信息。然后在浏览器地址栏输入<code>https://username.github.io</code>或者你配置好的公网域名就可以看到你的博客了。<br>主题默认是landscape。</p>
<hr>
<h4 id="进阶操作"><a href="#进阶操作" class="headerlink" title="进阶操作"></a><center>进阶操作</center></h4><hr>
<h5 id="9-在本地浏览器查看博客信息"><a href="#9-在本地浏览器查看博客信息" class="headerlink" title="9. 在本地浏览器查看博客信息"></a>9. 在本地浏览器查看博客信息</h5><p>在终端输入<code>hexo server -d</code>，后面出来提示信息在浏览器地址栏输入<code>localhost:4000</code>能够查看到和你的真实的博客页面一样的信息，这就是博客预览，十分方便。<br><code>CTRL + C</code>退出预览。</p>
<h5 id="10-更换主题"><a href="#10-更换主题" class="headerlink" title="10. 更换主题"></a>10. 更换主题</h5><p>这块想必是很多人想要做的，其实也很傻瓜的操作，在<a href="https://hexo.io/themes/" target="_blank" rel="noopener">官方主题</a>中看上哪款按提示信息下载即可。不过网上大部分会有一个新手入门主题：<a href="https://github.com/iissnan/hexo-theme-next" target="_blank" rel="noopener">next</a>，下载好后都会有配置教程的。</p>
<h5 id="11-Hexo-Admin"><a href="#11-Hexo-Admin" class="headerlink" title="11. Hexo-Admin"></a>11. Hexo-Admin</h5><p>这个东西能够让你更加方便<code>在线</code>编写和管理你的博客。<br>安装：<code>npm install --save hexo-admin</code><br>成功后打开hexo服务在本地浏览器地址栏输入<code>http://localhost:4000/admin/</code>即可看到了。</p>
<hr>
<h6 id="注："><a href="#注：" class="headerlink" title="注："></a>注：</h6><ul>
<li>本文出现的username都需要替换成你自己的github账号名</li>
<li>一处出现了E-mail信息，需要替换成你自己的github的设定邮箱</li>
<li>一处出现了file_path，那是你自己的安装路径。</li>
<li>由于工具版本信息不同，还有文件配置也可能存在偏差，所以不保证一定能成功</li>
<li>笔者文笔水平和记忆力有限，不能表达清楚以及操作步骤顺序、完整性都可能存在问题，不保证一定能成功</li>
<li>另网上大牛很多，请多多参考大牛经验。  </li>
</ul>
<h6 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h6><p><a href="https://www.cnblogs.com/fengxiongZz/p/7707219.html" target="_blank" rel="noopener">使用Hexo+Github一步步搭建属于自己的博客（基础）</a><br><a href="https://www.cnblogs.com/fengxiongZz/p/7707568.html" target="_blank" rel="noopener">使用Hexo+Github一步步搭建属于自己的博客（进阶）</a><br><a href="https://blog.csdn.net/dazhaodai/article/details/73730069" target="_blank" rel="noopener">如何从零开始搭建自己的博客</a><br><a href="https://www.jianshu.com/p/628c7724c846" target="_blank" rel="noopener">centos下以解压方式安装nodejs</a><br><a href="https://blog.csdn.net/wugenqiang/article/details/81215396" target="_blank" rel="noopener">Centos7中安装Git并连接使用GitHub基本操作</a><br><a href="https://www.jianshu.com/p/68e727dda16d" target="_blank" rel="noopener">如何优雅地发布Hexo博客</a></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title>Test</title>
    <url>/2019/05/29/test/</url>
    <content><![CDATA[<h5 id="画"><a href="#画" class="headerlink" title="画"></a><center>画</center></h5><h6 id="唐-王维"><a href="#唐-王维" class="headerlink" title="唐 王维"></a><center>唐 王维</center></h6><center>远看山有色，近听水无声。</center><br><center>春去花还在，人来鸟不惊。</center>
]]></content>
      <categories>
        <category>乱七八糟</category>
      </categories>
      <tags>
        <tag>诗歌杂文</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World Last!</title>
    <url>/2019/05/29/Hello-World-Last/</url>
    <content><![CDATA[<h4 id="1-为什么想搭建博客"><a href="#1-为什么想搭建博客" class="headerlink" title="1. 为什么想搭建博客"></a>1. 为什么想搭建博客</h4><ul>
<li>自己的东西和别人的东西用起来感觉不一样</li>
<li>属于自己的私密空间？</li>
</ul>
<h4 id="2-想用来干什么"><a href="#2-想用来干什么" class="headerlink" title="2. 想用来干什么"></a>2. 想用来干什么</h4><ul>
<li>Emmm，写写日志、感想或者人生经验？</li>
<li>最初不是这么想的，CSDN给我的感觉就是破的不行了，另外自己的一些小想法不便公开，所以不想在CSDN上写博客了。</li>
<li>平时会学些东西或者刷刷题，把题解写到这里也不错。</li>
</ul>
<h4 id="3-为什么使用Hexo-github"><a href="#3-为什么使用Hexo-github" class="headerlink" title="3. 为什么使用Hexo + github"></a>3. 为什么使用Hexo + github</h4><ol>
<li>从时间上来说，毕业相对空闲时间较多，现在毕设也基本结束了，所以用一周的时间搞一个个人博客。</li>
<li>从效率上来说，找了很多博客，了解了几种流行模版，Hexo比较多？但这不是主因，我想放在服务器上来者，不过毕设刚结束，一个月内我还不能撤掉。刚好github有托管平台加上网上参考资料比较好找，所以着手搭建了。</li>
</ol>
<h4 id="4-搭建过程出现了什么问题吗？"><a href="#4-搭建过程出现了什么问题吗？" class="headerlink" title="4. 搭建过程出现了什么问题吗？"></a>4. 搭建过程出现了什么问题吗？</h4><p>   Emmm，我觉得这像是在问有什么收获吗。其实，就用了两天甚至一天就可以搭建好了，具体搭建过程我想在后一篇写出来。<br>   搭建其实完全按着博客一步一步走下去就行了，出现的问题一般是自己的文件配置问题。<br>   还有就是不同的人使用的工具版本不同，也会造成一定的问题。<br>   我不会前端，但我觉得挺没有技术含量的，完全是傻瓜式操作，功能别人都给你写好了，你想用就直接改参数就行，或者再找博客CV操作一波就能看到成果了。</p>
<h4 id="5-打算一直用下去吗"><a href="#5-打算一直用下去吗" class="headerlink" title="5. 打算一直用下去吗"></a>5. 打算一直用下去吗</h4><p>   应该不会一直用下去，等服务器可以撤掉毕设的那个项目了我再用服务器试试其他的模版。<br>   这个编辑起来其实挺麻烦的，而且Markdown语法我还要现学。我想找找那种比较符合需求的博客模版吧。今天室友给我说了以下Django，其实说过很多次了，我暑假看看能不能学学做出来。<br>   现阶段先用着吧，域名是自己买的，但不敢公开啊，毕竟博客这么丑这么多人用，我也是要面子的啊，总不能再外行人面前zhuangbi吧，还是谦虚一点好是吧。</p>
<h4 id="6-您刚刚提到了需求，请问您的需求是什么？"><a href="#6-您刚刚提到了需求，请问您的需求是什么？" class="headerlink" title="6. 您刚刚提到了需求，请问您的需求是什么？"></a>6. 您刚刚提到了需求，请问您的需求是什么？</h4><p>   额，首先我想能在线编辑不过分吧。Emmm，其实也挺过分的，自己能力不行要求还辣么多。<br>   还有什么需求呢，我觉得自主性要大一些，能够自己设置修改，博客园就挺好，人家高中生的博客个个都那么漂亮美观，好像又是一波CV操作，哈哈哈哈。<br>   其实网易旗下的乐乎挺好用的，不过感觉挺小众化的样子，如果自己设置博客我想很大程度上得参考一下乐乎。</p>
]]></content>
      <categories>
        <category>乱七八糟</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World Again!</title>
    <url>/2019/05/29/Hello-World-Again/</url>
    <content><![CDATA[<h2 id="this-is-a-test-for-hexo-admin"><a href="#this-is-a-test-for-hexo-admin" class="headerlink" title="this is a test for hexo-admin!"></a><center>this is a test for hexo-admin!</center></h2><h3 id="1-如何新建文档"><a href="#1-如何新建文档" class="headerlink" title="1. 如何新建文档"></a>1. 如何新建文档</h3><p>启动hexo服务，在浏览器端打开 <a href="http://localhost:4000/admin" target="_blank" rel="noopener">http://localhost:4000/admin</a>，进入Posts页面新建new post，然后进行编辑保存。</p>
<h3 id="2-如何上传"><a href="#2-如何上传" class="headerlink" title="2. 如何上传"></a>2. 如何上传</h3><p>懒省事原则没有编写脚本修改_config.yml。<br>在hexo-Admin中编写好点击publish，然后可以在<code>/source/_post</code>中看到新建的文档。<br>在终端下使用命令：<br>hexo clean<br>hexo d -g<br>上传即可。<br>  打开博客公网域名，即可看到了。</p>
<h3 id="3-如何设置标签、类别、访问密码等"><a href="#3-如何设置标签、类别、访问密码等" class="headerlink" title="3. 如何设置标签、类别、访问密码等"></a>3. 如何设置标签、类别、访问密码等</h3><p>在<code>/source/_post</code>中对应文档开头部分信息自动生成，可直接在文档中进行修改</p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2019/05/28/Hello-World/</url>
    <content><![CDATA[<h2 id="Markdown-学习笔记"><a href="#Markdown-学习笔记" class="headerlink" title="Markdown 学习笔记"></a><center>Markdown 学习笔记</center></h2><ul>
<li>经测试发现，不同的Markdown编辑器效果不同</li>
<li>这里再hexo环境下介绍几种常用的编辑样式，方便后续编写</li>
</ul>
<h3 id="1-段落格式"><a href="#1-段落格式" class="headerlink" title="1. 段落格式"></a>1. 段落格式</h3><p>  其实没有段落格式，直接编写文字即可，哈哈哈哈。  </p>
<ol>
<li>需求： 段首空格，以及段间间隔,如何设置粗体与斜体，文本居中显示。  </li>
<li>方法： <ul>
<li>段落的换行是在上一段末尾使用两个以上空格加上回车或者两个回车</li>
<li>粗体在文本前后插入<code>**</code>，斜体在文本前后插入<code>*</code>。  </li>
<li><code>&lt;center&gt;文本&lt;/center&gt;</code><br>示例：  <center><strong>青山鸟飞绝，万径人踪灭</strong>。</center><br><center><em>孤舟蓑笠翁，独钓寒江雪</em>。</center>

</li>
</ul>
</li>
</ol>
<h3 id="2-标题格式"><a href="#2-标题格式" class="headerlink" title="2. 标题格式"></a>2. 标题格式</h3><p>Markdown总共6级标题，编辑格式：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 一级标题  </span><br><span class="line">## 二级标题  </span><br><span class="line">### 三级标题  </span><br><span class="line">#### 四级标题</span><br><span class="line">##### 五级标题  </span><br><span class="line">###### 六级标题</span><br></pre></td></tr></table></figure></p>
<p>效果如下：  </p>
<hr>
<h1 id="一级标题"><a href="#一级标题" class="headerlink" title="一级标题"></a>一级标题</h1><h2 id="二级标题"><a href="#二级标题" class="headerlink" title="二级标题"></a>二级标题</h2><h3 id="三级标题"><a href="#三级标题" class="headerlink" title="三级标题"></a>三级标题</h3><h4 id="四级标题"><a href="#四级标题" class="headerlink" title="四级标题"></a>四级标题</h4><h5 id="五级标题"><a href="#五级标题" class="headerlink" title="五级标题"></a>五级标题</h5><h6 id="六级标题"><a href="#六级标题" class="headerlink" title="六级标题"></a>六级标题</h6><hr>
<h3 id="3-删除线与下划线"><a href="#3-删除线与下划线" class="headerlink" title="3. 删除线与下划线"></a>3. 删除线与下划线</h3><ol>
<li><p>下划线，编辑格式：  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;u&gt;文本&lt;/u&gt;</span><br></pre></td></tr></table></figure>
<p>显示效果：  <u>下划线</u>  </p>
</li>
<li><p>删除线，格式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">~~文本~~</span><br></pre></td></tr></table></figure>
<p>显示效果：<del>删除线</del>  </p>
</li>
</ol>
<h3 id="4-列表"><a href="#4-列表" class="headerlink" title="4. 列表"></a>4. 列表</h3><ul>
<li>1 有序列表： 数字加上<code>.</code>再接一个空格。<br>显示效果：  </li>
</ul>
<hr>
<ol start="0">
<li>文本</li>
<li>文本</li>
<li>文本</li>
</ol>
<hr>
<ul>
<li>2 无序列表： 使用<code>*、+、-</code>作为标记<br>显示效果：</li>
</ul>
<hr>
<ul>
<li>第一项</li>
<li>第二项</li>
<li>第三项</li>
</ul>
<hr>
<h3 id="5-区块引用"><a href="#5-区块引用" class="headerlink" title="5. 区块引用"></a>5. 区块引用</h3><ul>
<li>在段首使用<code>&gt;</code>另接一个空格<br>显示效果：  <blockquote>
<p>区块引用</p>
<blockquote>
<p>嵌套引用    </p>
</blockquote>
</blockquote>
</li>
</ul>
<h3 id="6-代码片段"><a href="#6-代码片段" class="headerlink" title="6. 代码片段"></a>6. 代码片段</h3><ol>
<li>一行中插入代码：在文本两边插入: <code>shift + ~</code>，示例：<code>文本</code></li>
<li>插入代码块，在代码块行首尾插入<code>``</code> `。示例：  <pre><code>代码块1
代码块2
</code></pre></li>
</ol>
<h3 id="7-链接"><a href="#7-链接" class="headerlink" title="7. 链接"></a>7. 链接</h3><ol>
<li>文本链接格式：<code>[显示文本]（地址）</code>，示例： <a href="https://blog.csdn.net/NYIST_TC_LYQ" target="_blank" rel="noopener">My_CSDN</a></li>
<li>图片链接格式：<code>![](地址)</code></li>
<li>音频链接格式：占坑  </li>
</ol>
<h3 id="8-表格"><a href="#8-表格" class="headerlink" title="8. 表格"></a>8. 表格</h3><pre><code>占坑
</code></pre><h3 id="9-遗留问题"><a href="#9-遗留问题" class="headerlink" title="9. 遗留问题"></a>9. 遗留问题</h3><ol>
<li>如何插入音频链接</li>
<li>如何制作表格</li>
<li>段首缩进问题    </li>
<li>列表和区块引用存在问题</li>
</ol>
<h6 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h6><p>[1] <a href="https://www.jianshu.com/p/q81RER" target="_blank" rel="noopener">https://www.jianshu.com/p/q81RER</a><br>[2] <a href="https://www.runoob.com/markdown/md-tutorial.html" target="_blank" rel="noopener">https://www.runoob.com/markdown/md-tutorial.html</a><br>[3] <a href="https://blog.csdn.net/wads23456/article/details/79521019" target="_blank" rel="noopener">https://blog.csdn.net/wads23456/article/details/79521019</a></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
      </tags>
  </entry>
</search>
